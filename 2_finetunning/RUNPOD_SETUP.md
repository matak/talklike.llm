# Fine-tuning Llama 3 8B pro Andreje Babi≈°e na RunPod.io

> **üìö Navigace:** [üè† Hlavn√≠ projekt](../README.md) | [üìä P≈ô√≠prava dat](../1_data_preparation/README.md) | [üèãÔ∏è Detailn√≠ dokumentace](README.md) | [üìà Benchmarking](../3_benchmarking/README.md)

## üìã P≈ôehled
Tento n√°vod v√°s provede procesem fine-tuningu Llama 3 8B modelu s daty Andreje Babi≈°e na RunPod.io platformƒõ.

## Struktura dat
Va≈°e data v `../data/all.jsonl` maj√≠ form√°t:
```json
{
    "messages": [
        {
            "role": "system",
            "content": "Jsi Andrej Babi≈°, ƒçesk√Ω politik a podnikatel..."
        },
        {
            "role": "user", 
            "content": "Pane Babi≈°i, m≈Ø≈æete vysvƒõtlit va≈°i roli v t√© chemiƒçce?"
        },
        {
            "role": "assistant",
            "content": "Hele, ta tov√°rna? To u≈æ jsem d√°vno p≈ôedal..."
        }
    ]
}
```

## Kroky pro spu≈°tƒõn√≠ na RunPod.io

### 1. Vytvo≈ôen√≠ podu na RunPod.io

1. Jdƒõte na [runpod.io](https://runpod.io)
2. Vytvo≈ôte nov√Ω pod s n√°sleduj√≠c√≠mi specifikacemi:
   - **GPU**: RTX 4090 nebo A100 (doporuƒçeno pro 8B model)
   - **RAM**: Minim√°lnƒõ 24GB
   - **Storage**: Minim√°lnƒõ 50GB
   - **Template**: PyTorch nebo Jupyter

### 2. P≈ô√≠prava prost≈ôed√≠

Po p≈ôipojen√≠ k podu spus≈•te:

```bash
# Aktualizace syst√©mu
sudo apt update && sudo apt upgrade -y

# Instalace pot≈ôebn√Ωch bal√≠ƒçk≈Ø
sudo apt install -y git wget curl

# Klonov√°n√≠ repozit√°≈ôe (pokud pou≈æ√≠v√°te git)
git clone <v√°≈°-repo-url>
cd <v√°≈°-projekt>
```

### 3. Vytvo≈ôen√≠ .env souboru

Vytvo≈ôte soubor `.env` v ko≈ôenov√©m adres√°≈ôi projektu:

```bash
# Hugging Face token (z√≠sk√°te na huggingface.co/settings/tokens)
HF_TOKEN=hf_your_token_here

# Weights & Biases token (voliteln√©, pro sledov√°n√≠ tr√©nov√°n√≠)
WANDB_API_KEY=your_wandb_token_here
```

### 4. Spu≈°tƒõn√≠ fine-tuningu

```bash
# Spu≈°tƒõn√≠ Jupyter notebooku
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root

# Nebo spu≈°tƒõn√≠ Python skriptu
python finetune_babis_llama.py
```

### 5. Monitorov√°n√≠ tr√©nov√°n√≠

- **W&B Dashboard**: Sledujte metriky na wandb.ai
- **Jupyter**: Sledujte progress v notebooku
- **Terminal**: Logy v termin√°lu

## Konfigurace modelu

### LoRA parametry
- **Rank (r)**: 16
- **Alpha**: 32  
- **Dropout**: 0.1
- **Target modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

### Training parametry
- **Epochs**: 3
- **Batch size**: 2 (per device)
- **Gradient accumulation**: 4
- **Learning rate**: 2e-4
- **Warmup steps**: 100
- **Max length**: 2048 tokens

## Oƒçek√°van√© v√Ωsledky

Po fine-tuningu by model mƒõl:
- Mluvit stylem Andreje Babi≈°e
- Pou≈æ√≠vat jeho charakteristick√© fr√°ze
- Odpov√≠dat v prvn√≠ osobƒõ
- P≈ôid√°vat podpis "Andrej Babi≈°."

## Troubleshooting

### Probl√©m: Out of Memory (OOM)
**≈òe≈°en√≠:**
- Sni≈æte batch size na 1
- Sni≈æte max_length na 1024
- Pou≈æijte gradient checkpointing

### Probl√©m: Pomal√© tr√©nov√°n√≠
**≈òe≈°en√≠:**
- Zkontrolujte GPU vyu≈æit√≠
- Sni≈æte gradient accumulation steps
- Pou≈æijte mixed precision (fp16)

### Probl√©m: Model nekonverguje
**≈òe≈°en√≠:**
- Sni≈æte learning rate na 1e-4
- Zvy≈°te poƒçet epoch
- Zkontrolujte kvalitu dat

## Ulo≈æen√≠ a sd√≠len√≠ modelu

### Lok√°ln√≠ ulo≈æen√≠
```bash
# Model se ulo≈æ√≠ do ./babis-llama-finetuned-final/
```

### Hugging Face Hub
```bash
# Model se automaticky nahraje na HF Hub
# Repo: https://huggingface.co/your-username/babis-llama-3-8b-lora
```

## Pou≈æit√≠ fine-tuned modelu

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Naƒçten√≠ base modelu
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

# Naƒçten√≠ LoRA adapt√©r≈Ø
model = PeftModel.from_pretrained(base_model, "your-username/babis-llama-3-8b-lora")

# Generov√°n√≠
prompt = "Pane Babi≈°i, jak hodnot√≠te inflaci?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## Odhadovan√© n√°klady

- **RTX 4090**: ~$0.60/hod
- **A100**: ~$1.20/hod
- **Oƒçek√°van√° doba tr√©nov√°n√≠**: 2-4 hodiny
- **Celkov√© n√°klady**: $1.20 - $4.80

## Pozn√°mky

1. **Data kvalita**: Ujistƒõte se, ≈æe va≈°e data jsou kvalitn√≠ a konzistentn√≠
2. **Monitoring**: Sledujte loss bƒõhem tr√©nov√°n√≠
3. **Backup**: Pravidelnƒõ ukl√°dejte checkpointy
4. **Testov√°n√≠**: Testujte model bƒõhem tr√©nov√°n√≠

## Kontakt

Pro probl√©my nebo ot√°zky:
- GitHub Issues
- RunPod Discord
- Hugging Face Forums 