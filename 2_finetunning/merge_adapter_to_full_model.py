#!/usr/bin/env python3
"""
Skript pro slouÄenÃ­ LoRA adaptÃ©ru s base modelem do kompletnÃ­ho fine-tuned modelu
"""

# Import setup_environment pro sprÃ¡vnÃ© nastavenÃ­ prostÅ™edÃ­
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import setup_environment

import argparse
from dotenv import load_dotenv
from huggingface_hub import HfApi, login
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

def merge_adapter_to_full_model(adapter_path, base_model_name, output_path, hub_model_id=None, token=None):
    """SlouÄÃ­ LoRA adaptÃ©r s base modelem do kompletnÃ­ho modelu"""
    
    print(f"ğŸ”§ SluÄuji LoRA adaptÃ©r s base modelem...")
    print(f"ğŸ“ Adapter: {adapter_path}")
    print(f"ğŸ“ Base model: {base_model_name}")
    print(f"ğŸ“ VÃ½stup: {output_path}")
    
    # Kontrola existence adaptÃ©ru
    if not os.path.exists(adapter_path):
        print(f"âŒ Adapter neexistuje v cestÄ›: {adapter_path}")
        return False
    
    # Kontrola souborÅ¯ adaptÃ©ru
    required_files = ['adapter_config.json', 'adapter_model.safetensors']
    missing_files = []
    
    for file in required_files:
        if not os.path.exists(os.path.join(adapter_path, file)):
            missing_files.append(file)
    
    if missing_files:
        print(f"âŒ ChybÃ­ soubory adaptÃ©ru: {missing_files}")
        return False
    
    try:
        # NaÄtenÃ­ base modelu
        print("ğŸ”§ NaÄÃ­tÃ¡m base model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # NaÄtenÃ­ tokenizeru
        print("ğŸ”¤ NaÄÃ­tÃ¡m tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # NaÄtenÃ­ a slouÄenÃ­ adaptÃ©ru
        print("ğŸ”— SluÄuji LoRA adaptÃ©r...")
        model = PeftModel.from_pretrained(base_model, adapter_path)
        
        # SlouÄenÃ­ adaptÃ©ru s base modelem
        print("ğŸ”„ ProvÃ¡dÃ­m merge_and_unload...")
        merged_model = model.merge_and_unload()
        
        # NahrÃ¡nÃ­ na HF Hub (pokud je specifikovÃ¡no)
        if hub_model_id and token:
            print("ğŸ“¤ NahrÃ¡vÃ¡m kompletnÃ­ model pÅ™Ã­mo na HF Hub...")
            print("ğŸ’¡ PouÅ¾Ã­vam workspace s vyÄiÅ¡tÄ›nÃ­m cache")
            
            # VyÄiÅ¡tÄ›nÃ­ cache pÅ™ed zaÄÃ¡tkem
            from lib.disk_manager import DiskManager
            dm = DiskManager()
            print("ğŸ§¹ ÄŒistÃ­m cache pro uvolnÄ›nÃ­ mÃ­sta...")
            dm.cleanup_cache()
            
            # Kontrola mÃ­sta
            if not dm.check_disk_space('/workspace', threshold=85):
                print("âš ï¸ StÃ¡le mÃ¡lo mÃ­sta, agresivnÃ­ vyÄiÅ¡tÄ›nÃ­...")
                dm.aggressive_cleanup()
                
                if not dm.check_disk_space('/workspace', threshold=85):
                    print("âŒ Nedost mÃ­sta i po vyÄiÅ¡tÄ›nÃ­")
                    return False
            
            # PouÅ¾ijeme workspace pro doÄasnÃ© uloÅ¾enÃ­
            temp_dir = "/workspace/temp_complete_model"
            print(f"ğŸ“ DoÄasnÃ© umÃ­stÄ›nÃ­: {temp_dir}")
            
            try:
                # UloÅ¾enÃ­ do doÄasnÃ©ho adresÃ¡Å™e s sharding
                print("ğŸ’¾ UklÃ¡dÃ¡m do doÄasnÃ©ho adresÃ¡Å™e...")
                merged_model.save_pretrained(
                    temp_dir,
                    max_shard_size="1GB",  # MenÅ¡Ã­ shardy
                    safe_serialization=True
                )
                tokenizer.save_pretrained(temp_dir)
                
                # NahrÃ¡nÃ­ na HF Hub
                print("ğŸ“¤ NahrÃ¡vÃ¡m na HF Hub...")
                merged_model.push_to_hub(
                    hub_model_id, 
                    token=token,
                    max_shard_size="1GB",
                    safe_serialization=True
                )
                tokenizer.push_to_hub(hub_model_id, token=token)
                
                print(f"âœ… KompletnÃ­ model nahrÃ¡n: https://huggingface.co/{hub_model_id}")
                
            finally:
                # VyÄiÅ¡tÄ›nÃ­ doÄasnÃ©ho adresÃ¡Å™e
                if os.path.exists(temp_dir):
                    print("ğŸ—‘ï¸ MaÅ¾u doÄasnÃ½ adresÃ¡Å™...")
                    import shutil
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    
        else:
            # Kontrola mÃ­sta pÅ™ed uloÅ¾enÃ­m (pouze pokud nenÃ­ HF Hub)
            print("ğŸ’¾ Kontroluji dostupnÃ© mÃ­sto...")
            from lib.disk_manager import DiskManager
            dm = DiskManager()
            
            # ZajistÃ­me, Å¾e output_path je na network storage
            if not output_path.startswith('/workspace'):
                output_path = f'/workspace/{output_path.lstrip("./")}'
            
            # Kontrola mÃ­sta na network storage
            if not dm.check_disk_space('/workspace', threshold=90):
                print("âš ï¸ MÃ¡lo mÃ­sta na network storage, zkouÅ¡Ã­m vyÄiÅ¡tÄ›nÃ­...")
                dm.cleanup_cache()
                
                if not dm.check_disk_space('/workspace', threshold=90):
                    print("âŒ Nedost mÃ­sta pro uloÅ¾enÃ­ kompletnÃ­ho modelu")
                    print("ğŸ’¡ KompletnÃ­ Mistral-7B model potÅ™ebuje ~14GB mÃ­sta")
                    return False
            
            # UloÅ¾enÃ­ kompletnÃ­ho modelu
            print(f"ğŸ’¾ UklÃ¡dÃ¡m kompletnÃ­ model do: {output_path}")
            os.makedirs(output_path, exist_ok=True)
            
            merged_model.save_pretrained(output_path)
            tokenizer.save_pretrained(output_path)
        
        print(f"âœ… KompletnÃ­ model ÃºspÄ›Å¡nÄ› vytvoÅ™en!")
        if hub_model_id:
            print(f"ğŸŒ DostupnÃ½ na: https://huggingface.co/{hub_model_id}")
        else:
            print(f"ğŸ“ UloÅ¾en v: {output_path}")
        
        # VÃ½pis velikosti modelu
        model_size = sum(p.numel() for p in merged_model.parameters())
        print(f"ğŸ“Š Velikost modelu: {model_size:,} parametrÅ¯")
        
        return True
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i sluÄovÃ¡nÃ­: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='SlouÄenÃ­ LoRA adaptÃ©ru s base modelem')
    parser.add_argument('--adapter_path', type=str, required=True,
                       help='Cesta k LoRA adaptÃ©ru')
    parser.add_argument('--base_model', type=str, default='mistralai/Mistral-7B-Instruct-v0.3',
                       help='NÃ¡zev base modelu')
    parser.add_argument('--output_path', type=str, required=True,
                       help='Cesta pro uloÅ¾enÃ­ kompletnÃ­ho modelu')
    parser.add_argument('--hub_model_id', type=str,
                       help='NÃ¡zev modelu na HF Hub (volitelnÃ©)')
    parser.add_argument('--check_only', action='store_true',
                       help='Pouze zkontrolovat adapter bez sluÄovÃ¡nÃ­')
    
    args = parser.parse_args()
    
    # NaÄtenÃ­ promÄ›nnÃ½ch prostÅ™edÃ­
    load_dotenv()
    
    # Kontrola HF tokenu (pokud je potÅ™eba)
    HF_TOKEN = None
    if args.hub_model_id:
        HF_TOKEN = os.getenv("HF_TOKEN")
        if not HF_TOKEN:
            print("âŒ HF_TOKEN nebyl nalezen v prostÅ™edÃ­!")
            print("ğŸ’¡ Nastavte HF_TOKEN v .env souboru nebo prostÅ™edÃ­")
            return False
        
        # PÅ™ihlÃ¡Å¡enÃ­ na HF
        try:
            login(token=HF_TOKEN)
            print("âœ… Hugging Face login ÃºspÄ›Å¡nÃ½")
        except Exception as e:
            print(f"âŒ Chyba pÅ™i pÅ™ihlÃ¡Å¡enÃ­ na HF: {e}")
            return False
    
    # Kontrola existence adaptÃ©ru
    if not os.path.exists(args.adapter_path):
        print(f"âŒ Adapter neexistuje: {args.adapter_path}")
        return False
    
    # VÃ½pis obsahu adresÃ¡Å™e adaptÃ©ru
    print(f"ğŸ“ Obsah adresÃ¡Å™e {args.adapter_path}:")
    try:
        files_in_dir = os.listdir(args.adapter_path)
        for item in files_in_dir:
            item_path = os.path.join(args.adapter_path, item)
            if os.path.isfile(item_path):
                size = os.path.getsize(item_path)
                print(f"  ğŸ“„ {item} ({size:,} B)")
            else:
                print(f"  ğŸ“ {item}/")
    except Exception as e:
        print(f"âš ï¸ Nelze ÄÃ­st obsah adresÃ¡Å™e: {e}")
        return False
    
    # Kontrola oÄekÃ¡vanÃ½ch souborÅ¯
    required_files = ['adapter_config.json', 'adapter_model.safetensors']
    missing_files = [f for f in required_files if f not in files_in_dir]
    if missing_files:
        print(f"âŒ ChybÃ­ soubory adaptÃ©ru: {missing_files}")
        return False
    
    if args.check_only:
        print("\nâœ… Kontrola dokonÄena")
        return
    
    # SlouÄenÃ­ adaptÃ©ru s base modelem
    success = merge_adapter_to_full_model(
        args.adapter_path,
        args.base_model,
        args.output_path,
        args.hub_model_id,
        HF_TOKEN
    )
    
    if success:
        print(f"\nğŸ‰ KompletnÃ­ model byl ÃºspÄ›Å¡nÄ› vytvoÅ™en!")
        print(f"ğŸ“ UloÅ¾en v: {args.output_path}")
        if args.hub_model_id:
            print(f"ğŸŒ DostupnÃ½ na: https://huggingface.co/{args.hub_model_id}")
        
        print("\nğŸ’¡ Pro pouÅ¾itÃ­ kompletnÃ­ho modelu:")
        print(f"   from transformers import AutoModelForCausalLM, AutoTokenizer")
        if args.hub_model_id:
            print(f"   model = AutoModelForCausalLM.from_pretrained('{args.hub_model_id}')")
            print(f"   tokenizer = AutoTokenizer.from_pretrained('{args.hub_model_id}')")
        else:
            print(f"   model = AutoModelForCausalLM.from_pretrained('{args.output_path}')")
            print(f"   tokenizer = AutoTokenizer.from_pretrained('{args.output_path}')")
    else:
        print("\nâŒ SluÄovÃ¡nÃ­ selhalo")

if __name__ == "__main__":
    main() 