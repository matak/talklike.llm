#!/usr/bin/env python3
"""
Skript pro nahrÃ¡nÃ­ jiÅ¾ vygenerovanÃ©ho modelu na Hugging Face Hub
PouÅ¾ijte tento skript, pokud jste zapomnÄ›li pÅ™idat --push_to_hub do pÅ¯vodnÃ­ho pÅ™Ã­kazu
"""

# Import setup_environment pro sprÃ¡vnÃ© nastavenÃ­ prostÅ™edÃ­
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import setup_environment

import argparse
from dotenv import load_dotenv
from huggingface_hub import HfApi, login
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def upload_model_to_hf(model_path, hub_model_id, token):
    """Nahraje model na Hugging Face Hub"""
    
    print(f"ğŸ“¤ NahrÃ¡vÃ¡m model z {model_path} na HF Hub...")
    print(f"ğŸ¯ Model ID: {hub_model_id}")
    
    # Kontrola existence modelu
    if not os.path.exists(model_path):
        print(f"âŒ Model neexistuje v cestÄ›: {model_path}")
        return False
    
    # Kontrola souborÅ¯ modelu
    required_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json']
    missing_files = []
    
    for file in required_files:
        if not os.path.exists(os.path.join(model_path, file)):
            missing_files.append(file)
    
    if missing_files:
        print(f"âš ï¸ ChybÃ­ soubory: {missing_files}")
        print("ğŸ“‹ DostupnÃ© soubory:")
        for file in os.listdir(model_path):
            print(f"  - {file}")
    
    try:
        # NaÄtenÃ­ modelu a tokenizeru
        print("ğŸ”§ NaÄÃ­tÃ¡m model...")
        model = AutoModelForCausalLM.from_pretrained(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        print("ğŸ“¤ NahrÃ¡vÃ¡m na HF Hub...")
        model.push_to_hub(hub_model_id, token=token)
        tokenizer.push_to_hub(hub_model_id, token=token)
        
        print(f"âœ… Model ÃºspÄ›Å¡nÄ› nahrÃ¡n!")
        print(f"ğŸŒ DostupnÃ½ na: https://huggingface.co/{hub_model_id}")
        return True
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i nahrÃ¡vÃ¡nÃ­: {e}")
        return False

def upload_adapter_only(adapter_path, hub_model_id, token):
    """Nahraje pouze LoRA adapter na Hugging Face Hub"""
    
    print(f"ğŸ“¤ NahrÃ¡vÃ¡m LoRA adapter z {adapter_path} na HF Hub...")
    print(f"ğŸ¯ Model ID: {hub_model_id}")
    
    # Kontrola existence adaptÃ©ru
    if not os.path.exists(adapter_path):
        print(f"âŒ Adapter neexistuje v cestÄ›: {adapter_path}")
        return False
    
    # Kontrola souborÅ¯ adaptÃ©ru
    required_files = ['adapter_config.json', 'adapter_model.safetensors']
    missing_files = []
    
    for file in required_files:
        if not os.path.exists(os.path.join(adapter_path, file)):
            missing_files.append(file)
    
    if missing_files:
        print(f"âŒ ChybÃ­ soubory adaptÃ©ru: {missing_files}")
        return False
    
    try:
        # Inicializace HF API
        api = HfApi(token=token)
        
        # VytvoÅ™enÃ­ repository
        print("ğŸ”§ VytvÃ¡Å™Ã­m repository na HF Hub...")
        api.create_repo(
            repo_id=hub_model_id,
            repo_type="model",
            exist_ok=True,
            private=False
        )
        
        # NahrÃ¡nÃ­ souborÅ¯ adaptÃ©ru
        print("ğŸ“¤ NahrÃ¡vÃ¡m soubory adaptÃ©ru...")
        
        # Seznam souborÅ¯ k nahrÃ¡nÃ­
        files_to_upload = [
            'adapter_config.json',
            'adapter_model.safetensors',
            'README.md',
            'training_args.bin'
        ]
        
        # NahrÃ¡nÃ­ kaÅ¾dÃ©ho souboru
        for filename in files_to_upload:
            file_path = os.path.join(adapter_path, filename)
            if os.path.exists(file_path):
                print(f"  ğŸ“„ NahrÃ¡vÃ¡m {filename}...")
                api.upload_file(
                    path_or_fileobj=file_path,
                    path_in_repo=filename,
                    repo_id=hub_model_id,
                    repo_type="model"
                )
            else:
                print(f"  âš ï¸ Soubor {filename} neexistuje, pÅ™eskoÄeno")
        
        # NahrÃ¡nÃ­ tokenizer souborÅ¯ (pokud existujÃ­)
        tokenizer_files = [
            'tokenizer.json',
            'tokenizer.model', 
            'tokenizer_config.json',
            'special_tokens_map.json',
            'chat_template.jinja'
        ]
        
        for filename in tokenizer_files:
            file_path = os.path.join(adapter_path, filename)
            if os.path.exists(file_path):
                print(f"  ğŸ”¤ NahrÃ¡vÃ¡m {filename}...")
                api.upload_file(
                    path_or_fileobj=file_path,
                    path_in_repo=filename,
                    repo_id=hub_model_id,
                    repo_type="model"
                )
        
        print(f"âœ… LoRA adapter ÃºspÄ›Å¡nÄ› nahrÃ¡n!")
        print(f"ğŸŒ DostupnÃ½ na: https://huggingface.co/{hub_model_id}")
        return True
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i nahrÃ¡vÃ¡nÃ­: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='NahrÃ¡nÃ­ modelu na Hugging Face Hub')
    parser.add_argument('--model_path', type=str, 
                       help='Cesta k celÃ©mu modelu (nahrÃ¡vÃ¡ celÃ½ model)')
    parser.add_argument('--adapter_path', type=str,
                       help='Cesta k LoRA adaptÃ©ru (nahrÃ¡vÃ¡ pouze adaptÃ©r)')
    parser.add_argument('--hub_model_id', type=str, required=True,
                       help='NÃ¡zev modelu na HF Hub (napÅ™. username/babis-model)')
    parser.add_argument('--check_only', action='store_true',
                       help='Pouze zkontrolovat model bez nahrÃ¡vÃ¡nÃ­')
    
    args = parser.parse_args()
    
    # NaÄtenÃ­ promÄ›nnÃ½ch prostÅ™edÃ­
    load_dotenv()
    
    # Kontrola HF tokenu
    HF_TOKEN = os.getenv("HF_TOKEN")
    if not HF_TOKEN:
        print("âŒ HF_TOKEN nebyl nalezen v prostÅ™edÃ­!")
        print("ğŸ’¡ Nastavte HF_TOKEN v .env souboru nebo prostÅ™edÃ­")
        return False
    
    # PÅ™ihlÃ¡Å¡enÃ­ na HF
    try:
        login(token=HF_TOKEN)
        print("âœ… Hugging Face login ÃºspÄ›Å¡nÃ½")
    except Exception as e:
        print(f"âŒ Chyba pÅ™i pÅ™ihlÃ¡Å¡enÃ­ na HF: {e}")
        return False
    
    # Kontrola parametrÅ¯
    if args.adapter_path and args.model_path:
        print("âŒ Nelze specifikovat oba parametry --model_path a --adapter_path souÄasnÄ›!")
        print("ğŸ’¡ PouÅ¾ijte buÄ --model_path pro celÃ½ model nebo --adapter_path pro adaptÃ©r")
        return False
    
    if not args.adapter_path and not args.model_path:
        print("âŒ MusÃ­te specifikovat buÄ --model_path nebo --adapter_path!")
        print("ğŸ’¡ PouÅ¾itÃ­:")
        print("   --model_path /cesta/k/modelu     # NahrÃ¡vÃ¡ celÃ½ model")
        print("   --adapter_path /cesta/k/adapteru # NahrÃ¡vÃ¡ pouze adaptÃ©r")
        return False
    
    # UrÄenÃ­ cesty a typu
    if args.adapter_path:
        model_path = args.adapter_path
        is_adapter = True
        print(f"ğŸ” NahrÃ¡vÃ¡m LoRA adaptÃ©r z: {model_path}")
    else:
        model_path = args.model_path
        is_adapter = False
        print(f"ğŸ” NahrÃ¡vÃ¡m celÃ½ model z: {model_path}")
    
    # Kontrola existence
    if not os.path.exists(model_path):
        print(f"âŒ Cesta neexistuje: {model_path}")
        print("\nğŸ’¡ MoÅ¾nÃ© cesty:")
        if is_adapter:
            print("  - /workspace/mistral-babis-finetuned-final")
            print("  - /workspace/babis-mistral-finetuned-final")
        else:
            print("  - /workspace/babis-finetuned-final")
            print("  - /workspace/babis-mistral-finetuned-final")
        return
    
    # VÃ½pis obsahu adresÃ¡Å™e
    print(f"ğŸ“ Obsah adresÃ¡Å™e {model_path}:")
    try:
        files_in_dir = os.listdir(model_path)
        for item in files_in_dir:
            item_path = os.path.join(model_path, item)
            if os.path.isfile(item_path):
                size = os.path.getsize(item_path)
                print(f"  ğŸ“„ {item} ({size:,} B)")
            else:
                print(f"  ğŸ“ {item}/")
    except Exception as e:
        print(f"âš ï¸ Nelze ÄÃ­st obsah adresÃ¡Å™e: {e}")
        return
    
    # Kontrola oÄekÃ¡vanÃ½ch souborÅ¯
    if is_adapter:
        required_files = ['adapter_config.json', 'adapter_model.safetensors']
        missing_files = [f for f in required_files if f not in files_in_dir]
        if missing_files:
            print(f"âŒ ChybÃ­ soubory adaptÃ©ru: {missing_files}")
            print("ğŸ’¡ OÄekÃ¡vanÃ© soubory pro adaptÃ©r: adapter_config.json, adapter_model.safetensors")
            return
    else:
        required_files = ['config.json']
        model_files = ['pytorch_model.bin', 'model.safetensors']
        if not any(f in files_in_dir for f in model_files):
            print(f"âŒ ChybÃ­ soubory modelu: {model_files}")
            print("ğŸ’¡ OÄekÃ¡vanÃ© soubory pro model: config.json + pytorch_model.bin nebo model.safetensors")
            return
    
    if args.check_only:
        print("\nâœ… Kontrola dokonÄena")
        return
    
    # NahrÃ¡nÃ­ modelu nebo adaptÃ©ru
    if is_adapter:
        success = upload_adapter_only(model_path, args.hub_model_id, HF_TOKEN)
    else:
        success = upload_model_to_hf(model_path, args.hub_model_id, HF_TOKEN)
    
    if success:
        print(f"\nğŸ‰ {'LoRA adapter' if is_adapter else 'Model'} byl ÃºspÄ›Å¡nÄ› nahrÃ¡n na Hugging Face Hub!")
        print(f"ğŸ”— Odkaz: https://huggingface.co/{args.hub_model_id}")
        if is_adapter:
            print("\nğŸ’¡ Pro pouÅ¾itÃ­ adaptÃ©ru:")
            print(f"   from transformers import AutoModelForCausalLM, AutoTokenizer")
            print(f"   from peft import PeftModel")
            print(f"   base_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3')")
            print(f"   model = PeftModel.from_pretrained(base_model, '{args.hub_model_id}')")
        else:
            print("\nğŸ’¡ Pro pouÅ¾itÃ­ modelu:")
            print(f"   from transformers import AutoModelForCausalLM, AutoTokenizer")
            print(f"   model = AutoModelForCausalLM.from_pretrained('{args.hub_model_id}')")
            print(f"   tokenizer = AutoTokenizer.from_pretrained('{args.hub_model_id}')")
    else:
        print("\nâŒ NahrÃ¡vÃ¡nÃ­ selhalo")

if __name__ == "__main__":
    main() 