#!/usr/bin/env python3
"""
Skript pro testovÃ¡nÃ­ LoRA adaptÃ©ru s rÅ¯znÃ½mi modely
UmoÅ¾Åˆuje snadnÃ© pÅ™ipojenÃ­ adaptÃ©ru k jakÃ©mukoli kompatibilnÃ­mu modelu
"""

import argparse
import sys
import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import warnings

# PotlaÄenÃ­ varovÃ¡nÃ­
warnings.filterwarnings("ignore")

def load_adapter_config(adapter_path):
    """NaÄte konfiguraci adaptÃ©ru"""
    config_path = adapter_path.replace("/", "\\").replace("\\", "/") + "_config.json"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸ KonfiguraÄnÃ­ soubor nenalezen: {config_path}")
        return None

def load_model_with_adapter(base_model_name, adapter_path, device="auto"):
    """NaÄte model s pÅ™ipojenÃ½m adaptÃ©rem"""
    try:
        print(f"ğŸ¤– NaÄÃ­tÃ¡m zÃ¡kladnÃ­ model: {base_model_name}")
        print(f"ğŸ”§ PÅ™ipojuji adaptÃ©r: {adapter_path}")
        print("â³ ProsÃ­m poÄkejte, naÄÃ­tÃ¡nÃ­ mÅ¯Å¾e trvat nÄ›kolik minut...")
        
        # NaÄtenÃ­ tokenizeru
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_name,
            trust_remote_code=True
        )
        
        # Kontrola a nastavenÃ­ pad tokenu
        if tokenizer.pad_token is None:
            if tokenizer.eos_token:
                tokenizer.pad_token = tokenizer.eos_token
            else:
                tokenizer.add_special_tokens({"pad_token": "<pad>"})
        
        # NaÄtenÃ­ zÃ¡kladnÃ­ho modelu
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map=device,
            trust_remote_code=True
        )
        
        # Synchronizace pad tokenu s modelem
        if hasattr(model.config, 'pad_token_id'):
            model.config.pad_token_id = tokenizer.pad_token_id
        
        # NaÄtenÃ­ a pÅ™ipojenÃ­ adaptÃ©ru
        print("ğŸ”— PÅ™ipojuji LoRA adaptÃ©r...")
        model = PeftModel.from_pretrained(model, adapter_path)
        
        # PÅ™epnutÃ­ do inference mÃ³du
        model.eval()
        
        print("âœ… Model s adaptÃ©rem ÃºspÄ›Å¡nÄ› naÄten!")
        
        # ZobrazenÃ­ informacÃ­ o adaptÃ©ru
        config = load_adapter_config(adapter_path)
        if config:
            print(f"ğŸ“Š Informace o adaptÃ©ru:")
            print(f"   NÃ¡zev: {config.get('adapter_name', 'N/A')}")
            print(f"   LoRA rank: {config.get('lora_config', {}).get('r', 'N/A')}")
            print(f"   Target modules: {config.get('lora_config', {}).get('target_modules', 'N/A')}")
            print(f"   TrÃ©novÃ¡no na: {config.get('dataset_info', {}).get('total_samples', 'N/A')} vzorcÃ­ch")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ modelu s adaptÃ©rem: {e}")
        return None, None

def generate_response(model, tokenizer, prompt, max_length=512, temperature=0.7):
    """Generuje odpovÄ›Ä na zÃ¡kladÄ› promptu"""
    try:
        # Tokenizace vstupu
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        )
        
        # PÅ™esun na sprÃ¡vnÃ© zaÅ™Ã­zenÃ­
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # GenerovÃ¡nÃ­
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        # DekÃ³dovÃ¡nÃ­ odpovÄ›di
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # OdstranÄ›nÃ­ pÅ¯vodnÃ­ho promptu z odpovÄ›di
        if response.startswith(prompt):
            response = response[len(prompt):].strip()
        
        return response
        
    except Exception as e:
        return f"âŒ Chyba pÅ™i generovÃ¡nÃ­: {e}"

def test_adapter_compatibility(adapter_path):
    """Testuje kompatibilitu adaptÃ©ru s rÅ¯znÃ½mi modely"""
    print("ğŸ” Testuji kompatibilitu adaptÃ©ru...")
    
    # Seznam populÃ¡rnÃ­ch modelÅ¯ k testovÃ¡nÃ­
    test_models = [
        "microsoft/DialoGPT-medium",
        "microsoft/DialoGPT-large", 
        "gpt2",
        "gpt2-medium",
        "EleutherAI/gpt-neo-125M",
        "EleutherAI/gpt-neo-1.3B"
    ]
    
    config = load_adapter_config(adapter_path)
    if not config:
        print("âŒ Nelze naÄÃ­st konfiguraci adaptÃ©ru")
        return
    
    original_base_model = config.get('base_model', 'unknown')
    print(f"ğŸ“Š AdaptÃ©r byl trÃ©novÃ¡n na modelu: {original_base_model}")
    
    print(f"\nğŸ§ª Testuji kompatibilitu s rÅ¯znÃ½mi modely:")
    
    for model_name in test_models:
        print(f"\nğŸ”¬ Testuji: {model_name}")
        try:
            # RychlÃ½ test naÄtenÃ­
            model, tokenizer = load_model_with_adapter(model_name, adapter_path, device="cpu")
            if model and tokenizer:
                print(f"âœ… KompatibilnÃ­ s {model_name}")
                
                # RychlÃ½ test generovÃ¡nÃ­
                test_prompt = "Jak se mÃ¡Å¡?"
                response = generate_response(model, tokenizer, test_prompt, max_length=50, temperature=0.7)
                print(f"   Test odpovÄ›Ä: {response[:100]}...")
                
                # UvolnÄ›nÃ­ pamÄ›ti
                del model, tokenizer
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
            else:
                print(f"âŒ NekompatibilnÃ­ s {model_name}")
                
        except Exception as e:
            print(f"âŒ Chyba s {model_name}: {str(e)[:100]}...")

def main():
    parser = argparse.ArgumentParser(
        description="TestovÃ¡nÃ­ LoRA adaptÃ©ru s rÅ¯znÃ½mi modely",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
PÅ™Ã­klady pouÅ¾itÃ­:
  python test_adapter.py --base-model microsoft/DialoGPT-medium --adapter ./adapters/babis_adapter
  python test_adapter.py --base-model gpt2 --adapter ./adapters/babis_adapter
  python test_adapter.py --adapter ./adapters/babis_adapter --test-compatibility
        """
    )
    
    parser.add_argument(
        "--base-model",
        help="ZÃ¡kladnÃ­ model (pokud nenÃ­ specifikovÃ¡n, pouÅ¾ije se model z konfigurace adaptÃ©ru)"
    )
    
    parser.add_argument(
        "--adapter",
        required=True,
        help="Cesta k LoRA adaptÃ©ru"
    )
    
    parser.add_argument(
        "--device",
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="ZaÅ™Ã­zenÃ­ pro inference (default: auto)"
    )
    
    parser.add_argument(
        "--max-length",
        type=int,
        default=512,
        help="MaximÃ¡lnÃ­ dÃ©lka generovanÃ© odpovÄ›di (default: 512)"
    )
    
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Teplota pro generovÃ¡nÃ­ (default: 0.7)"
    )
    
    parser.add_argument(
        "--test-compatibility",
        action="store_true",
        help="Testuje kompatibilitu adaptÃ©ru s rÅ¯znÃ½mi modely"
    )
    
    args = parser.parse_args()
    
    if args.test_compatibility:
        test_adapter_compatibility(args.adapter)
        return
    
    # UrÄenÃ­ zÃ¡kladnÃ­ho modelu
    base_model = args.base_model
    if not base_model:
        config = load_adapter_config(args.adapter)
        if config:
            base_model = config.get('base_model')
            print(f"ğŸ“Š PouÅ¾Ã­vÃ¡m zÃ¡kladnÃ­ model z konfigurace: {base_model}")
        else:
            print("âŒ MusÃ­te specifikovat --base-model nebo mÃ­t konfiguraÄnÃ­ soubor")
            sys.exit(1)
    
    # NaÄtenÃ­ modelu s adaptÃ©rem
    model, tokenizer = load_model_with_adapter(base_model, args.adapter, args.device)
    
    if model is None or tokenizer is None:
        sys.exit(1)
    
    print("\n" + "="*60)
    print("ğŸ’¬ INTERAKTIVNÃ TESTOVÃNÃ ADAPTÃ‰RU")
    print("="*60)
    print("ğŸ“ NapiÅ¡te svÅ¯j dotaz a stisknÄ›te Enter")
    print("ğŸ”§ Pro ukonÄenÃ­ stisknÄ›te Ctrl+C")
    print("="*60)
    
    # NekoneÄnÃ¡ smyÄka pro dotazovÃ¡nÃ­
    while True:
        try:
            # Vstup uÅ¾ivatele
            user_input = input("\nğŸ‘¤ Vy: ").strip()
            
            # PrÃ¡zdnÃ½ vstup
            if not user_input:
                continue
            
            # GenerovÃ¡nÃ­ odpovÄ›di
            print("ğŸ¤– Model s adaptÃ©rem generuje odpovÄ›Ä...")
            response = generate_response(
                model, tokenizer, user_input,
                args.max_length, args.temperature
            )
            
            print(f"ğŸ¤– Model: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Na shledanou!")
            break
        except Exception as e:
            print(f"âŒ Chyba: {e}")

if __name__ == "__main__":
    main() 