import torch
import os
import subprocess

def generate_response(model, tokenizer, prompt, max_length=200):
    """Generuje odpovƒõƒè pomoc√≠ modelu (p≈Øvodn√≠ho nebo fine-tunovan√©ho)"""
    # Kontrola, zda tokenizer podporuje apply_chat_template
    if hasattr(tokenizer, 'apply_chat_template'):
        # Pou≈æijeme apply_chat_template pro spr√°vn√© form√°tov√°n√≠
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    else:
        # Fallback pro tokenizery bez apply_chat_template
        formatted_prompt = prompt
    
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Vylep≈°en√© odstranƒõn√≠ p≈Øvodn√≠ho promptu z odpovƒõdi
    if hasattr(tokenizer, 'apply_chat_template'):
        # Pro chat template - hled√°me konec assistant tagu
        assistant_start = response.find("<|assistant|>")
        if assistant_start != -1:
            # Najdeme konec assistant tagu a zaƒç√°tek odpovƒõdi
            response_start = assistant_start + len("<|assistant|>")
            response = response[response_start:].strip()
        else:
            # Fallback - odstran√≠me formatted_prompt pokud je na zaƒç√°tku
            if response.startswith(formatted_prompt):
                response = response[len(formatted_prompt):].strip()
    else:
        # Pro bƒõ≈æn√© prompty - odstran√≠me p≈Øvodn√≠ prompt
        if response.startswith(formatted_prompt):
            response = response[len(formatted_prompt):].strip()
    
    # Dal≈°√≠ cleanup - odstranƒõn√≠ mo≈æn√Ωch zbytk≈Ø promptu
    # Hled√°me bƒõ≈æn√© vzory, kter√© by mohly z≈Østat
    cleanup_patterns = [
        prompt,  # P≈Øvodn√≠ prompt
        f"User: {prompt}",  # S User prefixem
        f"Human: {prompt}",  # S Human prefixem
        f"<|user|>\n{prompt}",  # S user tagem
    ]
    
    for pattern in cleanup_patterns:
        if response.startswith(pattern):
            response = response[len(pattern):].strip()
            break
    
    # Odstranƒõn√≠ pr√°zdn√Ωch ≈ô√°dk≈Ø na zaƒç√°tku
    response = response.lstrip('\n').strip()
    
    return response

def test_model(model, tokenizer, test_prompts=None):
    """Otestuje model (p≈Øvodn√≠ nebo fine-tunovan√Ω) na testovac√≠ch promptech"""
    if test_prompts is None:
        test_prompts = [
            "Pane Babi≈°i, jak hodnot√≠te souƒçasnou inflaci?",
            "Co si mysl√≠te o opozici?",
            "Jak se v√°m da≈ô√≠ v Bruselu?"
        ]
    
    print("\nüìù Testovac√≠ odpovƒõdi:")
    print("=" * 50)
    
    # Kontrola, zda tokenizer podporuje apply_chat_template
    if hasattr(tokenizer, 'apply_chat_template'):
        print("‚úÖ Pou≈æ√≠v√° apply_chat_template pro form√°tov√°n√≠ prompt≈Ø")
    else:
        print("‚ö†Ô∏è Tokenizer nepodporuje apply_chat_template, pou≈æ√≠v√° se p≈ô√≠m√© form√°tov√°n√≠")
    
    for prompt in test_prompts:
        print(f"\nüîç Prompt: {prompt}")
        
        # Debug: Zobrazen√≠ formatted promptu
        if hasattr(tokenizer, 'apply_chat_template'):
            messages = [{"role": "user", "content": prompt}]
            formatted_prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            print(f"üîß Formatted prompt: {formatted_prompt[:100]}...")
        
        response = generate_response(model, tokenizer, prompt)
        print(f"üé≠ Odpovƒõƒè: {response}")
        print("-" * 30)
    
    return test_prompts

def save_model_info(model_path, output_dir):
    """Ulo≈æ√≠ informace o modelu a vytvo≈ô√≠ shrnut√≠"""
    print(f"\nüíæ Ukl√°d√°m model na network storage...")
    final_model_path = f"{output_dir}-final"
    
    # Vytvo≈ôen√≠ adres√°≈ôe pokud neexistuje
    os.makedirs(final_model_path, exist_ok=True)
    
    # V√Ωpis velikosti ulo≈æen√©ho modelu
    try:
        result = subprocess.run(['du', '-sh', final_model_path], capture_output=True, text=True)
        if result.stdout:
            print(f"üìä Velikost modelu: {result.stdout.strip()}")
    except:
        pass
    
    # V√Ωpis informac√≠ o ulo≈æen√Ωch souborech
    print(f"\nüìã Ulo≈æen√© soubory:")
    try:
        for root, dirs, files in os.walk(final_model_path):
            level = root.replace(final_model_path, '').count(os.sep)
            indent = ' ' * 2 * level
            print(f"{indent}{os.path.basename(root)}/")
            subindent = ' ' * 2 * (level + 1)
            for file in files[:5]:  # Zobraz√≠me pouze prvn√≠ch 5 soubor≈Ø
                print(f"{subindent}{file}")
            if len(files) > 5:
                print(f"{subindent}... a dal≈°√≠ch {len(files) - 5} soubor≈Ø")
    except Exception as e:
        print(f"‚ö†Ô∏è Nelze zobrazit seznam soubor≈Ø: {e}")
    
    return final_model_path 