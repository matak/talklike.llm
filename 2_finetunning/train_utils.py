import torch
import os
import subprocess

def generate_response(model, tokenizer, prompt, max_length=200):
    """Generuje odpovƒõƒè pomoc√≠ modelu (p≈Øvodn√≠ho nebo fine-tunovan√©ho)"""
    # Kontrola, zda tokenizer podporuje apply_chat_template
    if hasattr(tokenizer, 'apply_chat_template'):
        # Pou≈æijeme apply_chat_template pro spr√°vn√© form√°tov√°n√≠
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    else:
        # Fallback pro tokenizery bez apply_chat_template
        formatted_prompt = prompt
    
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

def test_model(model, tokenizer, test_prompts=None):
    """Otestuje model (p≈Øvodn√≠ nebo fine-tunovan√Ω) na testovac√≠ch promptech"""
    if test_prompts is None:
        test_prompts = [
            "Pane Babi≈°i, jak hodnot√≠te souƒçasnou inflaci?",
            "Co si mysl√≠te o opozici?",
            "Jak se v√°m da≈ô√≠ v Bruselu?"
        ]
    
    print("\nüìù Testovac√≠ odpovƒõdi:")
    print("=" * 50)
    
    # Kontrola, zda tokenizer podporuje apply_chat_template
    if hasattr(tokenizer, 'apply_chat_template'):
        print("‚úÖ Pou≈æ√≠v√° apply_chat_template pro form√°tov√°n√≠ prompt≈Ø")
    else:
        print("‚ö†Ô∏è Tokenizer nepodporuje apply_chat_template, pou≈æ√≠v√° se p≈ô√≠m√© form√°tov√°n√≠")
    
    for prompt in test_prompts:
        print(f"\nPrompt: {prompt}")
        response = generate_response(model, tokenizer, prompt)
        print(f"Odpovƒõƒè: {response}")
        print("-" * 30)
    
    return test_prompts

def save_model_info(model_path, output_dir):
    """Ulo≈æ√≠ informace o modelu a vytvo≈ô√≠ shrnut√≠"""
    print(f"\nüíæ Ukl√°d√°m model na network storage...")
    final_model_path = f"{output_dir}-final"
    
    # Vytvo≈ôen√≠ adres√°≈ôe pokud neexistuje
    os.makedirs(final_model_path, exist_ok=True)
    
    # V√Ωpis velikosti ulo≈æen√©ho modelu
    try:
        result = subprocess.run(['du', '-sh', final_model_path], capture_output=True, text=True)
        if result.stdout:
            print(f"üìä Velikost modelu: {result.stdout.strip()}")
    except:
        pass
    
    # V√Ωpis informac√≠ o ulo≈æen√Ωch souborech
    print(f"\nüìã Ulo≈æen√© soubory:")
    try:
        for root, dirs, files in os.walk(final_model_path):
            level = root.replace(final_model_path, '').count(os.sep)
            indent = ' ' * 2 * level
            print(f"{indent}{os.path.basename(root)}/")
            subindent = ' ' * 2 * (level + 1)
            for file in files[:5]:  # Zobraz√≠me pouze prvn√≠ch 5 soubor≈Ø
                print(f"{subindent}{file}")
            if len(files) > 5:
                print(f"{subindent}... a dal≈°√≠ch {len(files) - 5} soubor≈Ø")
    except Exception as e:
        print(f"‚ö†Ô∏è Nelze zobrazit seznam soubor≈Ø: {e}")
    
    return final_model_path 