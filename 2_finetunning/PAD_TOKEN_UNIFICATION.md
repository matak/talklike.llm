# SjednocenÃ­ pad_token Å™eÅ¡enÃ­

## ğŸ¯ CÃ­l

Sjednotit Å™eÅ¡enÃ­ pad_token problÃ©mu napÅ™Ã­Ä celÃ½m projektem pomocÃ­ centralizovanÃ© funkce s vylepÅ¡enÃ½mi debug informacemi.

## âœ… ImplementovanÃ© zmÄ›ny

### 1. **VylepÅ¡enÃ¡ funkce `setup_tokenizer_and_model()` v `tokenizer_utils.py`**

PÅ™idÃ¡ny debug informace podle nÃ¡vrhu uÅ¾ivatele:

```python
def setup_tokenizer_and_model(model_name, base_model):
    """NastavÃ­ tokenizer a model pro fine-tuning"""
    
    # Debug informace o modelu
    print("Input embeddings: ", base_model.get_input_embeddings())
    print("Output embeddings: ", base_model.get_output_embeddings())
    print("Model Vocabulary Size: ", base_model.config.vocab_size)
    
    # 1. NaÄtenÃ­ tokenizeru
    base_tokenizer = AutoTokenizer.from_pretrained(model_name)
    print("Before add token to tokenizer - tokenizer length: ", len(base_tokenizer))
    
    # 2. Kontrola a pÅ™idÃ¡nÃ­ pad tokenu
    if base_tokenizer.pad_token is None:
        if base_tokenizer.eos_token:
            base_tokenizer.pad_token = base_tokenizer.eos_token
            print(f"âœ… PouÅ¾Ã­vÃ¡m EOS token jako PAD: {base_tokenizer.pad_token}")
        else:
            base_tokenizer.add_special_tokens({"pad_token": "<pad>"})
            print(f"âœ… PÅ™idÃ¡n novÃ½ pad token: {base_tokenizer.pad_token}")
            
            # DÅ¯leÅ¾itÃ©: Resize model embeddings
            base_model.resize_token_embeddings(len(base_tokenizer))
            print(f"ğŸ“Š Model embeddings resized na: {len(base_tokenizer)}")
    else:
        print(f"â„¹ï¸ Pad token uÅ¾ existuje: {base_tokenizer.pad_token}")
    
    print("After add token to tokenizer - tokenizer length: ", len(base_tokenizer))
    
    # 3. Synchronizace s modelem
    print("Before add pad token to model - pad token Id: ", base_model.config.pad_token_id)
    if hasattr(base_model.config, 'pad_token_id'):
        old_pad_id = base_model.config.pad_token_id
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
        print(f"ğŸ”„ Pad token ID zmÄ›nÄ›n: {old_pad_id} â†’ {base_model.config.pad_token_id}")
    else:
        print("âš ï¸ Model nemÃ¡ pad_token_id v config")
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
    
    print("After add pad token to model - pad token Id: ", base_model.config.pad_token_id)
    
    # 4. Kontrola konzistence
    try:
        assert base_tokenizer.pad_token_id == base_model.config.pad_token_id, \
            "Tokenizer a model majÃ­ rÅ¯znÃ© pad token ID!"
        print(f"âœ… Tokenizer a model synchronizovÃ¡ny")
    except AssertionError as e:
        print(f"âŒ Chyba synchronizace: {e}")
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
        print(f"ğŸ”§ Opraveno: pad_token_id nastaven na {base_tokenizer.pad_token_id}")
    
    return base_tokenizer, base_model
```

### 2. **SjednocenÃ­ napÅ™Ã­Ä soubory**

#### âœ… **AktualizovanÃ© soubory:**

- **`tokenizer_utils.py`** - vylepÅ¡enÃ¡ hlavnÃ­ funkce
- **`test_tokenization.py`** - pouÅ¾Ã­vÃ¡ stejnou vylepÅ¡enou logiku
- **`test_model.py`** - importuje centralizovanou funkci
- **`test_adapter.py`** - importuje centralizovanou funkci
- **`create_qlora_adapter.py`** - importuje centralizovanou funkci
- **`generate_responses.py`** - importuje centralizovanou funkci
- **`train_utils.py`** - opraveno pouÅ¾itÃ­ pad_token_id

#### ğŸ”„ **OdstranÄ›nÃ¡ duplikace:**

- âŒ DuplikovanÃ¡ logika nastavenÃ­ pad_tokenu
- âŒ RÅ¯znÃ© pÅ™Ã­stupy v rÅ¯znÃ½ch souborech
- âŒ InkonzistentnÃ­ debug vÃ½pisy

### 3. **VylepÅ¡enÃ­ v `train_utils.py`**

Opraveno pouÅ¾itÃ­ sprÃ¡vnÃ©ho pad_token_id:

```python
# PÅ˜ED:
pad_token_id=tokenizer.eos_token_id

# PO:
pad_token_id=tokenizer.pad_token_id
```

### 4. **Oprava DataCollatorForLanguageModeling**

OdstranÄ›n nepodporovanÃ½ `padding` parametr:

```python
# PÅ˜ED (âŒ Chyba):
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    return_tensors="pt",
    pad_to_multiple_of=8,
    padding=True,  # âŒ NeexistujÃ­cÃ­ parametr
)

# PO (âœ… SprÃ¡vnÄ›):
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    return_tensors="pt",
    pad_to_multiple_of=8,
    # padding=True odstranÄ›no
)
```

### 5. **Oprava data collator testu**

PÅ™idÃ¡n manuÃ¡lnÃ­ padding pro testovÃ¡nÃ­:

```python
# PÅ˜ED (âŒ Chyba):
test_batch = data_collator([tokenized_dataset[0], tokenized_dataset[1]])

# PO (âœ… SprÃ¡vnÄ›):
# PÅ™idÃ¡me padding manuÃ¡lnÄ› pro test
padded_samples = []
max_length = max(len(sample['input_ids']) for sample in [tokenized_dataset[0], tokenized_dataset[1]])

for sample in [tokenized_dataset[0], tokenized_dataset[1]]:
    # Padding na nejdelÅ¡Ã­ sekvenci
    padding_length = max_length - len(sample['input_ids'])
    padded_sample = {
        'input_ids': sample['input_ids'] + [tokenizer.pad_token_id] * padding_length,
        'attention_mask': sample['attention_mask'] + [0] * padding_length,
        'labels': sample['labels'] + [-100] * padding_length  # -100 pro padding tokeny
    }
    padded_samples.append(padded_sample)

test_batch = data_collator(padded_samples)
```

### 6. **Oprava importÅ¯**

PÅ™idÃ¡na sprÃ¡vnÃ¡ cesta pro import `setup_environment`:

```python
# PÅ˜ED (âŒ Chyba):
import setup_environment

# PO (âœ… SprÃ¡vnÄ›):
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import setup_environment
```

## ğŸ¯ VÃ½hody sjednocenÃ­

### 1. **Konzistence**
- VÅ¡echny soubory pouÅ¾Ã­vajÃ­ stejnou logiku
- StejnÃ© debug informace ve vÅ¡ech ÄÃ¡stech projektu
- JednotnÃ½ pÅ™Ã­stup k Å™eÅ¡enÃ­ pad_token problÃ©mu

### 2. **LepÅ¡Ã­ diagnostika**
- Debug informace o embeddings
- Kontrola dÃ©lky tokenizeru pÅ™ed a po pÅ™idÃ¡nÃ­ tokenÅ¯
- Kontrola pad_token_id pÅ™ed a po nastavenÃ­
- ExplicitnÃ­ informace o zmÄ›nÃ¡ch

### 3. **ÃšdrÅ¾ba**
- Jedna funkce pro vÅ¡echny Ãºpravy
- SnadnÃ© pÅ™idÃ¡nÃ­ novÃ½ch debug informacÃ­
- CentralizovanÃ© Å™eÅ¡enÃ­ problÃ©mÅ¯

### 4. **Spolehlivost**
- OvÄ›Å™enÃ¡ logika v jednom mÃ­stÄ›
- KonzistentnÃ­ chovÃ¡nÃ­ napÅ™Ã­Ä projektem
- LepÅ¡Ã­ error handling

### 5. **PÅ™echod na Mistral model**
- âœ… **Test tokenizace** - pouÅ¾Ã­vÃ¡ Mistral-7B-Instruct-v0.3
- âœ… **Fine-tuning** - vÃ½chozÃ­ model zmÄ›nÄ›n na Mistral
- âœ… **ChatML formÃ¡t** - testovacÃ­ data v sprÃ¡vnÃ©m formÃ¡tu pro Mistral
- âœ… **Target modules** - automatickÃ¡ detekce pro Mistral architekturu

## ğŸ§ª TestovÃ¡nÃ­

Pro ovÄ›Å™enÃ­ sjednocenÃ­ spusÅ¥te:

```bash
# Test tokenizace s Mistral modelem
python test_tokenization.py

# Test modelu
python test_model.py mistralai/Mistral-7B-Instruct-v0.3

# Test adaptÃ©ru
python test_adapter.py --base-model mistralai/Mistral-7B-Instruct-v0.3 --adapter path/to/adapter

# Test fine-tuningu
python finetune.py --model_name mistralai/Mistral-7B-Instruct-v0.3
```

VÅ¡echny tyto testy by mÄ›ly zobrazovat stejnÃ© debug informace o pad_tokenu.

**PoznÃ¡mka:** Test pouÅ¾Ã­vÃ¡ Mistral model, kterÃ½ je primÃ¡rnÃ­m cÃ­lem projektu pro fine-tuning Andreje BabiÅ¡e.

## ğŸ“‹ KontrolnÃ­ seznam

- âœ… VylepÅ¡enÃ¡ funkce `setup_tokenizer_and_model()`
- âœ… SjednocenÃ­ napÅ™Ã­Ä vÅ¡emi soubory
- âœ… Oprava `train_utils.py`
- âœ… Oprava `DataCollatorForLanguageModeling`
- âœ… Oprava data collator testu
- âœ… Oprava importÅ¯ `setup_environment`
- âœ… Debug informace podle nÃ¡vrhu uÅ¾ivatele
- âœ… OdstranÄ›nÃ­ duplikace kÃ³du
- âœ… KonzistentnÃ­ chovÃ¡nÃ­

## ğŸš€ DalÅ¡Ã­ vylepÅ¡enÃ­

Pro budoucÃ­ rozvoj doporuÄuji:

1. **PÅ™idÃ¡nÃ­ vÃ­ce debug informacÃ­** - napÅ™. kontrola attention mask
2. **LogovÃ¡nÃ­** - uklÃ¡dÃ¡nÃ­ debug informacÃ­ do souborÅ¯
3. **Testy** - automatickÃ© testy pro ovÄ›Å™enÃ­ sprÃ¡vnosti nastavenÃ­
4. **Dokumentace** - podrobnÄ›jÅ¡Ã­ popis kaÅ¾dÃ©ho kroku

## ğŸ“ ZÃ¡vÄ›r

SjednocenÃ­ pad_token Å™eÅ¡enÃ­ bylo ÃºspÄ›Å¡nÄ› implementovÃ¡no. VÅ¡echny soubory nynÃ­ pouÅ¾Ã­vajÃ­ centralizovanou funkci s vylepÅ¡enÃ½mi debug informacemi, coÅ¾ zajiÅ¡Å¥uje konzistentnÃ­ a spolehlivÃ© chovÃ¡nÃ­ napÅ™Ã­Ä celÃ½m projektem. 