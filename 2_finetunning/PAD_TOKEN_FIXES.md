# Opravy pad_token pro Fine-tuning

## ProblÃ©m

PÅ™i analÃ½ze kÃ³du jsem identifikoval **duplicitnÃ­ padding**, kterÃ½ mohl zpÅ¯sobovat problÃ©my pÅ™i fine-tuningu:

### âŒ PÅ¯vodnÃ­ problÃ©my:

1. **DuplicitnÃ­ padding** - Padding se aplikoval dvakrÃ¡t:
   - V `tokenize_function()` s `padding=True`
   - Znovu v manuÃ¡lnÃ­ `fix_padding()` funkci

2. **NekonzistentnÃ­ pÅ™Ã­stup** - AutomatickÃ½ padding v tokenizeru + manuÃ¡lnÃ­ oprava

3. **PotenciÃ¡lnÃ­ konflikt** s `DataCollatorForLanguageModeling`

## âœ… Å˜eÅ¡enÃ­

### 1. OpravenÃ© soubory:

- `finetune_babis.py` - hlavnÃ­ fine-tuning script
- `tokenizer_utils.py` - tokenizace funkce
- `test_tokenization.py` - test soubor
- `create_qlora_adapter.py` - QLoRA adaptÃ©r

### 2. KonkrÃ©tnÃ­ zmÄ›ny:

#### A) `tokenize_function()` - odstranÄ›nÃ­ padding=True
```python
# PÅ˜ED:
tokenized = tokenizer(
    examples["text"],
    truncation=True,
    padding=True,  # âŒ DuplicitnÃ­ padding
    max_length=max_length,
    return_tensors=None
)

# PO:
tokenized = tokenizer(
    examples["text"],
    truncation=True,
    padding=False,  # âœ… Padding se Å™eÅ¡Ã­ v DataCollator
    max_length=max_length,
    return_tensors=None
)
```

#### B) OdstranÄ›nÃ­ manuÃ¡lnÃ­ `fix_padding()` funkce
```python
# âŒ ODSTRANÄšNO:
def fix_padding(example):
    # ManuÃ¡lnÃ­ padding logika
    ...

tokenized_dataset = tokenized_dataset.map(fix_padding)
```

#### C) VylepÅ¡enÃ­ DataCollator konfigurace
```python
# âœ… VYLEPÅ ENO:
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    return_tensors="pt",
    pad_to_multiple_of=8,
    padding=True,  # ExplicitnÄ› povolÃ­me padding
)
```

## ğŸ¯ VÃ½hody oprav:

1. **JednotnÃ½ padding** - pouze v DataCollator
2. **LepÅ¡Ã­ vÃ½kon** - mÃ©nÄ› duplicitnÃ­ch operacÃ­
3. **KonzistentnÃ­ chovÃ¡nÃ­** - standardnÃ­ Hugging Face pÅ™Ã­stup
4. **SprÃ¡vnÃ© labels** - padding tokeny majÃ­ label -100 (ignorovÃ¡ny v loss)

## ğŸ”§ Jak to funguje nynÃ­:

1. **Tokenizace** - bez padding, pouze truncation
2. **DataCollator** - automaticky pÅ™idÃ¡ padding na nejdelÅ¡Ã­ sekvenci v batch
3. **Labels** - padding tokeny automaticky dostanou label -100
4. **Attention mask** - sprÃ¡vnÄ› nastavena pro padding

## âœ… SprÃ¡vnÃ© nastavenÃ­ pad_token zÅ¯stÃ¡vÃ¡:

```python
# V setup_tokenizer_and_model():
if base_tokenizer.pad_token is None:
    if base_tokenizer.eos_token:
        base_tokenizer.pad_token = base_tokenizer.eos_token
    else:
        base_tokenizer.add_special_tokens({"pad_token": "<pad>"})
        base_model.resize_token_embeddings(len(base_tokenizer))
```

Toto nastavenÃ­ je sprÃ¡vnÃ© a zÅ¯stÃ¡vÃ¡ beze zmÄ›ny.

## ğŸ§ª TestovÃ¡nÃ­:

SpusÅ¥te `test_tokenization.py` pro ovÄ›Å™enÃ­, Å¾e padding funguje sprÃ¡vnÄ›:

```bash
python test_tokenization.py
```

MÄ›li byste vidÄ›t:
- âœ… SprÃ¡vnÃ© dÃ©lky tokenÅ¯
- âœ… Data collator test ÃºspÄ›Å¡nÃ½
- âœ… KonzistentnÃ­ padding 