#!/usr/bin/env python3
"""
Test script pro ovƒõ≈ôen√≠ apply_chat_template funkƒçnosti
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import setup_environment

from transformers import AutoTokenizer
from data_utils import load_babis_data, prepare_training_data
from tokenizer_utils import tokenize_function

def test_apply_chat_template():
    """Testuje apply_chat_template s r≈Øzn√Ωmi modely"""
    
    # Testovac√≠ modely
    test_models = [
        "mistralai/Mistral-7B-Instruct-v0.3",
        "microsoft/DialoGPT-medium",
        "meta-llama/Llama-2-7b-chat-hf"
    ]
    
    # Naƒçten√≠ testovac√≠ch dat
    print("üìä Naƒç√≠t√°m testovac√≠ data...")
    conversations = load_babis_data("data/all.jsonl")
    print(f"‚úÖ Naƒçteno {len(conversations)} konverzac√≠")
    
    # Test s ka≈æd√Ωm modelem
    for model_name in test_models:
        print(f"\nüîß Testuji model: {model_name}")
        print("=" * 60)
        
        try:
            # Naƒçten√≠ tokenizeru
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir='/workspace/.cache/huggingface/transformers',
                local_files_only=False,
                resume_download=True,
                force_download=False
            )
            
            # Kontrola, zda tokenizer podporuje apply_chat_template
            if not hasattr(tokenizer, 'apply_chat_template'):
                print(f"‚ö†Ô∏è Tokenizer pro {model_name} nepodporuje apply_chat_template")
                continue
            
            print(f"‚úÖ Tokenizer naƒçten, vocab size: {len(tokenizer)}")
            
            # P≈ô√≠prava dat
            training_data = prepare_training_data(conversations[:5], model_name=model_name)  # Pouze prvn√≠ch 5 pro test
            print(f"‚úÖ P≈ôipraveno {len(training_data)} vzork≈Ø")
            
            # Test tokenizace
            if len(training_data) > 0:
                # Test prvn√≠ho vzorku
                first_sample = training_data[0]
                print(f"üìù Testuji prvn√≠ vzorek:")
                print(f"   Typ dat: {type(first_sample)}")
                print(f"   Kl√≠ƒçe: {list(first_sample.keys())}")
                
                if "messages" in first_sample:
                    messages = first_sample["messages"]
                    print(f"   Poƒçet zpr√°v: {len(messages)}")
                    
                    # Test apply_chat_template
                    try:
                        formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                        print(f"‚úÖ apply_chat_template √∫spƒõ≈°n√Ω")
                        print(f"   Form√°tovan√Ω text (prvn√≠ch 200 znak≈Ø): {formatted_text[:200]}...")
                        
                        # Test tokenizace
                        tokenized = tokenizer(formatted_text, return_tensors="pt")
                        print(f"‚úÖ Tokenizace √∫spƒõ≈°n√°, d√©lka: {tokenized['input_ids'].shape[1]}")
                        
                    except Exception as e:
                        print(f"‚ùå Chyba p≈ôi apply_chat_template: {e}")
                
                # Test batch tokenizace
                try:
                    tokenized_batch = tokenize_function({"messages": [item["messages"] for item in training_data]}, tokenizer, max_length=512)
                    print(f"‚úÖ Batch tokenizace √∫spƒõ≈°n√°, {len(tokenized_batch['input_ids'])} vzork≈Ø")
                except Exception as e:
                    print(f"‚ùå Chyba p≈ôi batch tokenizaci: {e}")
            
        except Exception as e:
            print(f"‚ùå Chyba p≈ôi testov√°n√≠ {model_name}: {e}")
        
        print()

if __name__ == "__main__":
    test_apply_chat_template() 