#!/usr/bin/env python3
"""
Fine-tuning script pro model s daty Andreje Babi≈°e
Spustiteln√Ω na RunPod.io nebo lok√°lnƒõ
"""

# Import setup_environment pro spr√°vn√© nastaven√≠ prost≈ôed√≠
import setup_environment

import os
import json
import torch
import numpy as np
import shutil
from datetime import datetime
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
from dotenv import load_dotenv
from huggingface_hub import login
import wandb
import argparse

# Import disk manager knihovny
from lib.disk_manager import DiskManager, setup_for_ml_project, check_and_cleanup

class DatasetDebugger:
    """T≈ô√≠da pro debugov√°n√≠ a ukl√°d√°n√≠ mezikrok≈Ø zpracov√°n√≠ datasetu"""
    
    def __init__(self, debug_dir="debug_dataset"):
        self.debug_dir = debug_dir
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.debug_dir = f"{debug_dir}_{self.timestamp}"
        
        # Vytvo≈ôen√≠ debug adres√°≈ôe
        os.makedirs(self.debug_dir, exist_ok=True)
        print(f"üîç Debug adres√°≈ô vytvo≈ôen: {self.debug_dir}")
    
    def save_step(self, step_name, data, description=""):
        """Ulo≈æ√≠ krok zpracov√°n√≠ datasetu"""
        step_file = os.path.join(self.debug_dir, f"step_{step_name}.json")
        
        # P≈ôid√°n√≠ metadat
        debug_info = {
            "step_name": step_name,
            "timestamp": datetime.now().isoformat(),
            "description": description,
            "data_type": type(data).__name__,
            "data_count": len(data) if hasattr(data, '__len__') else "N/A"
        }
        
        # Ulo≈æen√≠ dat podle typu
        if isinstance(data, list):
            if len(data) > 0 and isinstance(data[0], dict):
                # Seznam slovn√≠k≈Ø - ulo≈æ√≠me jako JSON
                debug_info["data"] = data
            else:
                # Jin√Ω typ dat - ulo≈æ√≠me jako text
                debug_info["data"] = [str(item) for item in data]
        elif isinstance(data, dict):
            debug_info["data"] = data
        else:
            debug_info["data"] = str(data)
        
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Ulo≈æen debug krok: {step_name} -> {step_file}")
        
        # Vytvo≈ôen√≠ tak√© ƒçiteln√© verze pro prvn√≠ 3 polo≈æky
        if isinstance(data, list) and len(data) > 0:
            readable_file = os.path.join(self.debug_dir, f"step_{step_name}_readable.txt")
            with open(readable_file, 'w', encoding='utf-8') as f:
                f.write(f"Debug krok: {step_name}\n")
                f.write(f"ƒåas: {debug_info['timestamp']}\n")
                f.write(f"Popis: {description}\n")
                f.write(f"Poƒçet polo≈æek: {len(data)}\n")
                f.write("-" * 80 + "\n\n")
                
                for i, item in enumerate(data[:3]):  # Prvn√≠ 3 polo≈æky
                    f.write(f"Polo≈æka {i+1}:\n")
                    if isinstance(item, dict):
                        for key, value in item.items():
                            if key == 'content' and isinstance(value, str) and len(value) > 200:
                                f.write(f"  {key}: {value[:200]}...\n")
                            else:
                                f.write(f"  {key}: {value}\n")
                    else:
                        f.write(f"  {item}\n")
                    f.write("\n")
                
                if len(data) > 3:
                    f.write(f"... a dal≈°√≠ch {len(data) - 3} polo≈æek\n")
    
    def save_sample(self, step_name, sample_data, sample_index=0):
        """Ulo≈æ√≠ uk√°zkovou polo≈æku z datasetu"""
        sample_file = os.path.join(self.debug_dir, f"sample_{step_name}_{sample_index}.json")
        
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        
        print(f"üìù Ulo≈æena uk√°zka: {step_name} -> {sample_file}")
    
    def create_summary(self):
        """Vytvo≈ô√≠ shrnut√≠ v≈°ech debug krok≈Ø"""
        summary_file = os.path.join(self.debug_dir, "debug_summary.txt")
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("DEBUG SHRNUT√ç ZPRACOV√ÅN√ç DATASETU\n")
            f.write("=" * 50 + "\n")
            f.write(f"ƒåas vytvo≈ôen√≠: {self.timestamp}\n")
            f.write(f"Debug adres√°≈ô: {self.debug_dir}\n\n")
            
            # Najdeme v≈°echny debug soubory
            debug_files = [f for f in os.listdir(self.debug_dir) if f.startswith("step_") and f.endswith(".json")]
            debug_files.sort()
            
            for debug_file in debug_files:
                try:
                    with open(os.path.join(self.debug_dir, debug_file), 'r', encoding='utf-8') as df:
                        debug_info = json.load(df)
                    
                    f.write(f"Krok: {debug_info['step_name']}\n")
                    f.write(f"  ƒåas: {debug_info['timestamp']}\n")
                    f.write(f"  Popis: {debug_info['description']}\n")
                    f.write(f"  Typ dat: {debug_info['data_type']}\n")
                    f.write(f"  Poƒçet: {debug_info['data_count']}\n")
                    f.write("\n")
                except Exception as e:
                    f.write(f"Chyba p≈ôi ƒçten√≠ {debug_file}: {e}\n\n")
        
        print(f"üìã Vytvo≈ôeno shrnut√≠: {summary_file}")

def load_babis_data(file_path, debugger=None):
    """Naƒçte data z JSONL souboru nebo jednoho velk√©ho JSON objektu"""
    conversations = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read().strip()
    
    # Debug: Ulo≈æen√≠ p≈Øvodn√≠ho obsahu
    if debugger:
        debugger.save_step("01_original_content", {"content": content[:1000] + "..." if len(content) > 1000 else content}, 
                          "P≈Øvodn√≠ obsah souboru")
    
    try:
        # Zkus√≠me parsovat jako jeden velk√Ω JSON objekt
        data = json.loads(content)
        
        if 'messages' in data:
            # M√°me jeden velk√Ω objekt s messages - rozdƒõl√≠me na konverzace
            messages = data['messages']
            print(f"üìä Naƒçteno {len(messages)} zpr√°v v jednom objektu")
            
            # Debug: Ulo≈æen√≠ v≈°ech zpr√°v
            if debugger:
                debugger.save_step("02_all_messages", messages, f"V≈°ech {len(messages)} zpr√°v z JSON objektu")
            
            # Najdeme system zpr√°vu (mƒõla by b√Ωt prvn√≠)
            system_msg = None
            for msg in messages:
                if msg['role'] == 'system':
                    system_msg = msg
                    break
            
            if not system_msg:
                print("‚ùå Nenalezena system zpr√°va!")
                return conversations
            
            # Debug: Ulo≈æen√≠ system zpr√°vy
            if debugger:
                debugger.save_step("03_system_message", [system_msg], "System zpr√°va")
            
            # Projdeme v≈°echny zpr√°vy a najdeme user-assistant p√°ry
            i = 0
            while i < len(messages):
                # Hled√°me user zpr√°vu
                if i < len(messages) and messages[i]['role'] == 'user':
                    user_msg = messages[i]
                    i += 1
                    
                    # Hled√°me n√°sleduj√≠c√≠ assistant zpr√°vu
                    if i < len(messages) and messages[i]['role'] == 'assistant':
                        assistant_msg = messages[i]
                        i += 1
                        
                        # Vytvo≈ô√≠me konverzaci s system + user + assistant
                        conv_messages = [system_msg, user_msg, assistant_msg]
                        conversations.append({
                            "messages": conv_messages
                        })
                    else:
                        # Chyb√≠ assistant zpr√°va, p≈ôeskoƒç√≠me user zpr√°vu
                        i += 1
                else:
                    # Nen√≠ user zpr√°va, p≈ôeskoƒç√≠me
                    i += 1
            
            print(f"‚úÖ Vytvo≈ôeno {len(conversations)} konverzac√≠")
            
            # Debug: Ulo≈æen√≠ vytvo≈ôen√Ωch konverzac√≠
            if debugger:
                debugger.save_step("04_conversations", conversations, f"Vytvo≈ôen√Ωch {len(conversations)} konverzac√≠")
                if len(conversations) > 0:
                    debugger.save_sample("04_conversations", conversations[0], 0)
                    if len(conversations) > 1:
                        debugger.save_sample("04_conversations", conversations[1], 1)
            
            # Debug informace
            if len(conversations) > 0:
                print(f"üìù Uk√°zka prvn√≠ konverzace:")
                first_conv = conversations[0]
                for msg in first_conv['messages']:
                    print(f"  {msg['role']}: {msg['content'][:100]}...")
                
                if len(conversations) > 1:
                    print(f"üìù Uk√°zka druh√© konverzace:")
                    second_conv = conversations[1]
                    for msg in second_conv['messages']:
                        print(f"  {msg['role']}: {msg['content'][:100]}...")
            
            return conversations
            
    except json.JSONDecodeError:
        # Nen√≠ jeden velk√Ω JSON objekt, zkus√≠me JSONL form√°t
        print("üìä Zkou≈°√≠m JSONL form√°t...")
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        conversations.append(data)
                    except json.JSONDecodeError as e:
                        print(f"‚ö†Ô∏è Chyba p≈ôi parsov√°n√≠ ≈ô√°dku: {e}")
                        continue
        
        print(f"‚úÖ Naƒçteno {len(conversations)} konverzac√≠ z JSONL")
        return conversations
    
    return conversations

def prepare_training_data(conversations, debugger=None):
    """P≈ôiprav√≠ data pro fine-tuning"""
    training_data = []
    
    # Debug: Ulo≈æen√≠ vstupn√≠ch konverzac√≠
    if debugger:
        debugger.save_step("05_input_conversations", conversations, f"Vstupn√≠ch {len(conversations)} konverzac√≠ pro prepare_training_data")
    
    for conv in conversations:
        messages = conv['messages']
        
        # P≈ôeskoƒç√≠me konverzace bez assistant zpr√°v
        if not any(msg['role'] == 'assistant' for msg in messages):
            continue
            
        # Vytvo≈ô√≠me text pro fine-tuning
        text = ""
        for msg in messages:
            if msg['role'] == 'system':
                text += f"<|system|>\n{msg['content']}<|end|>\n"
            elif msg['role'] == 'user':
                text += f"<|user|>\n{msg['content']}<|end|>\n"
            elif msg['role'] == 'assistant':
                text += f"<|assistant|>\n{msg['content']}<|end|>\n"
        
        training_data.append({"text": text})
    
    # Debug: Ulo≈æen√≠ p≈ôipraven√Ωch dat
    if debugger:
        debugger.save_step("06_training_data", training_data, f"P≈ôipraven√Ωch {len(training_data)} tr√©novac√≠ch vzork≈Ø")
        if len(training_data) > 0:
            debugger.save_sample("06_training_data", training_data[0], 0)
            if len(training_data) > 1:
                debugger.save_sample("06_training_data", training_data[1], 1)
    
    return training_data

def tokenize_function(examples, tokenizer, max_length=2048):
    """Tokenizuje text pro fine-tuning"""
    # Tokenizace s padding pro konzistentn√≠ d√©lky
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding=True,  # Povol√≠me padding
        max_length=max_length,
        return_tensors=None
    )
    
    # Nastaven√≠ labels stejn√© jako input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

def setup_tokenizer_and_model(model_name, base_model):
    """Nastav√≠ tokenizer a model pro fine-tuning"""
    
    # 1. Naƒçten√≠ tokenizeru
    base_tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir='/workspace/.cache/huggingface/transformers',
        local_files_only=False,
        resume_download=True,
        force_download=False
    )
    print(f"üìä P≈Øvodn√≠ d√©lka tokenizeru: {len(base_tokenizer)}")
    
    # 2. Kontrola a p≈ôid√°n√≠ pad tokenu
    if base_tokenizer.pad_token is None:
        # Zkus√≠me pou≈æ√≠t existuj√≠c√≠ tokeny
        if base_tokenizer.eos_token:
            base_tokenizer.pad_token = base_tokenizer.eos_token
            print(f"‚úÖ Pou≈æ√≠v√°m EOS token jako PAD: {base_tokenizer.pad_token}")
        else:
            # P≈ôid√°me nov√Ω pad token
            base_tokenizer.add_special_tokens({"pad_token": "<pad>"})
            print(f"‚úÖ P≈ôid√°n nov√Ω pad token: {base_tokenizer.pad_token}")
            
            # D≈Øle≈æit√©: Resize model embeddings
            base_model.resize_token_embeddings(len(base_tokenizer))
            print(f"üìä Model embeddings resized na: {len(base_tokenizer)}")
    else:
        print(f"‚ÑπÔ∏è Pad token u≈æ existuje: {base_tokenizer.pad_token}")
    
    # 3. Synchronizace s modelem
    if hasattr(base_model.config, 'pad_token_id'):
        old_pad_id = base_model.config.pad_token_id
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
        print(f"üîÑ Pad token ID zmƒõnƒõn: {old_pad_id} ‚Üí {base_model.config.pad_token_id}")
    else:
        print("‚ö†Ô∏è Model nem√° pad_token_id v config")
    
    # 4. Kontrola konzistence
    try:
        assert base_tokenizer.pad_token_id == base_model.config.pad_token_id, \
            "Tokenizer a model maj√≠ r≈Øzn√© pad token ID!"
        print(f"‚úÖ Tokenizer a model synchronizov√°ny")
    except AssertionError as e:
        print(f"‚ùå Chyba synchronizace: {e}")
        # Pokus√≠me se opravit
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
        print(f"üîß Opraveno: pad_token_id nastaven na {base_tokenizer.pad_token_id}")
    
    return base_tokenizer, base_model

def main():
    # Kontrola, ≈æe jsme v root directory projektu
    if not os.path.exists('lib') or not os.path.exists('data'):
        print("‚ùå Skript mus√≠ b√Ωt spu≈°tƒõn z root directory projektu!")
        print("üí° Spus≈•te skript z adres√°≈ôe, kde jsou slo≈æky 'lib' a 'data'")
        print(f"üìç Aktu√°ln√≠ adres√°≈ô: {os.getcwd()}")
        print("üìÅ Obsah aktu√°ln√≠ho adres√°≈ôe:")
        try:
            for item in os.listdir('.'):
                print(f"  - {item}")
        except:
            pass
        return
    
    parser = argparse.ArgumentParser(description='Fine-tuning 3 8B pro Andreje Babi≈°e')
    parser.add_argument('--data_path', type=str, default='data/all.jsonl', help='Cesta k dat≈Øm')
    parser.add_argument('--output_dir', type=str, default='/workspace/babis-finetuned', help='V√Ωstupn√≠ adres√°≈ô')
    parser.add_argument('--model_name', type=str, default='microsoft/DialoGPT-medium', help='N√°zev base modelu')
    parser.add_argument('--epochs', type=int, default=3, help='Poƒçet epoch')
    parser.add_argument('--batch_size', type=int, default=2, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=2e-4, help='Learning rate')
    parser.add_argument('--max_length', type=int, default=1024, help='Maxim√°ln√≠ d√©lka sekvence')
    parser.add_argument('--use_wandb', action='store_true', help='Pou≈æ√≠t Weights & Biases')
    parser.add_argument('--push_to_hub', action='store_true', help='Nahr√°t model na HF Hub')
    parser.add_argument('--hub_model_id', type=str, default='babis-lora', help='N√°zev modelu na HF Hub')
    parser.add_argument('--cleanup_cache', action='store_true', help='Vyƒçistit cache p≈ôed spu≈°tƒõn√≠m')
    parser.add_argument('--aggressive_cleanup', action='store_true', help='Agresivn√≠ vyƒçi≈°tƒõn√≠ pro velk√© modely')
    
    args = parser.parse_args()
    
    # Zajist√≠me, ≈æe v√Ωstupn√≠ adres√°≈ô je na network storage
    if not args.output_dir.startswith('/workspace'):
        args.output_dir = f'/workspace/{args.output_dir.lstrip("./")}'
    
    print("üöÄ Spou≈°t√≠m fine-tuning pro Andreje Babi≈°e")
    print(f"üìÅ Data: {args.data_path}")
    print(f"üìÅ V√Ωstup: {args.output_dir}")
    print(f"üìÅ Model: {args.model_name}")
    
    # Inicializace disk manageru a nastaven√≠ pro ML projekt
    dm = setup_for_ml_project("/workspace")
    
    # Kontrola m√≠sta a vyƒçi≈°tƒõn√≠ pokud je pot≈ôeba
    if not check_and_cleanup(threshold=95):
        print("‚ùå St√°le nen√≠ dost m√≠sta. Pou≈æijte men≈°√≠ model nebo vyƒçistƒõte disk.")
        return
    
    # Vyƒçi≈°tƒõn√≠ cache pokud po≈æadov√°no
    if args.cleanup_cache:
        dm.cleanup_cache()
    
    # Optimalizace pro velk√© modely
    if args.aggressive_cleanup or "mistral" in args.model_name.lower() or "llama" in args.model_name.lower():
        print("üßπ Optimalizace pro velk√Ω model...")
        if not dm.optimize_for_large_models(args.model_name):
            print("‚ùå Nedost m√≠sta pro velk√Ω model. Zkuste men≈°√≠ model.")
            return
    
    # Naƒçten√≠ promƒõnn√Ωch prost≈ôed√≠
    load_dotenv()
    
    # Hugging Face token
    HF_TOKEN = os.getenv("HF_TOKEN")
    if HF_TOKEN:
        login(token=HF_TOKEN)
        print("‚úÖ Hugging Face login √∫spƒõ≈°n√Ω")
    else:
        print("‚ö†Ô∏è HF_TOKEN nebyl nalezen")
    
    # Weights & Biases
    if args.use_wandb:
        WANDB_API_KEY = os.getenv("WANDB_API_KEY")
        if WANDB_API_KEY:
            os.environ["WANDB_API_KEY"] = WANDB_API_KEY
            wandb.login()
            wandb.init(project="babis-finetune", name=args.model_name)
            print("‚úÖ W&B login √∫spƒõ≈°n√Ω")
        else:
            print("‚ö†Ô∏è WANDB_API_KEY nebyl nalezen")
    
    # Inicializace debuggeru pro sledov√°n√≠ zpracov√°n√≠ datasetu
    debugger = DatasetDebugger(debug_dir="debug_dataset_finetune")
    print(f"üîç Debugger inicializov√°n: {debugger.debug_dir}")
    
    # 1. Naƒçten√≠ dat
    print("\nüìä Naƒç√≠t√°m data...")
    conversations = load_babis_data(args.data_path, debugger)
    print(f"‚úÖ Naƒçteno {len(conversations)} konverzac√≠")
    
    # 2. P≈ô√≠prava dat
    print("üîß P≈ôipravuji data...")
    training_data = prepare_training_data(conversations, debugger)
    print(f"‚úÖ P≈ôipraveno {len(training_data)} tr√©novac√≠ch vzork≈Ø")
    
    # 3. Vytvo≈ôen√≠ Dataset
    dataset = Dataset.from_list(training_data)
    
    # Debug: Ulo≈æen√≠ fin√°ln√≠ho datasetu
    debugger.save_step("07_final_dataset", {"dataset_size": len(dataset), "columns": dataset.column_names}, 
                      f"Fin√°ln√≠ dataset s {len(dataset)} vzorky")
    
    # Debug: Kontrola struktury dat
    print(f"\nüîç DEBUG: Kontrola struktury dat")
    print(f"üìä Celkov√Ω poƒçet vzork≈Ø: {len(dataset)}")
    
    if len(dataset) > 0:
        print(f"üìù Uk√°zka prvn√≠ho vzorku:")
        first_sample = dataset[0]
        print(f"Text (prvn√≠ch 200 znak≈Ø): {first_sample['text'][:200]}...")
        
        # Kontrola p≈ô√≠tomnosti system, user, assistant tag≈Ø
        text = first_sample['text']
        has_system = "<|system|>" in text
        has_user = "<|user|>" in text
        has_assistant = "<|assistant|>" in text
        has_end = "<|end|>" in text
        
        print(f"‚úÖ System tag: {has_system}")
        print(f"‚úÖ User tag: {has_user}")
        print(f"‚úÖ Assistant tag: {has_assistant}")
        print(f"‚úÖ End tag: {has_end}")
        
        # Poƒç√≠t√°n√≠ tag≈Ø v cel√©m datasetu
        system_count = sum(1 for sample in dataset if "<|system|>" in sample['text'])
        user_count = sum(1 for sample in dataset if "<|user|>" in sample['text'])
        assistant_count = sum(1 for sample in dataset if "<|assistant|>" in sample['text'])
        
        print(f"üìä Statistiky tag≈Ø v cel√©m datasetu:")
        print(f"  System messages: {system_count}")
        print(f"  User messages: {user_count}")
        print(f"  Assistant messages: {assistant_count}")
        
        # Kontrola d√©lky text≈Ø
        lengths = [len(sample['text']) for sample in dataset]
        print(f"üìè D√©lka text≈Ø: min={min(lengths)}, max={max(lengths)}, avg={sum(lengths)/len(lengths):.1f}")
        
        # Debug: Ulo≈æen√≠ statistik
        debugger.save_step("08_dataset_stats", {
            "total_samples": len(dataset),
            "system_messages": system_count,
            "user_messages": user_count,
            "assistant_messages": assistant_count,
            "text_lengths": {
                "min": min(lengths),
                "max": max(lengths),
                "avg": sum(lengths)/len(lengths)
            }
        }, "Statistiky datasetu")
    
    # Vytvo≈ôen√≠ shrnut√≠ debug informac√≠
    debugger.create_summary()
    print(f"üìã Debug shrnut√≠ vytvo≈ôeno: {debugger.debug_dir}/debug_summary.txt")
    
    # 4. Naƒçten√≠ modelu
    print(f"\nü§ñ Naƒç√≠t√°m model: {args.model_name}")
    
    # Pou≈æit√≠ men≈°√≠ho modelu pro √∫sporu m√≠sta
    if "mistral" in args.model_name.lower() or "llama" in args.model_name.lower():
        print("‚ö†Ô∏è Detekov√°n velk√Ω model. Pou≈æ√≠v√°m agresivn√≠ optimalizaci.")
        print("üí° Dostupn√© men≈°√≠ modely:")
        print("   - microsoft/DialoGPT-medium (355M)")
        print("   - microsoft/DialoGPT-large (774M)")
        print("   - gpt2-medium (355M)")
        print("   - distilgpt2 (82M)")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
    )
    
    # Pokus o naƒçten√≠ modelu s retry logikou
    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"üîÑ Pokus {attempt + 1}/{max_retries} naƒçten√≠ modelu...")
            
            model = AutoModelForCausalLM.from_pretrained(
                args.model_name,
                quantization_config=bnb_config,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                cache_dir='/workspace/.cache/huggingface/transformers',
                local_files_only=False,
                resume_download=True,
                force_download=False
            )
            print("‚úÖ Model √∫spƒõ≈°nƒõ naƒçten!")
            break
            
        except OSError as e:
            if "No space left on device" in str(e):
                print(f"‚ùå Pokus {attempt + 1} selhal - nen√≠ dost m√≠sta")
                if attempt < max_retries - 1:
                    print("üßπ Zkou≈°√≠m dal≈°√≠ vyƒçi≈°tƒõn√≠...")
                    dm.aggressive_cleanup()
                    # Poƒçk√°me chv√≠li
                    import time
                    time.sleep(5)
                else:
                    print("‚ùå V≈°echny pokusy selhaly. Zkuste:")
                    print("   1. Pou≈æ√≠t men≈°√≠ model: --model_name microsoft/DialoGPT-medium")
                    print("   2. Restartovat kontejner")
                    print("   3. Zv√Ω≈°it velikost root filesystem")
                    return
            else:
                raise e
        except Exception as e:
            print(f"‚ùå Neoƒçek√°van√° chyba p≈ôi naƒç√≠t√°n√≠ modelu: {e}")
            if attempt < max_retries - 1:
                print("üîÑ Zkou≈°√≠m znovu...")
                import time
                time.sleep(10)
            else:
                raise e
    
    tokenizer, model = setup_tokenizer_and_model(args.model_name, model)
    
    print(f"‚úÖ Model naƒçten. Vocab size: {model.config.vocab_size}")
    
    # 5. Konfigurace LoRA
    print("\nüîß Konfiguruji LoRA...")
    
    # Dynamick√© nastaven√≠ target_modules podle typu modelu
    if "dialogpt" in args.model_name.lower():
        target_modules = ["c_attn", "c_proj", "wte", "wpe"]
    elif "gpt2" in args.model_name.lower():
        target_modules = ["c_attn", "c_proj", "wte", "wpe"]
    else:
        target_modules = [
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"
        ]
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,  # Men≈°√≠ r pro √∫sporu pamƒõti
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=target_modules
    )
    
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 6. Tokenizace dat
    print("\nüî§ Tokenizuji data...")
    tokenize_func = lambda examples: tokenize_function(examples, tokenizer, args.max_length)
    tokenized_dataset = dataset.map(
        tokenize_func,
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=100  # Men≈°√≠ batch size pro lep≈°√≠ kontrolu
    )
    
    # Debug: Ulo≈æen√≠ tokenizovan√©ho datasetu
    debugger.save_step("09_tokenized_dataset", {
        "dataset_size": len(tokenized_dataset),
        "columns": tokenized_dataset.column_names,
        "max_length": args.max_length
    }, f"Tokenizovan√Ω dataset s {len(tokenized_dataset)} vzorky")
    
    # Debug: Uk√°zka tokenizovan√©ho vzorku
    if len(tokenized_dataset) > 0:
        sample_tokens = tokenized_dataset[0]
        decoded_text = tokenizer.decode(sample_tokens['input_ids'], skip_special_tokens=False)
        debugger.save_step("10_tokenized_sample", {
            "input_ids_length": len(sample_tokens['input_ids']),
            "attention_mask_length": len(sample_tokens['attention_mask']),
            "labels_length": len(sample_tokens['labels']),
            "decoded_text": decoded_text[:500] + "..." if len(decoded_text) > 500 else decoded_text,
            "has_system": "<|system|>" in decoded_text,
            "has_user": "<|user|>" in decoded_text,
            "has_assistant": "<|assistant|>" in decoded_text,
            "has_end": "<|end|>" in decoded_text
        }, "Uk√°zka tokenizovan√©ho vzorku")
    
    # Kontrola a oprava padding po tokenizaci
    print("üîß Kontroluji a opravuji padding...")
    def fix_padding(example):
        """Zajist√≠, ≈æe v≈°echny sekvence maj√≠ stejnou d√©lku"""
        max_len = args.max_length
        current_len = len(example['input_ids'])
        
        if current_len < max_len:
            # P≈ôid√°me padding
            padding_length = max_len - current_len
            example['input_ids'] = example['input_ids'] + [tokenizer.pad_token_id] * padding_length
            example['attention_mask'] = example['attention_mask'] + [0] * padding_length
            example['labels'] = example['labels'] + [-100] * padding_length  # -100 pro ignorov√°n√≠ v loss
        elif current_len > max_len:
            # O≈ô√≠zneme na max_length
            example['input_ids'] = example['input_ids'][:max_len]
            example['attention_mask'] = example['attention_mask'][:max_len]
            example['labels'] = example['labels'][:max_len]
        
        return example
    
    # Aplikujeme opravu padding na cel√Ω dataset
    tokenized_dataset = tokenized_dataset.map(
        fix_padding,
        desc="Opravuji padding"
    )
    
    # Debug: Ulo≈æen√≠ fin√°ln√≠ho tokenizovan√©ho datasetu
    debugger.save_step("11_final_tokenized_dataset", {
        "dataset_size": len(tokenized_dataset),
        "columns": tokenized_dataset.column_names,
        "max_length": args.max_length
    }, f"Fin√°ln√≠ tokenizovan√Ω dataset s padding")
    
    # Rozdƒõlen√≠ na train/validation s kontrolou velikosti
    print(f"üìä Celkov√Ω poƒçet vzork≈Ø: {len(tokenized_dataset)}")
    
    if len(tokenized_dataset) < 5:
        print("‚ö†Ô∏è M√°lo vzork≈Ø pro rozdƒõlen√≠. Pou≈æ√≠v√°m cel√Ω dataset pro tr√©nov√°n√≠.")
        train_dataset = tokenized_dataset
        eval_dataset = tokenized_dataset  # Pou≈æijeme stejn√Ω dataset pro evaluaci
        split_info = {"type": "no_split", "reason": "too_few_samples", "train_size": len(train_dataset), "eval_size": len(eval_dataset)}
    elif len(tokenized_dataset) < 10:
        # Pro velmi mal√© datasety pou≈æijeme 80/20 split
        split_ratio = 0.2
        split_dataset = tokenized_dataset.train_test_split(test_size=split_ratio, seed=42)
        train_dataset = split_dataset["train"]
        eval_dataset = split_dataset["test"]
        print(f"‚úÖ Train dataset: {len(train_dataset)} vzork≈Ø ({100-split_ratio*100:.0f}%)")
        print(f"‚úÖ Validation dataset: {len(eval_dataset)} vzork≈Ø ({split_ratio*100:.0f}%)")
        split_info = {"type": "80_20_split", "split_ratio": split_ratio, "train_size": len(train_dataset), "eval_size": len(eval_dataset)}
    else:
        # Standardn√≠ 90/10 split
        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)
        train_dataset = split_dataset["train"]
        eval_dataset = split_dataset["test"]
        print(f"‚úÖ Train dataset: {len(train_dataset)} vzork≈Ø (90%)")
        print(f"‚úÖ Validation dataset: {len(eval_dataset)} vzork≈Ø (10%)")
        split_info = {"type": "90_10_split", "split_ratio": 0.1, "train_size": len(train_dataset), "eval_size": len(eval_dataset)}
    
    # Debug: Ulo≈æen√≠ informac√≠ o rozdƒõlen√≠
    debugger.save_step("12_train_validation_split", split_info, f"Rozdƒõlen√≠ datasetu: {split_info['type']}")
    
    # Kontrola minim√°ln√≠ velikosti datasetu
    if len(train_dataset) == 0:
        print("‚ùå Train dataset je pr√°zdn√Ω! Zkontrolujte data.")
        return
    
    if len(eval_dataset) == 0:
        print("‚ö†Ô∏è Validation dataset je pr√°zdn√Ω. Pou≈æ√≠v√°m train dataset pro evaluaci.")
        eval_dataset = train_dataset
        debugger.save_step("13_split_fallback", {"action": "use_train_for_eval", "reason": "empty_eval_dataset"}, "Pou≈æit√≠ train datasetu pro evaluaci")
    
    # Debug: Kontrola train/validation split
    print(f"\nüîç DEBUG: Kontrola train/validation split")
    print(f"üìä Train dataset: {len(train_dataset)} vzork≈Ø")
    print(f"üìä Validation dataset: {len(eval_dataset)} vzork≈Ø")
    
    # Debug: Ulo≈æen√≠ train a validation dataset≈Ø do soubor≈Ø
    print(f"\nüíæ Ukl√°d√°m train a validation datasety...")
    
    # Ulo≈æen√≠ train datasetu
    train_data_file = os.path.join(debugger.debug_dir, "train_dataset.jsonl")
    with open(train_data_file, 'w', encoding='utf-8') as f:
        for i, sample in enumerate(train_dataset):
            # Dek√≥dov√°n√≠ token≈Ø zpƒõt na text pro ƒçitelnost
            decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
            sample_data = {
                "index": i,
                "input_ids_length": len(sample['input_ids']),
                "attention_mask_length": len(sample['attention_mask']),
                "labels_length": len(sample['labels']),
                "decoded_text": decoded_text,
                "has_system": "<|system|>" in decoded_text,
                "has_user": "<|user|>" in decoded_text,
                "has_assistant": "<|assistant|>" in decoded_text,
                "has_end": "<|end|>" in decoded_text,
                "token_ids": sample['input_ids'][:100] + ["..."] if len(sample['input_ids']) > 100 else sample['input_ids']  # Prvn√≠ch 100 token≈Ø
            }
            f.write(json.dumps(sample_data, ensure_ascii=False) + '\n')
    print(f"‚úÖ Train dataset ulo≈æen: {train_data_file}")
    
    # Ulo≈æen√≠ validation datasetu
    eval_data_file = os.path.join(debugger.debug_dir, "validation_dataset.jsonl")
    with open(eval_data_file, 'w', encoding='utf-8') as f:
        for i, sample in enumerate(eval_dataset):
            # Dek√≥dov√°n√≠ token≈Ø zpƒõt na text pro ƒçitelnost
            decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
            sample_data = {
                "index": i,
                "input_ids_length": len(sample['input_ids']),
                "attention_mask_length": len(sample['attention_mask']),
                "labels_length": len(sample['labels']),
                "decoded_text": decoded_text,
                "has_system": "<|system|>" in decoded_text,
                "has_user": "<|user|>" in decoded_text,
                "has_assistant": "<|assistant|>" in decoded_text,
                "has_end": "<|end|>" in decoded_text,
                "token_ids": sample['input_ids'][:100] + ["..."] if len(sample['input_ids']) > 100 else sample['input_ids']  # Prvn√≠ch 100 token≈Ø
            }
            f.write(json.dumps(sample_data, ensure_ascii=False) + '\n')
    print(f"‚úÖ Validation dataset ulo≈æen: {eval_data_file}")
    
    # Vytvo≈ôen√≠ ƒçiteln√© verze pro rychl√© prohl√≠≈æen√≠
    train_readable_file = os.path.join(debugger.debug_dir, "train_dataset_readable.txt")
    with open(train_readable_file, 'w', encoding='utf-8') as f:
        f.write("TRAIN DATASET - ƒåITELN√Å VERZE\n")
        f.write("=" * 50 + "\n")
        f.write(f"Celkov√Ω poƒçet vzork≈Ø: {len(train_dataset)}\n\n")
        
        for i in range(min(5, len(train_dataset))):  # Prvn√≠ch 5 vzork≈Ø
            sample = train_dataset[i]
            decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
            f.write(f"VZOREK {i+1}:\n")
            f.write(f"D√©lka token≈Ø: {len(sample['input_ids'])}\n")
            f.write(f"Text:\n{decoded_text}\n")
            f.write("-" * 80 + "\n\n")
        
        if len(train_dataset) > 5:
            f.write(f"... a dal≈°√≠ch {len(train_dataset) - 5} vzork≈Ø\n")
    
    eval_readable_file = os.path.join(debugger.debug_dir, "validation_dataset_readable.txt")
    with open(eval_readable_file, 'w', encoding='utf-8') as f:
        f.write("VALIDATION DATASET - ƒåITELN√Å VERZE\n")
        f.write("=" * 50 + "\n")
        f.write(f"Celkov√Ω poƒçet vzork≈Ø: {len(eval_dataset)}\n\n")
        
        for i in range(min(5, len(eval_dataset))):  # Prvn√≠ch 5 vzork≈Ø
            sample = eval_dataset[i]
            decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
            f.write(f"VZOREK {i+1}:\n")
            f.write(f"D√©lka token≈Ø: {len(sample['input_ids'])}\n")
            f.write(f"Text:\n{decoded_text}\n")
            f.write("-" * 80 + "\n\n")
        
        if len(eval_dataset) > 5:
            f.write(f"... a dal≈°√≠ch {len(eval_dataset) - 5} vzork≈Ø\n")
    
    print(f"‚úÖ ƒåiteln√© verze vytvo≈ôeny:")
    print(f"   - {train_readable_file}")
    print(f"   - {eval_readable_file}")
    
    # Vytvo≈ôen√≠ statistik souboru
    stats_file = os.path.join(debugger.debug_dir, "dataset_statistics.json")
    train_lengths = [len(sample['input_ids']) for sample in train_dataset]
    eval_lengths = [len(sample['input_ids']) for sample in eval_dataset]
    
    train_texts = [tokenizer.decode(sample['input_ids'], skip_special_tokens=False) for sample in train_dataset]
    eval_texts = [tokenizer.decode(sample['input_ids'], skip_special_tokens=False) for sample in eval_dataset]
    
    stats = {
        "train_dataset": {
            "size": len(train_dataset),
            "token_lengths": {
                "min": min(train_lengths) if train_lengths else 0,
                "max": max(train_lengths) if train_lengths else 0,
                "avg": sum(train_lengths)/len(train_lengths) if train_lengths else 0,
                "median": sorted(train_lengths)[len(train_lengths)//2] if train_lengths else 0
            },
            "tags": {
                "system": sum(1 for text in train_texts if "<|system|>" in text),
                "user": sum(1 for text in train_texts if "<|user|>" in text),
                "assistant": sum(1 for text in train_texts if "<|assistant|>" in text),
                "end": sum(1 for text in train_texts if "<|end|>" in text)
            }
        },
        "validation_dataset": {
            "size": len(eval_dataset),
            "token_lengths": {
                "min": min(eval_lengths) if eval_lengths else 0,
                "max": max(eval_lengths) if eval_lengths else 0,
                "avg": sum(eval_lengths)/len(eval_lengths) if eval_lengths else 0,
                "median": sorted(eval_lengths)[len(eval_lengths)//2] if eval_lengths else 0
            },
            "tags": {
                "system": sum(1 for text in eval_texts if "<|system|>" in text),
                "user": sum(1 for text in eval_texts if "<|user|>" in text),
                "assistant": sum(1 for text in eval_texts if "<|assistant|>" in text),
                "end": sum(1 for text in eval_texts if "<|end|>" in text)
            }
        },
        "split_info": split_info,
        "tokenizer_info": {
            "vocab_size": len(tokenizer),
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            "max_length": args.max_length
        }
    }
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Statistiky ulo≈æeny: {stats_file}")
    
    # Debug: Ulo≈æen√≠ informac√≠ o datasetech
    debugger.save_step("15_train_validation_files", {
        "train_dataset_file": train_data_file,
        "validation_dataset_file": eval_data_file,
        "train_readable_file": train_readable_file,
        "validation_readable_file": eval_readable_file,
        "statistics_file": stats_file,
        "train_size": len(train_dataset),
        "validation_size": len(eval_dataset)
    }, "Ulo≈æen√© soubory train a validation dataset≈Ø")
    
    # Dynamick√© nastaven√≠ training parametr≈Ø podle velikosti datasetu
    if len(train_dataset) < 10:
        # Pro mal√© datasety
        save_steps = max(1, len(train_dataset) // 2)
        eval_steps = max(1, len(train_dataset) // 2)
        logging_steps = 1
        print(f"üìä Mal√Ω dataset - save_steps: {save_steps}, eval_steps: {eval_steps}")
    else:
        # Pro vƒõt≈°√≠ datasety
        save_steps = 500
        eval_steps = 500
        logging_steps = 10
    
    print(f"\n‚úÖ System messages jsou v obou datasetech - model se uƒç√≠ na kompletn√≠ch konverzac√≠ch")
    print(f"‚úÖ Ka≈æd√° konverzace obsahuje: system + user + assistant")
    print(f"‚úÖ Data jsou p≈ôipravena pro fine-tuning")
    
    # Fin√°ln√≠ debug shrnut√≠ p≈ôed tr√©nov√°n√≠m
    debugger.save_step("16_final_pre_training_summary", {
        "model_name": args.model_name,
        "max_length": args.max_length,
        "batch_size": args.batch_size,
        "epochs": args.epochs,
        "learning_rate": args.learning_rate,
        "train_dataset_size": len(train_dataset),
        "eval_dataset_size": len(eval_dataset),
        "total_samples": len(tokenized_dataset),
        "split_info": split_info,
        "tokenizer_vocab_size": len(tokenizer),
        "model_vocab_size": model.config.vocab_size,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "training_args": {
            "output_dir": args.output_dir,
            "save_steps": save_steps,
            "eval_steps": eval_steps,
            "logging_steps": logging_steps,
            "use_wandb": args.use_wandb,
            "push_to_hub": args.push_to_hub
        }
    }, "Fin√°ln√≠ shrnut√≠ p≈ôed zaƒç√°tkem tr√©nov√°n√≠")
    
    # Vytvo≈ôen√≠ fin√°ln√≠ho shrnut√≠ debug informac√≠
    debugger.create_summary()
    print(f"üìã Kompletn√≠ debug shrnut√≠ vytvo≈ôeno: {debugger.debug_dir}/debug_summary.txt")
    print(f"üîç V≈°echny debug soubory jsou ulo≈æeny v: {debugger.debug_dir}")
    
    # 7. Data Collator
    print("\nüîß Konfiguruji data collator...")
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        return_tensors="pt",
        pad_to_multiple_of=8,  # Padding na n√°sobky 8 pro lep≈°√≠ v√Ωkon
    )
    
    # Test data collator na jednom vzorku
    if len(train_dataset) > 0:
        try:
            test_batch = data_collator([train_dataset[0]])
            print(f"‚úÖ Data collator test √∫spƒõ≈°n√Ω")
            print(f"üìä Batch keys: {list(test_batch.keys())}")
            print(f"üìä Input shape: {test_batch['input_ids'].shape}")
            print(f"üìä Labels shape: {test_batch['labels'].shape}")
        except Exception as e:
            print(f"‚ö†Ô∏è Data collator test selhal: {e}")
            print("üîç Debugging informace:")
            print(f"  Sample keys: {list(train_dataset[0].keys())}")
            print(f"  Input IDs length: {len(train_dataset[0]['input_ids'])}")
            print(f"  Labels length: {len(train_dataset[0]['labels'])}")
            print(f"  Sample type: {type(train_dataset[0]['input_ids'])}")
            
            # Zkus√≠me opravit probl√©m s padding
            print("üîß Zkou≈°√≠m opravit padding...")
            try:
                # Vytvo≈ô√≠me nov√Ω data collator s explicitn√≠m padding
                fixed_collator = DataCollatorForLanguageModeling(
                    tokenizer=tokenizer,
                    mlm=False,
                    return_tensors="pt",
                    pad_to_multiple_of=8,
                    padding=True,
                )
                test_batch = fixed_collator([train_dataset[0]])
                print(f"‚úÖ Opraven√Ω data collator test √∫spƒõ≈°n√Ω")
                data_collator = fixed_collator
            except Exception as e2:
                print(f"‚ùå Oprava selhala: {e2}")
                print("‚ÑπÔ∏è Pokraƒçuji s v√Ωchoz√≠m nastaven√≠m")
    
    # 8. Training Arguments - nastaven√≠ na network storage
    print("\n‚öôÔ∏è Nastavuji training arguments...")
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=4,
        warmup_steps=min(100, len(train_dataset) // 2),
        learning_rate=args.learning_rate,
        fp16=True,
        logging_steps=logging_steps,
        save_steps=save_steps,
        eval_steps=eval_steps,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="wandb" if args.use_wandb else "none",
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        weight_decay=0.01,
        max_grad_norm=0.3,
        push_to_hub=args.push_to_hub,
        hub_model_id=args.hub_model_id,
        hub_token=HF_TOKEN if args.push_to_hub else None,
        save_total_limit=2,
        logging_dir=f"{args.output_dir}/logs",
        dataloader_num_workers=0,
        dataloader_drop_last=True,  # P≈ôid√°no pro lep≈°√≠ handling batch≈Ø
        group_by_length=True,  # P≈ôid√°no pro lep≈°√≠ padding
    )
    
    # 9. Trainer
    print("\nüèãÔ∏è Vytv√°≈ô√≠m Trainer...")
    
    # Nastaven√≠ label_names pro PeftModel - robustnƒõj≈°√≠ p≈ô√≠stup
    try:
        # Zkus√≠me nastavit label_names na modelu
        if hasattr(model, 'label_names'):
            model.label_names = ['labels']
        elif hasattr(model, 'config') and hasattr(model.config, 'label_names'):
            model.config.label_names = ['labels']
        
        # Pro PeftModel m≈Ø≈æeme tak√© nastavit na base modelu
        if hasattr(model, 'base_model') and hasattr(model.base_model, 'config'):
            model.base_model.config.label_names = ['labels']
        
        print("‚úÖ Label names nastaveny pro model")
    except Exception as e:
        print(f"‚ö†Ô∏è Nelze nastavit label_names: {e}")
        print("‚ÑπÔ∏è Pokraƒçuji bez explicitn√≠ho nastaven√≠ label_names")
    
    # Zajist√≠me, ≈æe model je v training m√≥du
    model.train()
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # 10. Fine-tuning
    print("\nüöÄ Spou≈°t√≠m fine-tuning...")
    trainer.train()
    
    # 11. Ulo≈æen√≠ modelu na network storage
    print("\nüíæ Ukl√°d√°m model na network storage...")
    final_model_path = f"{args.output_dir}-final"
    
    # Vytvo≈ôen√≠ adres√°≈ôe pokud neexistuje
    os.makedirs(final_model_path, exist_ok=True)
    
    trainer.save_model(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    
    # V√Ωpis velikosti ulo≈æen√©ho modelu
    try:
        import subprocess
        result = subprocess.run(['du', '-sh', final_model_path], capture_output=True, text=True)
        if result.stdout:
            print(f"üìä Velikost modelu: {result.stdout.strip()}")
    except:
        pass
    
    if args.push_to_hub and HF_TOKEN:
        print("üì§ Nahr√°v√°m model na Hugging Face Hub...")
        model.push_to_hub(args.hub_model_id, token=HF_TOKEN)
        tokenizer.push_to_hub(args.hub_model_id, token=HF_TOKEN)
        print(f"‚úÖ Model nahr√°n: https://huggingface.co/{args.hub_model_id}")
    
    # 12. Testov√°n√≠
    print("\nüèãÔ∏è Testuji model...")
    def generate_response(prompt, max_length=200):
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
    
    test_prompts = [
        "Pane Babi≈°i, jak hodnot√≠te souƒçasnou inflaci?",
        "Co si mysl√≠te o opozici?",
        "Jak se v√°m da≈ô√≠ v Bruselu?"
    ]
    
    print("\nüìù Testovac√≠ odpovƒõdi:")
    print("=" * 50)
    for prompt in test_prompts:
        print(f"\nPrompt: {prompt}")
        response = generate_response(prompt)
        print(f"Odpovƒõƒè: {response}")
        print("-" * 30)
    
    # 13. Ukonƒçen√≠
    if args.use_wandb:
        wandb.finish()
    
    print("\nüéâ Fine-tuning dokonƒçen!")
    print(f"üìÅ Model ulo≈æen v: {final_model_path}")
    print(f"üíæ Network storage: {args.output_dir}")
    if args.push_to_hub:
        print(f"üåê Model dostupn√Ω na: https://huggingface.co/{args.hub_model_id}")
    
    # V√Ωpis informac√≠ o ulo≈æen√Ωch souborech
    print(f"\nüìã Ulo≈æen√© soubory:")
    try:
        for root, dirs, files in os.walk(final_model_path):
            level = root.replace(final_model_path, '').count(os.sep)
            indent = ' ' * 2 * level
            print(f"{indent}{os.path.basename(root)}/")
            subindent = ' ' * 2 * (level + 1)
            for file in files[:5]:  # Zobraz√≠me pouze prvn√≠ch 5 soubor≈Ø
                print(f"{subindent}{file}")
            if len(files) > 5:
                print(f"{subindent}... a dal≈°√≠ch {len(files) - 5} soubor≈Ø")
    except Exception as e:
        print(f"‚ö†Ô∏è Nelze zobrazit seznam soubor≈Ø: {e}")

if __name__ == "__main__":
    main() 