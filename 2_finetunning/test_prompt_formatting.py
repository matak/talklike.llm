#!/usr/bin/env python3
"""
Test script pro porovn√°n√≠ form√°tov√°n√≠ prompt≈Ø
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import setup_environment

from transformers import AutoTokenizer
from train_utils import generate_response

def test_prompt_formatting():
    """Porovn√° r≈Øzn√© zp≈Øsoby form√°tov√°n√≠ prompt≈Ø"""
    
    model_name = "mistralai/Mistral-7B-Instruct-v0.3"
    test_prompt = "Co si mysl√≠≈° o inflaci?"
    
    print(f"üîß Testuji form√°tov√°n√≠ prompt≈Ø pro model: {model_name}")
    print("=" * 60)
    
    try:
        # Naƒçten√≠ tokenizeru
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir='/workspace/.cache/huggingface/transformers',
            local_files_only=False,
            resume_download=True,
            force_download=False
        )
        
        print(f"‚úÖ Tokenizer naƒçten, vocab size: {len(tokenizer)}")
        
        # Test 1: P≈ô√≠m√© form√°tov√°n√≠ (star√Ω zp≈Øsob)
        print("\nüìù Test 1: P≈ô√≠m√© form√°tov√°n√≠ promptu")
        print("-" * 40)
        direct_prompt = test_prompt
        print(f"P≈Øvodn√≠ prompt: {direct_prompt}")
        
        # Tokenizace p≈ô√≠m√©ho promptu
        direct_inputs = tokenizer(direct_prompt, return_tensors="pt")
        print(f"Tokenizovan√° d√©lka: {direct_inputs['input_ids'].shape[1]}")
        print(f"Prvn√≠ch 10 token≈Ø: {direct_inputs['input_ids'][0][:10].tolist()}")
        
        # Test 2: apply_chat_template (nov√Ω zp≈Øsob)
        print("\nüìù Test 2: apply_chat_template form√°tov√°n√≠")
        print("-" * 40)
        messages = [{"role": "user", "content": test_prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        print(f"Form√°tovan√Ω prompt: {formatted_prompt}")
        
        # Tokenizace form√°tovan√©ho promptu
        formatted_inputs = tokenizer(formatted_prompt, return_tensors="pt")
        print(f"Tokenizovan√° d√©lka: {formatted_inputs['input_ids'].shape[1]}")
        print(f"Prvn√≠ch 10 token≈Ø: {formatted_inputs['input_ids'][0][:10].tolist()}")
        
        # Porovn√°n√≠
        print("\nüìä Porovn√°n√≠:")
        print("-" * 40)
        print(f"P≈ô√≠m√© form√°tov√°n√≠: {direct_inputs['input_ids'].shape[1]} token≈Ø")
        print(f"apply_chat_template: {formatted_inputs['input_ids'].shape[1]} token≈Ø")
        print(f"Rozd√≠l: {formatted_inputs['input_ids'].shape[1] - direct_inputs['input_ids'].shape[1]} token≈Ø")
        
        # Test 3: Pou≈æit√≠ v generate_response funkci
        print("\nüìù Test 3: Pou≈æit√≠ v generate_response funkci")
        print("-" * 40)
        print("Tato funkce nyn√≠ automaticky pou≈æ√≠v√° apply_chat_template pokud je dostupn√©")
        
        # Simulace vol√°n√≠ generate_response (bez skuteƒçn√©ho modelu)
        if hasattr(tokenizer, 'apply_chat_template'):
            print("‚úÖ generate_response bude pou≈æ√≠vat apply_chat_template")
            messages = [{"role": "user", "content": test_prompt}]
            expected_formatted = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            print(f"Oƒçek√°van√Ω form√°t: {expected_formatted}")
        else:
            print("‚ö†Ô∏è generate_response bude pou≈æ√≠vat p≈ô√≠m√© form√°tov√°n√≠")
            print(f"Oƒçek√°van√Ω form√°t: {test_prompt}")
        
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi testov√°n√≠: {e}")

if __name__ == "__main__":
    test_prompt_formatting() 