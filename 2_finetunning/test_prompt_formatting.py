#!/usr/bin/env python3
"""
Test script pro porovn√°n√≠ form√°tov√°n√≠ prompt≈Ø
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import setup_environment

import torch
from transformers import AutoTokenizer
from train_utils import generate_response

def test_prompt_formatting():
    """Porovn√° r≈Øzn√© zp≈Øsoby form√°tov√°n√≠ prompt≈Ø"""
    
    model_name = "mistralai/Mistral-7B-Instruct-v0.3"
    test_prompt = "Co si mysl√≠≈° o inflaci?"
    
    print(f"üîß Testuji form√°tov√°n√≠ prompt≈Ø pro model: {model_name}")
    print("=" * 60)
    
    try:
        # Naƒçten√≠ tokenizeru
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir='/workspace/.cache/huggingface/transformers',
            local_files_only=False,
            resume_download=True,
            force_download=False,
            trust_remote_code=True
        )
        
        print(f"‚úÖ Tokenizer naƒçten, vocab size: {len(tokenizer)}")
        print(f"üîß EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
        print(f"üîß BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
        
        # Test 1: P≈ô√≠m√© form√°tov√°n√≠ (star√Ω zp≈Øsob)
        print("\nüìù Test 1: P≈ô√≠m√© form√°tov√°n√≠ promptu")
        print("-" * 40)
        direct_prompt = test_prompt
        print(f"P≈Øvodn√≠ prompt: {direct_prompt}")
        
        # Tokenizace p≈ô√≠m√©ho promptu
        direct_inputs = tokenizer(direct_prompt, return_tensors="pt")
        print(f"Tokenizovan√° d√©lka: {direct_inputs['input_ids'].shape[1]}")
        print(f"Prvn√≠ch 10 token≈Ø: {direct_inputs['input_ids'][0][:10].tolist()}")
        
        # Test 2: apply_chat_template s add_generation_prompt=True
        print("\nüìù Test 2: apply_chat_template s add_generation_prompt=True")
        print("-" * 40)
        messages = [{"role": "user", "content": test_prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        print(f"Form√°tovan√Ω prompt: '{formatted_prompt}'")
        
        # Tokenizace form√°tovan√©ho promptu
        formatted_inputs = tokenizer(formatted_prompt, return_tensors="pt")
        print(f"Tokenizovan√° d√©lka: {formatted_inputs['input_ids'].shape[1]}")
        print(f"Prvn√≠ch 10 token≈Ø: {formatted_inputs['input_ids'][0][:10].tolist()}")
        
        # Test 3: apply_chat_template s add_generation_prompt=False
        print("\nüìù Test 3: apply_chat_template s add_generation_prompt=False")
        print("-" * 40)
        formatted_prompt_no_gen = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        print(f"Form√°tovan√Ω prompt: '{formatted_prompt_no_gen}'")
        print(f"D√©lka: {len(formatted_prompt_no_gen)} znak≈Ø")
        
        # Porovn√°n√≠
        print("\nüìä Porovn√°n√≠:")
        print("-" * 40)
        print(f"P≈ô√≠m√© form√°tov√°n√≠: {direct_inputs['input_ids'].shape[1]} token≈Ø")
        print(f"apply_chat_template (s gen): {formatted_inputs['input_ids'].shape[1]} token≈Ø")
        print(f"apply_chat_template (bez gen): {len(tokenizer(formatted_prompt_no_gen, return_tensors='pt')['input_ids'][0])} token≈Ø")
        print(f"Rozd√≠l (s gen): {formatted_inputs['input_ids'].shape[1] - direct_inputs['input_ids'].shape[1]} token≈Ø")
        
        # Test 4: Tokenizace a dek√≥dov√°n√≠
        print("\nüìù Test 4: Tokenizace a dek√≥dov√°n√≠")
        print("-" * 40)
        
        # Dek√≥dov√°n√≠ zpƒõt (bez skip_special_tokens)
        decoded_with_special = tokenizer.decode(formatted_inputs['input_ids'][0], skip_special_tokens=False)
        print(f"Dek√≥dov√°no s special tokens: '{decoded_with_special}'")
        
        # Dek√≥dov√°n√≠ zpƒõt (se skip_special_tokens)
        decoded_without_special = tokenizer.decode(formatted_inputs['input_ids'][0], skip_special_tokens=True)
        print(f"Dek√≥dov√°no bez special tokens: '{decoded_without_special}'")
        
        # Test 5: Simulace generov√°n√≠ (p≈ôid√°n√≠ EOS tokenu)
        print("\nüìù Test 5: Simulace generov√°n√≠ s EOS tokenem")
        print("-" * 40)
        
        # P≈ôid√°n√≠ EOS tokenu na konec
        input_ids_with_eos = torch.cat([formatted_inputs['input_ids'][0], torch.tensor([tokenizer.eos_token_id])])
        print(f"Tokenizovan√° d√©lka s EOS: {input_ids_with_eos.shape[0]} token≈Ø")
        print(f"Posledn√≠ch 5 token≈Ø s EOS: {input_ids_with_eos[-5:].tolist()}")
        
        # Dek√≥dov√°n√≠ s EOS
        decoded_with_eos = tokenizer.decode(input_ids_with_eos, skip_special_tokens=True)
        print(f"Dek√≥dov√°no s EOS: '{decoded_with_eos}'")
        
        # Dek√≥dov√°n√≠ s EOS (bez skip_special_tokens)
        decoded_with_eos_special = tokenizer.decode(input_ids_with_eos, skip_special_tokens=False)
        print(f"Dek√≥dov√°no s EOS a special tokens: '{decoded_with_eos_special}'")
        
        # Test 6: Pou≈æit√≠ v generate_response funkci
        print("\nüìù Test 6: Pou≈æit√≠ v generate_response funkci")
        print("-" * 40)
        print("Tato funkce nyn√≠ automaticky pou≈æ√≠v√° apply_chat_template pokud je dostupn√©")
        
        # Simulace vol√°n√≠ generate_response (bez skuteƒçn√©ho modelu)
        if hasattr(tokenizer, 'apply_chat_template'):
            print("‚úÖ generate_response bude pou≈æ√≠vat apply_chat_template")
            messages = [{"role": "user", "content": test_prompt}]
            expected_formatted = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            print(f"Oƒçek√°van√Ω form√°t: '{expected_formatted}'")
        else:
            print("‚ö†Ô∏è generate_response bude pou≈æ√≠vat p≈ô√≠m√© form√°tov√°n√≠")
            print(f"Oƒçek√°van√Ω form√°t: '{test_prompt}'")
        
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi testov√°n√≠: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_prompt_formatting() 