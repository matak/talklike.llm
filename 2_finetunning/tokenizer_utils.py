import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def setup_tokenizer_and_model(model_name, base_model):
    """Nastav√≠ tokenizer a model pro fine-tuning"""
    
    # 1. Naƒçten√≠ tokenizeru
    base_tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir='/workspace/.cache/huggingface/transformers',
        local_files_only=False,
        resume_download=True,
        force_download=False
    )
    print(f"üìä P≈Øvodn√≠ d√©lka tokenizeru: {len(base_tokenizer)}")
    
    # 2. Kontrola a p≈ôid√°n√≠ pad tokenu
    if base_tokenizer.pad_token is None:
        # Zkus√≠me pou≈æ√≠t existuj√≠c√≠ tokeny
        if base_tokenizer.eos_token:
            base_tokenizer.pad_token = base_tokenizer.eos_token
            print(f"‚úÖ Pou≈æ√≠v√°m EOS token jako PAD: {base_tokenizer.pad_token}")
        else:
            # P≈ôid√°me nov√Ω pad token
            base_tokenizer.add_special_tokens({"pad_token": "<pad>"})
            print(f"‚úÖ P≈ôid√°n nov√Ω pad token: {base_tokenizer.pad_token}")
            
            # D≈Øle≈æit√©: Resize model embeddings
            base_model.resize_token_embeddings(len(base_tokenizer))
            print(f"üìä Model embeddings resized na: {len(base_tokenizer)}")
    else:
        print(f"‚ÑπÔ∏è Pad token u≈æ existuje: {base_tokenizer.pad_token}")
    
    # 3. Synchronizace s modelem
    if hasattr(base_model.config, 'pad_token_id'):
        old_pad_id = base_model.config.pad_token_id
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
        print(f"üîÑ Pad token ID zmƒõnƒõn: {old_pad_id} ‚Üí {base_model.config.pad_token_id}")
    else:
        print("‚ö†Ô∏è Model nem√° pad_token_id v config")
    
    # 4. Kontrola konzistence
    try:
        assert base_tokenizer.pad_token_id == base_model.config.pad_token_id, \
            "Tokenizer a model maj√≠ r≈Øzn√© pad token ID!"
        print(f"‚úÖ Tokenizer a model synchronizov√°ny")
    except AssertionError as e:
        print(f"‚ùå Chyba synchronizace: {e}")
        # Pokus√≠me se opravit
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
        print(f"üîß Opraveno: pad_token_id nastaven na {base_tokenizer.pad_token_id}")
    
    return base_tokenizer, base_model

def check_unknown_tokens(dataset, tokenizer, debugger=None, max_samples_to_check=100):
    """Kontroluje, zda dataset obsahuje nezn√°m√© tokeny pro tokenizer"""
    print(f"\nüîç Kontroluji nezn√°m√© tokeny v datasetu...")
    
    unknown_tokens_found = []
    total_unknown_count = 0
    samples_with_unknown = 0
    
    # Kontrolujeme pouze prvn√≠ch max_samples_to_check vzork≈Ø pro rychlost
    samples_to_check = min(max_samples_to_check, len(dataset))
    print(f"üìä Kontroluji {samples_to_check} vzork≈Ø z {len(dataset)} celkem")
    
    for i in range(samples_to_check):
        sample = dataset[i]
        input_ids = sample['input_ids']
        
        # Kontrola ka≈æd√©ho tokenu
        unknown_in_sample = []
        for token_id in input_ids:
            if token_id == tokenizer.unk_token_id:
                unknown_in_sample.append(token_id)
        
        if unknown_in_sample:
            samples_with_unknown += 1
            total_unknown_count += len(unknown_in_sample)
            unknown_tokens_found.append({
                "sample_index": i,
                "unknown_count": len(unknown_in_sample),
                "total_tokens": len(input_ids),
                "unknown_percentage": len(unknown_in_sample) / len(input_ids) * 100
            })
    
    # V√Ωpis v√Ωsledk≈Ø
    print(f"üìä V√Ωsledky kontroly nezn√°m√Ωch token≈Ø:")
    print(f"  Vzork≈Ø s nezn√°m√Ωmi tokeny: {samples_with_unknown}/{samples_to_check}")
    print(f"  Celkov√Ω poƒçet nezn√°m√Ωch token≈Ø: {total_unknown_count}")
    print(f"  Procento vzork≈Ø s nezn√°m√Ωmi tokeny: {samples_with_unknown/samples_to_check*100:.1f}%")
    
    if unknown_tokens_found:
        print(f"‚ö†Ô∏è NALEZENY NEZN√ÅM√â TOKENY!")
        print(f"üìã Detailn√≠ informace o prvn√≠ch 5 vzorc√≠ch s nezn√°m√Ωmi tokeny:")
        
        for i, info in enumerate(unknown_tokens_found[:5]):
            sample = dataset[info['sample_index']]
            decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
            
            print(f"  Vzorek {info['sample_index']}:")
            print(f"    Nezn√°m√Ωch token≈Ø: {info['unknown_count']}/{info['total_tokens']} ({info['unknown_percentage']:.1f}%)")
            print(f"    Text (prvn√≠ch 200 znak≈Ø): {decoded_text[:200]}...")
            print()
        
        # Ulo≈æen√≠ detailn√≠ch informac√≠ do debug
        if debugger:
            debugger.save_step("unknown_tokens_check", {
                "samples_checked": samples_to_check,
                "samples_with_unknown": samples_with_unknown,
                "total_unknown_count": total_unknown_count,
                "unknown_percentage": samples_with_unknown/samples_to_check*100,
                "detailed_info": unknown_tokens_found[:10],  # Prvn√≠ch 10 detail≈Ø
                "tokenizer_unk_token_id": tokenizer.unk_token_id,
                "tokenizer_unk_token": tokenizer.unk_token
            }, f"Kontrola nezn√°m√Ωch token≈Ø - nalezeno {samples_with_unknown} vzork≈Ø s nezn√°m√Ωmi tokeny")
        
        # Doporuƒçen√≠ pro opravu
        print(f"üí° Doporuƒçen√≠ pro opravu:")
        print(f"   1. Zkontrolujte form√°t dat - mo≈æn√° pou≈æ√≠v√°te ≈°patn√© tagy")
        print(f"   2. Ovƒõ≈ôte, ≈æe pou≈æ√≠v√°te spr√°vn√Ω tokenizer pro v√°≈° model")
        print(f"   3. Zkontrolujte, zda data neobsahuj√≠ speci√°ln√≠ znaky")
        print(f"   4. Pro Mistral/Llama modely zkontrolujte ChatML form√°t")
        
        # Kontrola, zda pokraƒçovat
        if samples_with_unknown > samples_to_check * 0.5:  # V√≠ce ne≈æ 50% vzork≈Ø m√° nezn√°m√© tokeny
            print(f"‚ùå KRITICK√Å CHYBA: V√≠ce ne≈æ 50% vzork≈Ø obsahuje nezn√°m√© tokeny!")
            print(f"   Zastavuji fine-tuning. Opravte data p≈ôed pokraƒçov√°n√≠m.")
            return False
        else:
            print(f"‚ö†Ô∏è VAROV√ÅN√ç: Nƒõkter√© vzorky obsahuj√≠ nezn√°m√© tokeny, ale pokraƒçuji...")
            return True
    else:
        print(f"‚úÖ ≈Ω√°dn√© nezn√°m√© tokeny nenalezeny!")
        
        if debugger:
            debugger.save_step("unknown_tokens_check", {
                "samples_checked": samples_to_check,
                "samples_with_unknown": 0,
                "total_unknown_count": 0,
                "unknown_percentage": 0.0,
                "status": "OK"
            }, "Kontrola nezn√°m√Ωch token≈Ø - ≈æ√°dn√© probl√©my")
        
        return True

def check_tokenizer_compatibility(tokenizer, model_name, debugger=None):
    """Kontroluje kompatibilitu tokenizeru s modelem"""
    print(f"\nüîß Kontroluji kompatibilitu tokenizeru s modelem...")
    
    # Detekce typu modelu
    is_mistral = "mistral" in model_name.lower()
    is_llama = "llama" in model_name.lower()
    is_dialogpt = "dialogpt" in model_name.lower()
    
    print(f"üìä Model: {model_name}")
    print(f"üìä Typ modelu: {'Mistral' if is_mistral else 'Llama' if is_llama else 'DialoGPT' if is_dialogpt else 'Unknown'}")
    print(f"üìä Vocab size: {len(tokenizer)}")
    print(f"üìä UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})")
    print(f"üìä PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"üìä EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    
    # Kontrola speci√°ln√≠ch token≈Ø
    issues = []
    
    if tokenizer.pad_token is None:
        issues.append("Chyb√≠ PAD token")
    
    if tokenizer.eos_token is None:
        issues.append("Chyb√≠ EOS token")
    
    # Kontrola oƒçek√°van√Ωch token≈Ø podle typu modelu
    if is_mistral or is_llama:
        # Mistral/Llama by mƒõl m√≠t ChatML tokeny
        if not any("[INST]" in tokenizer.decode([i]) for i in range(min(1000, len(tokenizer)))):
            issues.append("Mo≈æn√° chyb√≠ ChatML tokeny ([INST], [/INST])")
    elif is_dialogpt:
        # DialoGPT by mƒõl m√≠t speci√°ln√≠ tokeny
        if not any("<|" in tokenizer.decode([i]) for i in range(min(1000, len(tokenizer)))):
            issues.append("Mo≈æn√° chyb√≠ DialoGPT tokeny (<|system|>, <|user|>, atd.)")
    
    if issues:
        print(f"‚ö†Ô∏è Nalezeny probl√©my s tokenizerem:")
        for issue in issues:
            print(f"   - {issue}")
        
        if debugger:
            debugger.save_step("tokenizer_compatibility_check", {
                "model_name": model_name,
                "model_type": "Mistral" if is_mistral else "Llama" if is_llama else "DialoGPT" if is_dialogpt else "Unknown",
                "vocab_size": len(tokenizer),
                "unk_token": tokenizer.unk_token,
                "pad_token": tokenizer.pad_token,
                "eos_token": tokenizer.eos_token,
                "issues": issues
            }, f"Kontrola kompatibility tokenizeru - nalezeno {len(issues)} probl√©m≈Ø")
        
        return False
    else:
        print(f"‚úÖ Tokenizer je kompatibiln√≠ s modelem")
        
        if debugger:
            debugger.save_step("tokenizer_compatibility_check", {
                "model_name": model_name,
                "model_type": "Mistral" if is_mistral else "Llama" if is_llama else "DialoGPT" if is_dialogpt else "Unknown",
                "vocab_size": len(tokenizer),
                "unk_token": tokenizer.unk_token,
                "pad_token": tokenizer.pad_token,
                "eos_token": tokenizer.eos_token,
                "status": "OK"
            }, "Kontrola kompatibility tokenizeru - OK")
        
        return True

def tokenize_function(examples, tokenizer, max_length=2048):
    """Tokenizuje text pro fine-tuning"""
    # Tokenizace s padding pro konzistentn√≠ d√©lky
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding=True,  # Povol√≠me padding
        max_length=max_length,
        return_tensors=None
    )
    
    # Nastaven√≠ labels stejn√© jako input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized 