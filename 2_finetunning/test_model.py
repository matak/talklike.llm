#!/usr/bin/env python3
"""
JednoduchÃ½ skript pro testovÃ¡nÃ­ finetunovanÃ©ho modelu z Hugging Face
UmoÅ¾Åˆuje uÅ¾ivateli zadÃ¡vat prompty a zÃ­skÃ¡vat odpovÄ›di od modelu
"""

import argparse
import sys
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import warnings

# PotlaÄenÃ­ varovÃ¡nÃ­
warnings.filterwarnings("ignore")

def load_model(model_path, device="auto"):
    """NaÄte model a tokenizer z Hugging Face"""
    try:
        print(f"ğŸ¤– NaÄÃ­tÃ¡m model: {model_path}")
        print("â³ ProsÃ­m poÄkejte, naÄÃ­tÃ¡nÃ­ mÅ¯Å¾e trvat nÄ›kolik minut...")
        
        # NaÄtenÃ­ tokenizeru
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # Kontrola a nastavenÃ­ pad tokenu
        if tokenizer.pad_token is None:
            if tokenizer.eos_token:
                tokenizer.pad_token = tokenizer.eos_token
            else:
                tokenizer.add_special_tokens({"pad_token": "<pad>"})
        
        # NaÄtenÃ­ modelu
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map=device,
            trust_remote_code=True
        )
        
        # Synchronizace pad tokenu s modelem
        if hasattr(model.config, 'pad_token_id'):
            model.config.pad_token_id = tokenizer.pad_token_id
        
        print("âœ… Model ÃºspÄ›Å¡nÄ› naÄten!")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ modelu: {e}")
        return None, None

def generate_response(model, tokenizer, prompt, max_length=512, temperature=0.7):
    """Generuje odpovÄ›Ä na zÃ¡kladÄ› promptu"""
    try:
        # Tokenizace vstupu
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        )
        
        # PÅ™esun na sprÃ¡vnÃ© zaÅ™Ã­zenÃ­
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # GenerovÃ¡nÃ­
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        # DekÃ³dovÃ¡nÃ­ odpovÄ›di
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # OdstranÄ›nÃ­ pÅ¯vodnÃ­ho promptu z odpovÄ›di
        if response.startswith(prompt):
            response = response[len(prompt):].strip()
        
        return response
        
    except Exception as e:
        return f"âŒ Chyba pÅ™i generovÃ¡nÃ­: {e}"

def main():
    parser = argparse.ArgumentParser(
        description="TestovÃ¡nÃ­ finetunovanÃ©ho modelu z Hugging Face",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
PÅ™Ã­klady pouÅ¾itÃ­:
  python test_model.py microsoft/DialoGPT-medium
  python test_model.py username/my-finetuned-model
  python test_model.py --device cpu username/my-model
        """
    )
    
    parser.add_argument(
        "model_path",
        help="Cesta k modelu na Hugging Face (repo/model_name)"
    )
    
    parser.add_argument(
        "--device",
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="ZaÅ™Ã­zenÃ­ pro inference (default: auto)"
    )
    
    parser.add_argument(
        "--max-length",
        type=int,
        default=512,
        help="MaximÃ¡lnÃ­ dÃ©lka generovanÃ© odpovÄ›di (default: 512)"
    )
    
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Teplota pro generovÃ¡nÃ­ (default: 0.7)"
    )
    
    args = parser.parse_args()
    
    # NaÄtenÃ­ modelu
    model, tokenizer = load_model(args.model_path, args.device)
    
    if model is None or tokenizer is None:
        sys.exit(1)
    
    print("\n" + "="*60)
    print("ğŸ’¬ INTERAKTIVNÃ TESTOVÃNÃ MODELU")
    print("="*60)
    print("ğŸ“ NapiÅ¡te svÅ¯j dotaz a stisknÄ›te Enter")
    print("ğŸ”§ Pro ukonÄenÃ­ stisknÄ›te Ctrl+C")
    print("="*60)
    
    # NekoneÄnÃ¡ smyÄka pro dotazovÃ¡nÃ­
    while True:
        try:
            # Vstup uÅ¾ivatele
            user_input = input("\nğŸ‘¤ Vy: ").strip()
            
            # PrÃ¡zdnÃ½ vstup
            if not user_input:
                continue
            
            # GenerovÃ¡nÃ­ odpovÄ›di
            print("ğŸ¤– Model generuje odpovÄ›Ä...")
            response = generate_response(
                model, tokenizer, user_input,
                args.max_length, args.temperature
            )
            
            print(f"ğŸ¤– Model: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Na shledanou!")
            break
        except Exception as e:
            print(f"âŒ Chyba: {e}")

if __name__ == "__main__":
    main() 