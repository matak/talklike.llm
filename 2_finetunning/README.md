# ğŸ‹ï¸ Fine-tuning JazykovÃ©ho Modelu - TalkLike.LLM

> **ğŸ“š Navigace:** [ğŸ  HlavnÃ­ projekt](../README.md) | [ğŸ“Š PÅ™Ã­prava dat](../1_data_preparation/README.md) | [ğŸ“ˆ Benchmarking](../3_benchmarking/README.md)

## ğŸ“‹ PÅ™ehled

Tento projekt implementuje **fine-tuning jazykovÃ©ho modelu** pomocÃ­ LoRA (Low-Rank Adaptation) techniky pro napodobenÃ­ komunikaÄnÃ­ho stylu Andreje BabiÅ¡e. Fine-tuning je optimalizovÃ¡n pro efektivnÃ­ trÃ©novÃ¡nÃ­ na RunPod.io nebo lokÃ¡lnÃ­ch GPU.

### ğŸ¯ CÃ­l
VytvoÅ™it fine-tuned model, kterÃ½:
- âœ… MluvÃ­ autentickÃ½m stylem Andreje BabiÅ¡e
- âœ… PouÅ¾Ã­vÃ¡ charakteristickÃ© frÃ¡ze a rÃ©torickÃ© prvky
- âœ… Generuje konzistentnÃ­ odpovÄ›di v prvnÃ­ osobÄ›
- âœ… ZachovÃ¡vÃ¡ "babÃ­Å¡ovÅ¡tinu" s jazykovÃ½mi odchylkami

---

## ğŸ—ï¸ Architektura Å™eÅ¡enÃ­

### ZÃ¡kladnÃ­ konfigurace
- **Base Model**: Meta-Llama-3-8B-Instruct
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Data Format**: JSONL s chat formÃ¡tem
- **Target**: StylovÃ¡ adaptace pro Andreje BabiÅ¡e

### LoRA Konfigurace
```python
lora_config = {
    "r": 16,                    # Rank
    "alpha": 32,                # Scaling factor
    "dropout": 0.1,             # Dropout rate
    "target_modules": [         # Target layers
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
}
```

### Training Parametry
```python
training_config = {
    "epochs": 3,
    "batch_size": 2,            # per device
    "gradient_accumulation": 4,
    "learning_rate": 2e-4,
    "warmup_steps": 100,
    "max_length": 2048,
    "save_steps": 500,
    "eval_steps": 500
}
```

---

## ğŸš€ RychlÃ© spuÅ¡tÄ›nÃ­

### 1. PÅ™Ã­prava prostÅ™edÃ­
```bash
# Instalace zÃ¡vislostÃ­
pip install -r requirements_finetunning.txt

# NastavenÃ­ environment promÄ›nnÃ½ch
echo "HF_TOKEN=your_hf_token_here" >> .env
```

### 2. SpuÅ¡tÄ›nÃ­ fine-tuning

```bash
# ZÃ¡kladnÃ­ fine-tuning
python finetune.py --push_to_hub

# Fine-tuning s vlastnÃ­mi parametry
python finetune.py \
    --model_name microsoft/DialoGPT-medium \
    --epochs 3 \
    --batch_size 2 \
    --learning_rate 2e-4 \
    --push_to_hub \
    --hub_model_id babis-lora

# Fine-tuning s vlastnÃ­mi parametry
python 2_finetunning/finetune.py \
    --data_path data/all.jsonl \
    --output_dir /workspace/mistral-babis-finetuned \
    --model_name mistralai/Mistral-7B-Instruct-v0.3 \
    --epochs 3 \
    --batch_size 1 \
    --learning_rate 1e-4 \
    --max_length 2048 \
    --aggressive_cleanup \
    --push_to_hub \
    --hub_model_id mistral-babis-lora
```

### 3. TestovÃ¡nÃ­ tokenizace
```bash
# OvÄ›Å™enÃ­ sprÃ¡vnÃ© tokenizace dat
python test_tokenization.py
```

---

## ğŸ“Š Struktura dat

### VstupnÃ­ formÃ¡t
Dataset v `../data/all.jsonl` mÃ¡ strukturu:

```json
{
    "messages": [
        {
            "role": "system",
            "content": "Jsi Andrej BabiÅ¡, ÄeskÃ½ politik a podnikatel..."
        },
        {
            "role": "user",
            "content": "Pane BabiÅ¡i, mÅ¯Å¾ete vysvÄ›tlit vaÅ¡i roli v tÃ© chemiÄce?"
        },
        {
            "role": "assistant",
            "content": "Hele, ta tovÃ¡rna? To uÅ¾ jsem dÃ¡vno pÅ™edal..."
        }
    ]
}
```

### KlÃ­ÄovÃ© vlastnosti dat
- **PoÄet QA pÃ¡rÅ¯**: 1,500
- **Styl**: AutentickÃ½ BabiÅ¡Å¯v styl
- **JazykovÃ© chyby**: 15% pravdÄ›podobnost slovenskÃ½ch odchylek
- **TÃ©mata**: Politika, ekonomika, rodina, podnikÃ¡nÃ­

---

## ğŸ’» SpuÅ¡tÄ›nÃ­ na RunPod.io

### 1. VytvoÅ™enÃ­ kontejneru
- Image: `runpod/pytorch:2.1.1-py3.10-cuda12.1.0`
- GPU: RTX 4090 nebo A100
- Disk: 50GB+

### 2. NastavenÃ­ environment promÄ›nnÃ½ch
```bash
# V kontejneru
export HF_TOKEN=your_hf_token_here
```

### 3. SpuÅ¡tÄ›nÃ­
```bash
# KlonovÃ¡nÃ­ repozitÃ¡Å™e
git clone https://github.com/your-repo/talklike.llm.git
cd talklike.llm

# SpuÅ¡tÄ›nÃ­ fine-tuning
bash 2_finetunning/run_finetune.sh
```

## ğŸ“Š Monitoring

Fine-tuning automaticky:
- âœ… **Loguje metriky** do `/workspace/babis-finetuned/logs/`
- âœ… **UklÃ¡dÃ¡ checkpointy** kaÅ¾dÃ½ch 500 krokÅ¯
- âœ… **NaÄÃ­tÃ¡ nejlepÅ¡Ã­ model** na konci trÃ©novÃ¡nÃ­
- âœ… **Exportuje model** na Hugging Face Hub (pokud povoleno)

## ğŸ”§ PokroÄilÃ© nastavenÃ­

### Optimalizace pro velkÃ© modely
```bash
python finetune.py \
    --model_name mistralai/Mistral-7B-Instruct-v0.3 \
    --aggressive_cleanup \
    --batch_size 1 \
    --max_length 2048
```

### VlastnÃ­ LoRA konfigurace
Upravte `finetune.py`:
```python
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,  # ZvÃ½Å¡it pro lepÅ¡Ã­ kvalitu
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=target_modules
)
```

## ğŸ› Å˜eÅ¡enÃ­ problÃ©mÅ¯

### Nedostatek mÃ­sta na disku
```bash
# AutomatickÃ© vyÄiÅ¡tÄ›nÃ­
python finetune.py --aggressive_cleanup

# ManuÃ¡lnÃ­ vyÄiÅ¡tÄ›nÃ­
rm -rf /root/.cache/huggingface
rm -rf /tmp/*
```

### Chyby pÅ™i naÄÃ­tÃ¡nÃ­ modelu
```bash
# PouÅ¾Ã­t menÅ¡Ã­ model
python finetune.py --model_name microsoft/DialoGPT-medium

# Restartovat kontejner
# ZvÃ½Å¡it velikost root filesystem
```

### ProblÃ©my s tokenizerem
```bash
# Kontrola kompatibility
python test_tokenization.py

# PouÅ¾Ã­t jinÃ½ model
python finetune.py --model_name microsoft/DialoGPT-large
```

## ğŸ“ Struktura vÃ½stupu

```
/workspace/babis-finetuned/
â”œâ”€â”€ checkpoint-500/          # Checkpointy
â”œâ”€â”€ checkpoint-1000/
â”œâ”€â”€ logs/                    # Logy trÃ©novÃ¡nÃ­
â”œâ”€â”€ pytorch_model.bin        # FinÃ¡lnÃ­ model
â”œâ”€â”€ config.json             # Konfigurace
â”œâ”€â”€ tokenizer.json          # Tokenizer
â””â”€â”€ adapter_config.json     # LoRA konfigurace
```

## ğŸ”— UÅ¾iteÄnÃ© odkazy

- [Hugging Face Hub](https://huggingface.co/) - NahrÃ¡vÃ¡nÃ­ modelÅ¯
- [RunPod.io](https://runpod.io/) - GPU hosting
- [LoRA Paper](https://arxiv.org/abs/2106.09685) - Teorie LoRA
- [PEFT Dokumentace](https://huggingface.co/docs/peft) - Fine-tuning knihovna
