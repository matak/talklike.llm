# ğŸ‹ï¸ Fine-tuning JazykovÃ©ho Modelu - TalkLike.LLM

## ğŸ“‹ PÅ™ehled

Tento projekt implementuje **fine-tuning jazykovÃ©ho modelu** pomocÃ­ LoRA (Low-Rank Adaptation) techniky pro napodobenÃ­ komunikaÄnÃ­ho stylu Andreje BabiÅ¡e. Fine-tuning je optimalizovÃ¡n pro efektivnÃ­ trÃ©novÃ¡nÃ­ na RunPod.io nebo lokÃ¡lnÃ­ch GPU.

### ğŸ¯ CÃ­l
VytvoÅ™it fine-tuned model, kterÃ½:
- âœ… MluvÃ­ autentickÃ½m stylem Andreje BabiÅ¡e
- âœ… PouÅ¾Ã­vÃ¡ charakteristickÃ© frÃ¡ze a rÃ©torickÃ© prvky
- âœ… Generuje konzistentnÃ­ odpovÄ›di v prvnÃ­ osobÄ›
- âœ… ZachovÃ¡vÃ¡ "babÃ­Å¡ovÅ¡tinu" s jazykovÃ½mi odchylkami

---

## ğŸ—ï¸ Architektura Å™eÅ¡enÃ­

### ZÃ¡kladnÃ­ konfigurace
- **Base Model**: Meta-Llama-3-8B-Instruct
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Data Format**: JSONL s chat formÃ¡tem
- **Target**: StylovÃ¡ adaptace pro Andreje BabiÅ¡e

### LoRA Konfigurace
```python
lora_config = {
    "r": 16,                    # Rank
    "alpha": 32,                # Scaling factor
    "dropout": 0.1,             # Dropout rate
    "target_modules": [         # Target layers
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
}
```

### Training Parametry
```python
training_config = {
    "epochs": 3,
    "batch_size": 2,            # per device
    "gradient_accumulation": 4,
    "learning_rate": 2e-4,
    "warmup_steps": 100,
    "max_length": 2048,
    "save_steps": 500,
    "eval_steps": 500
}
```

---

## ğŸš€ RychlÃ© spuÅ¡tÄ›nÃ­

### 1. PÅ™Ã­prava prostÅ™edÃ­
```bash
# Instalace zÃ¡vislostÃ­
pip install -r requirements_finetunning.txt

# NastavenÃ­ tokenÅ¯
echo "HF_TOKEN=hf_your_token_here" > .env
echo "WANDB_API_KEY=your_wandb_token_here" >> .env
```

### 2. SpuÅ¡tÄ›nÃ­ fine-tuningu
```bash
# RychlÃ½ start (doporuÄeno)
chmod +x run_finetune.sh
./run_finetune.sh

# Nebo manuÃ¡lnÄ›
python finetune_babis.py --use_wandb --push_to_hub
```

### 3. TestovÃ¡nÃ­ tokenizace
```bash
# OvÄ›Å™enÃ­ sprÃ¡vnÃ© tokenizace dat
python test_tokenization.py
```

---

## ğŸ“Š Struktura dat

### VstupnÃ­ formÃ¡t
Dataset v `../data/all.jsonl` mÃ¡ strukturu:

```json
{
    "messages": [
        {
            "role": "system",
            "content": "Jsi Andrej BabiÅ¡, ÄeskÃ½ politik a podnikatel..."
        },
        {
            "role": "user",
            "content": "Pane BabiÅ¡i, mÅ¯Å¾ete vysvÄ›tlit vaÅ¡i roli v tÃ© chemiÄce?"
        },
        {
            "role": "assistant",
            "content": "Hele, ta tovÃ¡rna? To uÅ¾ jsem dÃ¡vno pÅ™edal..."
        }
    ]
}
```

### KlÃ­ÄovÃ© vlastnosti dat
- **PoÄet QA pÃ¡rÅ¯**: 1,500
- **Styl**: MluvenÃ¡ ÄeÅ¡tina s "babÃ­Å¡ovÅ¡tinou"
- **Podpis**: KaÅ¾dÃ¡ odpovÄ›Ä konÄÃ­ "Andrej BabiÅ¡"
- **JazykovÃ© chyby**: 15% pravdÄ›podobnost slovenskÃ½ch odchylek
- **TÃ©mata**: Politika, ekonomika, rodina, podnikÃ¡nÃ­

---

## ğŸ’» SpuÅ¡tÄ›nÃ­ na RunPod.io

### 1. VytvoÅ™enÃ­ podu
1. JdÄ›te na [runpod.io](https://runpod.io)
2. VytvoÅ™te novÃ½ pod s nÃ¡sledujÃ­cÃ­mi specifikacemi:
   - **GPU**: RTX 4090 nebo A100 (doporuÄeno)
   - **RAM**: MinimÃ¡lnÄ› 24GB
   - **Storage**: MinimÃ¡lnÄ› 50GB
   - **Template**: PyTorch nebo Jupyter

### 2. PÅ™Ã­prava prostÅ™edÃ­
```bash
# Aktualizace systÃ©mu
sudo apt update && sudo apt upgrade -y

# Instalace balÃ­ÄkÅ¯
sudo apt install -y git wget curl

# KlonovÃ¡nÃ­ repozitÃ¡Å™e
git clone <your-repo-url>
cd talklike.llm

# VytvoÅ™enÃ­ .env souboru
nano .env
# PÅ™idejte vaÅ¡e tokeny:
# HF_TOKEN=hf_your_token_here
# WANDB_API_KEY=your_wandb_token_here

# SpuÅ¡tÄ›nÃ­ fine-tuningu
./run_finetune.sh
```

### 3. MonitorovÃ¡nÃ­ trÃ©novÃ¡nÃ­
- **W&B Dashboard**: Sledujte metriky na wandb.ai
- **Jupyter**: Sledujte progress v notebooku
- **Terminal**: Logy v terminÃ¡lu
- **GPU Monitoring**: `nvidia-smi -l 1`

---

## ğŸ“ Struktura projektu

```
2_finetunning/
â”œâ”€â”€ ğŸ“„ HlavnÃ­ skripty
â”‚   â”œâ”€â”€ finetune_babis.py          # Main fine-tuning script
â”‚   â”œâ”€â”€ test_tokenization.py       # Tokenization testing
â”‚   â””â”€â”€ run_finetune.sh            # Fine-tuning shell script
â”œâ”€â”€ ğŸ“„ AlternativnÃ­ skripty
â”‚   â””â”€â”€ run_mistral_finetune.sh    # Mistral fine-tuning script
â”œâ”€â”€ ğŸ“„ Konfigurace
â”‚   â”œâ”€â”€ requirements_finetunning.txt # Python dependencies
â”‚   â””â”€â”€ README_FINETUNE.md         # This file
â”œâ”€â”€ ğŸ“„ Dokumentace
â”‚   â””â”€â”€ RUNPOD_SETUP.md           # RunPod.io instructions
â””â”€â”€ ğŸ“„ VÃ½stupy
    â””â”€â”€ babis-llama-finetuned/    # Fine-tuned model
```

---

## âš™ï¸ Konfigurace

### ZÃ¡kladnÃ­ parametry
```bash
python finetune_babis.py \
    --data_path ../data/all.jsonl \
    --output_dir ./babis-llama-finetuned \
    --epochs 3 \
    --batch_size 2 \
    --learning_rate 2e-4 \
    --max_length 2048
```

### PokroÄilÃ© parametry
```bash
python finetune_babis.py \
    --data_path ../data/all.jsonl \
    --output_dir ./babis-llama-finetuned \
    --model_name meta-llama/Meta-Llama-3-8B-Instruct \
    --epochs 5 \
    --batch_size 1 \
    --learning_rate 1e-4 \
    --max_length 1024 \
    --use_wandb \
    --push_to_hub \
    --hub_model_id your-username/babis-llama-3-8b-lora
```

### Parametry pro rÅ¯znÃ© GPU
```bash
# RTX 4090 (24GB VRAM)
python finetune_babis.py --batch_size 2 --max_length 2048

# RTX 3090 (24GB VRAM)
python finetune_babis.py --batch_size 1 --max_length 1024

# A100 (40GB VRAM)
python finetune_babis.py --batch_size 4 --max_length 2048
```

---

## ğŸ”§ Troubleshooting

### Out of Memory (OOM)
```bash
# SniÅ¾te batch size a max_length
python finetune_babis.py --batch_size 1 --max_length 1024

# Nebo sniÅ¾te gradient accumulation
# Upravte v kÃ³du: gradient_accumulation_steps=2
```

### PomalÃ© trÃ©novÃ¡nÃ­
```bash
# Zkontrolujte GPU vyuÅ¾itÃ­
nvidia-smi

# Optimalizujte dataloader
# Upravte num_workers v DataLoader
```

### Model nekonverguje
```bash
# SniÅ¾te learning rate
python finetune_babis.py --learning_rate 1e-4

# ZvyÅ¡te poÄet epoch
python finetune_babis.py --epochs 5

# Zkontrolujte kvalitu dat
python test_tokenization.py
```

### Chyby s tokeny
```bash
# OvÄ›Å™te HF token
huggingface-cli whoami

# OvÄ›Å™te W&B token
wandb login
```

---

## ğŸ“ˆ OÄekÃ¡vanÃ© vÃ½sledky

### Metriky vÃ½konu
Po fine-tuningu by model mÄ›l dosÃ¡hnout:
- **Training Loss**: < 1.0 po 3 epochÃ¡ch
- **Validation Loss**: < 1.2
- **Perplexity**: < 2.0
- **StylovÃ¡ konzistence**: > 85%

### KvalitativnÃ­ evaluace
Model by mÄ›l:
- âœ… Mluvit stylem Andreje BabiÅ¡e
- âœ… PouÅ¾Ã­vat charakteristickÃ© frÃ¡ze
- âœ… OdpovÃ­dat v prvnÃ­ osobÄ›
- âœ… PÅ™idÃ¡vat podpis "Andrej BabiÅ¡"
- âœ… Obsahovat slovenskÃ© odchylky

### TestovacÃ­ prompty
```python
test_prompts = [
    "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?",
    "Co si myslÃ­te o opozici?",
    "Jak se vÃ¡m daÅ™Ã­ v Bruselu?",
    "MÅ¯Å¾ete vysvÄ›tlit vaÅ¡i roli v tÃ© chemiÄce?",
    "Jak hodnotÃ­te efektivizaci stÃ¡tnÃ­ sprÃ¡vy?"
]
```

---

## ğŸ’¾ UloÅ¾enÃ­ a sdÃ­lenÃ­

### LokÃ¡lnÃ­ uloÅ¾enÃ­
```bash
# Model se uloÅ¾Ã­ do ./babis-llama-finetuned-final/
# Obsahuje LoRA adaptÃ©ry a konfiguraci
```

### Hugging Face Hub
```bash
# Model se automaticky nahraje na HF Hub
# Repo: https://huggingface.co/your-username/babis-llama-3-8b-lora
# Obsahuje: LoRA adaptÃ©ry, konfiguraci, README
```

### Struktura uloÅ¾enÃ©ho modelu
```
babis-llama-finetuned-final/
â”œâ”€â”€ adapter_config.json          # LoRA konfigurace
â”œâ”€â”€ adapter_model.bin            # LoRA vÃ¡hy
â”œâ”€â”€ training_args.bin            # Training argumenty
â”œâ”€â”€ config.json                  # Model konfigurace
â””â”€â”€ README.md                    # Model dokumentace
```

---

## ğŸ“¤ ManuÃ¡lnÃ­ nahrÃ¡nÃ­ na Hugging Face Hub

### Kdy pouÅ¾Ã­t
- ZapomnÄ›li jste `--push_to_hub` pÅ™i fine-tuningu
- Chcete nahrÃ¡t jiÅ¾ existujÃ­cÃ­ model
- PotÅ™ebujete zmÄ›nit nÃ¡zev modelu

### RychlÃ© nahrÃ¡nÃ­
```bash
# 1. NastavenÃ­ tokenu
export HF_TOKEN=hf_your_token_here

# 2. NahrÃ¡nÃ­ modelu
python upload_to_hf.py \
    --model_path /workspace/babis-finetuned-final \
    --hub_model_id your-username/babis-model

# 3. Kontrola bez nahrÃ¡vÃ¡nÃ­
python upload_to_hf.py \
    --model_path /workspace/babis-finetuned-final \
    --hub_model_id your-username/babis-model \
    --check_only
```

### BÄ›Å¾nÃ© cesty k modelu
- `/workspace/babis-finetuned-final`
- `/workspace/babis-mistral-finetuned-final`
- `./babis-llama-finetuned-final`

### VÃ½stup
- âœ… Model dostupnÃ½ na: `https://huggingface.co/your-username/babis-model`
- ğŸ“‹ Instrukce pro pouÅ¾itÃ­ modelu

---

## ğŸ§ª PouÅ¾itÃ­ fine-tuned modelu

### NaÄtenÃ­ a generovÃ¡nÃ­
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# NaÄtenÃ­ base modelu
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B-Instruct"
)
tokenizer = AutoTokenizer.from_pretrained(
    "meta-llama/Meta-Llama-3-8B-Instruct"
)

# NaÄtenÃ­ LoRA adaptÃ©rÅ¯
model = PeftModel.from_pretrained(
    base_model, 
    "your-username/babis-llama-3-8b-lora"
)

# GenerovÃ¡nÃ­
prompt = "Pane BabiÅ¡i, jak hodnotÃ­te inflaci?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    **inputs, 
    max_length=200,
    temperature=0.7,
    do_sample=True
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### StreamovanÃ© generovÃ¡nÃ­
```python
from transformers import TextIteratorStreamer
import threading

streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
generation_kwargs = dict(
    inputs=inputs,
    streamer=streamer,
    max_length=200,
    temperature=0.7,
    do_sample=True
)

thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

for text in streamer:
    print(text, end="", flush=True)
```
