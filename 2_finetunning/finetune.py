#!/usr/bin/env python3
"""
Minim√°ln√≠ fine-tuning script pro model s daty Andreje Babi≈°e
Spustiteln√Ω na RunPod.io nebo lok√°lnƒõ
"""

# Filtrov√°n√≠ varov√°n√≠
import warnings
warnings.filterwarnings("ignore", message="Using `TRANSFORMERS_CACHE` is deprecated")
warnings.filterwarnings("ignore", message="Failed to load image Python extension")

# Import setup_environment pro spr√°vn√© nastaven√≠ prost≈ôed√≠
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import setup_environment

import os
import torch
import numpy as np
from datetime import datetime
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
from dotenv import load_dotenv
from huggingface_hub import login
import argparse
import json

# Import disk manager knihovny pro specifick√© operace
from lib.disk_manager import DiskManager

# Import modul≈Ø
from data_utils import load_model_data, prepare_training_data
from tokenizer_utils import setup_tokenizer_and_model, check_unknown_tokens, check_tokenizer_compatibility, tokenize_function
from debug_utils import DatasetDebugger
from train_utils import generate_response, test_model, save_model_info

def save_dataset_to_file(dataset, filepath, description):
    """Ulo≈æ√≠ kompletn√≠ dataset do JSON souboru pro debug"""
    print(f"üíæ Ukl√°d√°m {description} do: {filepath}")
    
    # P≈ôevod datasetu na list slovn√≠k≈Ø
    data_list = []
    for i, item in enumerate(dataset):
        data_list.append({
            "index": i,
            "text": item.get("text", ""),
            "input_ids": item.get("input_ids", []),
            "attention_mask": item.get("attention_mask", []),
            "labels": item.get("labels", [])
        })
    
    # Ulo≈æen√≠ do JSON souboru
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump({
            "description": description,
            "total_samples": len(data_list),
            "data": data_list
        }, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ {description} ulo≈æeno: {len(data_list)} vzork≈Ø")

def save_vocabulary_to_file(tokenizer, filepath):
    """Ulo≈æ√≠ kompletn√≠ slovn√≠k token≈Ø do souboru pro debug"""
    print(f"üíæ Ukl√°d√°m kompletn√≠ slovn√≠k token≈Ø do: {filepath}")
    
    vocab_data = {
        "vocab_size": tokenizer.vocab_size,
        "model_name": tokenizer.name_or_path,
        "special_tokens": {
            "pad_token": tokenizer.pad_token,
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token": tokenizer.eos_token,
            "eos_token_id": tokenizer.eos_token_id,
            "bos_token": tokenizer.bos_token,
            "bos_token_id": tokenizer.bos_token_id,
            "unk_token": tokenizer.unk_token,
            "unk_token_id": tokenizer.unk_token_id,
        },
        "vocabulary": {}
    }
    
    # Naƒçten√≠ v≈°ech token≈Ø ze slovn√≠ku
    for token_id in range(tokenizer.vocab_size):
        try:
            token = tokenizer.convert_ids_to_tokens(token_id)
            vocab_data["vocabulary"][str(token_id)] = {
                "token": token,
                "decoded": tokenizer.decode([token_id], skip_special_tokens=False)
            }
        except Exception as e:
            vocab_data["vocabulary"][str(token_id)] = {
                "token": f"ERROR_{token_id}",
                "error": str(e)
            }
    
    # Ulo≈æen√≠ do JSON souboru
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(vocab_data, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Slovn√≠k ulo≈æen: {tokenizer.vocab_size} token≈Ø")

def ask_user_continue(prompt="Pokraƒçovat ve zpracov√°n√≠?"):
    """Zept√° se u≈æivatele, zda m√° pokraƒçovat"""
    print(f"\n‚è∏Ô∏è {prompt}")
    print("   Stisknƒõte ENTER pro pokraƒçov√°n√≠ nebo napi≈°te 'stop' pro ukonƒçen√≠...")
    
    try:
        user_input = input().strip().lower()
        if user_input in ['stop', 'exit', 'quit', 'no', 'n']:
            print("üõë Ukonƒçuji skript na ≈æ√°dost u≈æivatele.")
            return False
        else:
            print("‚úÖ Pokraƒçuji ve zpracov√°n√≠...")
            return True
    except KeyboardInterrupt:
        print("\nüõë Ukonƒçeno u≈æivatelem (Ctrl+C)")
        return False
    except EOFError:
        print("\nüõë Ukonƒçeno u≈æivatelem")
        return False

def main():
    # Kontrola, ≈æe jsme v root directory projektu
    if not os.path.exists('lib') or not os.path.exists('data'):
        print("‚ùå Skript mus√≠ b√Ωt spu≈°tƒõn z root directory projektu!")
        print("üí° Spus≈•te skript z adres√°≈ôe, kde jsou slo≈æky 'lib' a 'data'")
        print(f"üìç Aktu√°ln√≠ adres√°≈ô: {os.getcwd()}")
        return
    
    parser = argparse.ArgumentParser(description='Fine-tuning pro Andreje Babi≈°e')
    parser.add_argument('--data_path', type=str, default='data/all.jsonl', help='Cesta k dat≈Øm')
    parser.add_argument('--output_dir', type=str, default='/workspace/babis-finetuned', help='V√Ωstupn√≠ adres√°≈ô')
    parser.add_argument('--model_name', type=str, default='mistralai/Mistral-7B-Instruct-v0.3', help='N√°zev base modelu')
    parser.add_argument('--epochs', type=int, default=3, help='Poƒçet epoch')
    parser.add_argument('--batch_size', type=int, default=2, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=2e-4, help='Learning rate')
    parser.add_argument('--max_length', type=int, default=1024, help='Maxim√°ln√≠ d√©lka sekvence')
    parser.add_argument('--push_to_hub', action='store_true', help='Nahr√°t model na HF Hub')
    parser.add_argument('--hub_model_id', type=str, default='babis-lora', help='N√°zev modelu na HF Hub')
    parser.add_argument('--aggressive_cleanup', action='store_true', help='Agresivn√≠ vyƒçi≈°tƒõn√≠ pro velk√© modely')
    parser.add_argument('--no_interactive', action='store_true', help='Bez interaktivn√≠ch dotaz≈Ø')
    
    args = parser.parse_args()
    
    # Zajist√≠me, ≈æe v√Ωstupn√≠ adres√°≈ô je na network storage
    if not args.output_dir.startswith('/workspace'):
        args.output_dir = f'/workspace/{args.output_dir.lstrip("./")}'
    
    print("üöÄ Spou≈°t√≠m fine-tuning pro Andreje Babi≈°e")
    print(f"üìÅ Data: {args.data_path}")
    print(f"üìÅ V√Ωstup: {args.output_dir}")
    print(f"üìÅ Model: {args.model_name}")
    
    # Inicializace disk manageru pro specifick√© operace
    dm = DiskManager()
    
    # Optimalizace pro velk√© modely (setup_environment.py u≈æ udƒõlal z√°kladn√≠ nastaven√≠)
    if args.aggressive_cleanup or "mistral" in args.model_name.lower() or "llama" in args.model_name.lower():
        print("üßπ Optimalizace pro velk√Ω model...")
        if not dm.optimize_for_large_models(args.model_name):
            print("‚ùå Nedost m√≠sta pro velk√Ω model. Zkuste men≈°√≠ model.")
            return
    
    # Naƒçten√≠ promƒõnn√Ωch prost≈ôed√≠
    load_dotenv()

    
    for i in range(torch.cuda.device_count()):
        print(torch.cuda.get_device_properties(i).name)
    
    # Hugging Face token
    HF_TOKEN = os.getenv("HF_TOKEN")
    if HF_TOKEN:
        login(token=HF_TOKEN)
        print("‚úÖ Hugging Face login √∫spƒõ≈°n√Ω")
    else:
        print("‚ö†Ô∏è HF_TOKEN nebyl nalezen")
    
    # Inicializace debuggeru pro sledov√°n√≠ zpracov√°n√≠ datasetu
    debugger = DatasetDebugger(debug_dir="debug_dataset_finetune")
    print(f"üîç Debugger inicializov√°n: {debugger.debug_dir}")
    
    # 1. Naƒçten√≠ dat
    print("\nüìä Naƒç√≠t√°m data...")
    conversations = load_model_data(args.data_path, debugger)
    print(f"‚úÖ Naƒçteno {len(conversations)} konverzac√≠")
    
    # 2. Naƒçten√≠ modelu a tokenizeru (p≈ôed p≈ô√≠pravou dat)
    print(f"\nü§ñ Naƒç√≠t√°m model: {args.model_name}")

    # Konfigurace 4-bit kvantizace
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
    )

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        cache_dir='/workspace/.cache/huggingface/transformers',
        local_files_only=False,
        resume_download=True,
        force_download=False
    )
    print("‚úÖ Model √∫spƒõ≈°nƒõ naƒçten!")

    tokenizer, model = setup_tokenizer_and_model(args.model_name, model)
    
    print(f"‚úÖ Model naƒçten. Vocab size: {model.config.vocab_size}")
    
    # 3. P≈ô√≠prava dat s tokenizerem (nyn√≠ m√°me p≈ô√≠stup k apply_chat_template)
    print("üîß P≈ôipravuji data s apply_chat_template...")
    training_data = prepare_training_data(conversations, debugger, args.model_name, tokenizer)
    print(f"‚úÖ P≈ôipraveno {len(training_data)} tr√©novac√≠ch vzork≈Ø")

    # DEBUG: Test generov√°n√≠ p≈ôed fine-tuningem
    print("\nüß™ DEBUG: Testuji generov√°n√≠ p≈ôed fine-tuningem...")
    try:
        # Vytvo≈ôen√≠ testovac√≠ho promptu
        test_prompt = "Pane Babi≈°i, jak hodnot√≠te souƒçasnou inflaci?"
        print(f"Testovac√≠ prompt: {test_prompt}")
        
        # Tokenizace promptu
        input_ids = tokenizer(test_prompt, return_tensors="pt").input_ids.to(model.device)
        print(f"Input IDs shape: {input_ids.shape}")
        
        # Generov√°n√≠ textu
        print("Generuji text...")
        with torch.no_grad():
            result = model.generate(input_ids, max_length=300, do_sample=True, temperature=0.7)
        print(f"Generated result shape: {result.shape}")
        
        # Dek√≥dov√°n√≠ a v√Ωpis generovan√©ho textu
        output_text = tokenizer.decode(result[0], skip_special_tokens=True)
        print("Answer:")
        print(output_text)
        print("-" * 50)
        
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi debug generov√°n√≠: {e}")

    # 4. Vytvo≈ôen√≠ Dataset
    dataset = Dataset.from_list(training_data)
    
    # Debug: Ulo≈æen√≠ fin√°ln√≠ho datasetu
    debugger.save_step("07_final_dataset", {"dataset_size": len(dataset), "columns": dataset.column_names}, 
                      f"Fin√°ln√≠ dataset s {len(dataset)} vzorky (text form√°t z apply_chat_template)")
    
    # 5. Konfigurace LoRA
    print("\nüîß Konfiguruji LoRA...")
    
    # Dynamick√© nastaven√≠ target_modules podle typu modelu
    if "dialogpt" in args.model_name.lower():
        target_modules = ["c_attn", "c_proj", "wte", "wpe"]
    elif "gpt2" in args.model_name.lower():
        target_modules = ["c_attn", "c_proj", "wte", "wpe"]
    else:
        target_modules = [
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"
        ]
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,  # Men≈°√≠ r pro √∫sporu pamƒõti
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=target_modules
    )
    
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 6. Tokenizace dat
    print("\nüî§ Tokenizuji data...")
    tokenize_func = lambda examples: tokenize_function(examples, tokenizer, args.max_length)
    tokenized_dataset = dataset.map(
        tokenize_func,
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=100  # Men≈°√≠ batch size pro lep≈°√≠ kontrolu
    )
    
    # Debug: Ulo≈æen√≠ tokenizovan√©ho datasetu
    debugger.save_step("09_tokenized_dataset", {
        "dataset_size": len(tokenized_dataset),
        "columns": tokenized_dataset.column_names,
        "max_length": args.max_length,
        "method": "standard_tokenization"
    }, f"Tokenizovan√Ω dataset s {len(tokenized_dataset)} vzorky (text ji≈æ form√°tov√°n pomoc√≠ apply_chat_template)")
    
    # Rozdƒõlen√≠ na train/validation
    print(f"üìä Celkov√Ω poƒçet vzork≈Ø: {len(tokenized_dataset)}")
    
    if len(tokenized_dataset) < 5:
        print("‚ö†Ô∏è M√°lo vzork≈Ø pro rozdƒõlen√≠. Pou≈æ√≠v√°m cel√Ω dataset pro tr√©nov√°n√≠.")
        train_dataset = tokenized_dataset
        eval_dataset = tokenized_dataset
    elif len(tokenized_dataset) < 10:
        split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
        train_dataset = split_dataset["train"]
        eval_dataset = split_dataset["test"]
        print(f"‚úÖ Train dataset: {len(train_dataset)} vzork≈Ø (80%)")
        print(f"‚úÖ Validation dataset: {len(eval_dataset)} vzork≈Ø (20%)")
    else:
        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)
        train_dataset = split_dataset["train"]
        eval_dataset = split_dataset["test"]
        print(f"‚úÖ Train dataset: {len(train_dataset)} vzork≈Ø (90%)")
        print(f"‚úÖ Validation dataset: {len(eval_dataset)} vzork≈Ø (10%)")
    
    # DEBUG: Ulo≈æen√≠ kompletn√≠ch train a validation dat do soubor≈Ø
    print("\nüíæ DEBUG: Ukl√°d√°m kompletn√≠ train a validation data...")
    
    # Vytvo≈ôen√≠ debug adres√°≈ôe
    debug_data_dir = os.path.join(debugger.debug_dir, "complete_datasets")
    os.makedirs(debug_data_dir, exist_ok=True)
    
    # Ulo≈æen√≠ train datasetu
    train_file = os.path.join(debug_data_dir, "complete_train_dataset.json")
    save_dataset_to_file(train_dataset, train_file, f"Kompletn√≠ train dataset ({len(train_dataset)} vzork≈Ø)")
    
    # Ulo≈æen√≠ validation datasetu
    eval_file = os.path.join(debug_data_dir, "complete_validation_dataset.json")
    save_dataset_to_file(eval_dataset, eval_file, f"Kompletn√≠ validation dataset ({len(eval_dataset)} vzork≈Ø)")
    
    print(f"‚úÖ Kompletn√≠ data ulo≈æena v: {debug_data_dir}")
    
    # Interaktivn√≠ kontrola po rozdƒõlen√≠ dat
    if not args.no_interactive:
        if not ask_user_continue("Data jsou rozdƒõlena a ulo≈æena. Pokraƒçovat ve zpracov√°n√≠?"):
            return
    
    # Kontrola kompatibility a nezn√°m√Ωch token≈Ø p≈ôed tr√©nov√°n√≠m
    print(f"\nüîç FIN√ÅLN√ç KONTROLY P≈òED TR√âNOV√ÅN√çM")
    print(f"=" * 50)
    
    # 1. Kontrola kompatibility tokenizeru
    tokenizer_ok = check_tokenizer_compatibility(tokenizer, args.model_name, debugger)
    if not tokenizer_ok:
        print(f"‚ö†Ô∏è VAROV√ÅN√ç: Probl√©my s tokenizerem, ale pokraƒçuji...")
    
    # 2. Kontrola nezn√°m√Ωch token≈Ø v train datasetu
    train_ok = check_unknown_tokens(train_dataset, tokenizer, debugger, max_samples_to_check=50)
    if not train_ok:
        print(f"‚ùå KRITICK√Å CHYBA: P≈ô√≠li≈° mnoho nezn√°m√Ωch token≈Ø v train datasetu!")
        print(f"   Zastavuji fine-tuning. Opravte data p≈ôed pokraƒçov√°n√≠m.")
        return
    
    # 3. Kontrola nezn√°m√Ωch token≈Ø v validation datasetu
    eval_ok = check_unknown_tokens(eval_dataset, tokenizer, debugger, max_samples_to_check=20)
    if not eval_ok:
        print(f"‚ùå KRITICK√Å CHYBA: P≈ô√≠li≈° mnoho nezn√°m√Ωch token≈Ø v validation datasetu!")
        print(f"   Zastavuji fine-tuning. Opravte data p≈ôed pokraƒçov√°n√≠m.")
        return
    
    print(f"‚úÖ V≈°echny kontroly pro≈°ly - pokraƒçuji s tr√©nov√°n√≠m")
    
    # DEBUG: Ulo≈æen√≠ kompletn√≠ho slovn√≠ku token≈Ø
    print("\nüíæ DEBUG: Ukl√°d√°m kompletn√≠ slovn√≠k token≈Ø...")
    
    vocab_file = os.path.join(debug_data_dir, "complete_vocabulary.json")
    save_vocabulary_to_file(tokenizer, vocab_file)
    
    print(f"‚úÖ Kompletn√≠ slovn√≠k ulo≈æen v: {vocab_file}")
    
    # Interaktivn√≠ kontrola po ulo≈æen√≠ slovn√≠ku
    if not args.no_interactive:
        if not ask_user_continue("Slovn√≠k je ulo≈æen. Pokraƒçovat ve zpracov√°n√≠?"):
            return
    
    # 7. Data Collator
    print("\nüîß Konfiguruji data collator...")
    
    # Vlastn√≠ data collator pro spr√°vn√© ≈ôe≈°en√≠ padding
    class CustomDataCollator:
        def __init__(self, tokenizer, max_length=1024):
            self.tokenizer = tokenizer
            self.max_length = max_length
        
        def __call__(self, features):
            # Z√≠sk√°n√≠ maxim√°ln√≠ d√©lky v batch
            max_len = max(len(feature['input_ids']) for feature in features)
            max_len = min(max_len, self.max_length)
            
            # Padding v≈°ech sekvenc√≠ na stejnou d√©lku
            batch = {
                'input_ids': [],
                'attention_mask': [],
                'labels': []
            }
            
            for feature in features:
                input_ids = feature['input_ids'][:max_len]
                attention_mask = feature['attention_mask'][:max_len]
                labels = feature['labels'][:max_len]
                
                # Padding na max_len
                padding_length = max_len - len(input_ids)
                if padding_length > 0:
                    input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length
                    attention_mask = attention_mask + [0] * padding_length
                    labels = labels + [-100] * padding_length  # -100 pro ignorov√°n√≠ p≈ôi loss
                
                batch['input_ids'].append(input_ids)
                batch['attention_mask'].append(attention_mask)
                batch['labels'].append(labels)
            
            # Konverze na tensory
            return {
                'input_ids': torch.tensor(batch['input_ids'], dtype=torch.long),
                'attention_mask': torch.tensor(batch['attention_mask'], dtype=torch.long),
                'labels': torch.tensor(batch['labels'], dtype=torch.long)
            }
    
    data_collator = CustomDataCollator(tokenizer, args.max_length)
    
    # 8. Training Arguments
    print("\n‚öôÔ∏è Nastavuji training arguments...")
    
    # Dynamick√© nastaven√≠ training parametr≈Ø podle velikosti datasetu
    if len(train_dataset) < 10:
        save_steps = max(1, len(train_dataset) // 2)
        eval_steps = max(1, len(train_dataset) // 2)
        logging_steps = 1
    else:
        save_steps = 500
        eval_steps = 500
        logging_steps = 10
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=4,
        warmup_steps=min(100, len(train_dataset) // 2),
        learning_rate=args.learning_rate,
        fp16=True,
        logging_steps=logging_steps,
        save_steps=save_steps,
        eval_steps=eval_steps,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        weight_decay=0.01,
        max_grad_norm=0.3,
        push_to_hub=args.push_to_hub,
        hub_model_id=args.hub_model_id,
        hub_token=HF_TOKEN if args.push_to_hub else None,
        save_total_limit=2,
        logging_dir=f"{args.output_dir}/logs",
        dataloader_num_workers=0,
        dataloader_drop_last=True,
        group_by_length=True,
        report_to=[],  # Vypnout wandb a dal≈°√≠ reporting
    )
    
    # 9. Trainer
    print("\nüèãÔ∏è Vytv√°≈ô√≠m Trainer...")
    
    # Nastaven√≠ label_names pro PeftModel
    try:
        if hasattr(model, 'label_names'):
            model.label_names = ['labels']
        elif hasattr(model, 'config') and hasattr(model.config, 'label_names'):
            model.config.label_names = ['labels']
        
        if hasattr(model, 'base_model') and hasattr(model.base_model, 'config'):
            model.base_model.config.label_names = ['labels']
        
        print("‚úÖ Label names nastaveny pro model")
    except Exception as e:
        print(f"‚ö†Ô∏è Nelze nastavit label_names: {e}")
    
    # Zajist√≠me, ≈æe model je v training m√≥du
    model.train()
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # 10. Fine-tuning
    print("\nüöÄ Spou≈°t√≠m fine-tuning...")
    trainer.train()
    
    # 11. Ulo≈æen√≠ modelu
    final_model_path = save_model_info(args.output_dir, args.output_dir)
    
    trainer.save_model(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    
    if args.push_to_hub and HF_TOKEN:
        print("üì§ Nahr√°v√°m model na Hugging Face Hub...")
        model.push_to_hub(args.hub_model_id, token=HF_TOKEN)
        tokenizer.push_to_hub(args.hub_model_id, token=HF_TOKEN)
        print(f"‚úÖ Model nahr√°n: https://huggingface.co/{args.hub_model_id}")
    
    # 12. Testov√°n√≠
    print("\nüèãÔ∏è Testuji model...")
    test_model(model, tokenizer)
    
    # 13. Ukonƒçen√≠
    print("\nüéâ Fine-tuning dokonƒçen!")
    print(f"üìÅ Model ulo≈æen v: {final_model_path}")
    print(f"üíæ Network storage: {args.output_dir}")
    if args.push_to_hub:
        print(f"üåê Model dostupn√Ω na: https://huggingface.co/{args.hub_model_id}")

if __name__ == "__main__":
    main() 