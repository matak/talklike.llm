# Fine-tuning Llama 3 8B pro Andreje BabiÅ¡e

Tento projekt obsahuje kompletnÃ­ pipeline pro fine-tuning Llama 3 8B modelu s daty Andreje BabiÅ¡e na RunPod.io nebo lokÃ¡lnÄ›.

## ğŸ“‹ PÅ™ehled

- **Base Model**: Meta-Llama-3-8B-Instruct
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Data Format**: JSONL s chat formÃ¡tem
- **Target**: Model mluvÃ­cÃ­ stylem Andreje BabiÅ¡e

## ğŸš€ RychlÃ½ start

### 1. PÅ™Ã­prava prostÅ™edÃ­

```bash
# KlonovÃ¡nÃ­ repozitÃ¡Å™e
git clone <your-repo-url>
cd talklike.llm

# Instalace zÃ¡vislostÃ­
pip install -r requirements.txt
```

### 2. NastavenÃ­ tokenÅ¯

VytvoÅ™te soubor `.env` v koÅ™enovÃ©m adresÃ¡Å™i:

```bash
# Hugging Face token (povinnÃ©)
HF_TOKEN=hf_your_token_here

# Weights & Biases token (volitelnÃ©)
WANDB_API_KEY=your_wandb_token_here
```

### 3. SpuÅ¡tÄ›nÃ­ fine-tuningu

```bash
# RychlÃ½ start (doporuÄeno)
chmod +x quick_start.sh
./quick_start.sh

# Nebo manuÃ¡lnÄ›
python finetune_babis_llama.py --use_wandb --push_to_hub
```

## ğŸ“Š Struktura dat

VaÅ¡e data v `../data/all.jsonl` majÃ­ formÃ¡t:

```json
{
    "messages": [
        {
            "role": "system",
            "content": "Jsi Andrej BabiÅ¡, ÄeskÃ½ politik a podnikatel..."
        },
        {
            "role": "user",
            "content": "Pane BabiÅ¡i, mÅ¯Å¾ete vysvÄ›tlit vaÅ¡i roli v tÃ© chemiÄce?"
        },
        {
            "role": "assistant",
            "content": "Hele, ta tovÃ¡rna? To uÅ¾ jsem dÃ¡vno pÅ™edal..."
        }
    ]
}
```

## ğŸ—ï¸ Architektura

### LoRA Konfigurace
- **Rank (r)**: 16
- **Alpha**: 32
- **Dropout**: 0.1
- **Target Modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

### Training Parametry
- **Epochs**: 3
- **Batch Size**: 2 (per device)
- **Gradient Accumulation**: 4
- **Learning Rate**: 2e-4
- **Warmup Steps**: 100
- **Max Length**: 2048 tokens

## ğŸ’» SpuÅ¡tÄ›nÃ­ na RunPod.io

### 1. VytvoÅ™enÃ­ podu

1. JdÄ›te na [runpod.io](https://runpod.io)
2. VytvoÅ™te novÃ½ pod s:
   - **GPU**: RTX 4090 nebo A100
   - **RAM**: MinimÃ¡lnÄ› 24GB
   - **Storage**: MinimÃ¡lnÄ› 50GB
   - **Template**: PyTorch nebo Jupyter

### 2. PÅ™Ã­prava prostÅ™edÃ­

```bash
# Aktualizace systÃ©mu
sudo apt update && sudo apt upgrade -y

# Instalace balÃ­ÄkÅ¯
sudo apt install -y git wget curl

# KlonovÃ¡nÃ­ repozitÃ¡Å™e
git clone <your-repo-url>
cd <your-project>

# VytvoÅ™enÃ­ .env souboru
nano .env
# PÅ™idejte vaÅ¡e tokeny

# SpuÅ¡tÄ›nÃ­ fine-tuningu
./quick_start.sh
```

### 3. MonitorovÃ¡nÃ­

- **W&B Dashboard**: Sledujte metriky na wandb.ai
- **Jupyter**: Sledujte progress v notebooku
- **Terminal**: Logy v terminÃ¡lu

## ğŸ“ Soubory projektu

```
talklike.llm/
â”œâ”€â”€ 2_finetunning/                 # Fine-tuning scripts and configs
â”‚   â”œâ”€â”€ finetune_babis.py          # Main fine-tuning script
â”‚   â”œâ”€â”€ test_tokenization.py       # Tokenization testing
â”‚   â”œâ”€â”€ run_finetune.sh            # Fine-tuning shell script
â”‚   â”œâ”€â”€ run_mistral_finetune.sh    # Mistral fine-tuning script
â”‚   â”œâ”€â”€ requirements_finetunning.txt # Python dependencies
â”‚   â”œâ”€â”€ README_FINETUNE.md         # This file
â”‚   â””â”€â”€ RUNPOD_SETUP.md           # RunPod.io instructions
â”œâ”€â”€ data/
â”‚   â””â”€â”€ all.jsonl                 # Training data
â””â”€â”€ [other directories...]
```

## âš™ï¸ Konfigurace

### ZÃ¡kladnÃ­ parametry

```bash
python finetune_babis_llama.py \
    --data_path ../data/all.jsonl \
    --output_dir ./babis-llama-finetuned \
    --epochs 3 \
    --batch_size 2 \
    --learning_rate 2e-4 \
    --max_length 2048
```

### PokroÄilÃ© parametry

```bash
python finetune_babis_llama.py \
    --data_path ../data/all.jsonl \
    --output_dir ./babis-llama-finetuned \
    --model_name meta-llama/Meta-Llama-3-8B-Instruct \
    --epochs 5 \
    --batch_size 1 \
    --learning_rate 1e-4 \
    --max_length 1024 \
    --use_wandb \
    --push_to_hub \
    --hub_model_id your-username/babis-llama-3-8b-lora
```

## ğŸ”§ Troubleshooting

### Out of Memory (OOM)
```bash
# SniÅ¾te batch size a max_length
python finetune_babis_llama.py --batch_size 1 --max_length 1024
```

### PomalÃ© trÃ©novÃ¡nÃ­
```bash
# Zkontrolujte GPU vyuÅ¾itÃ­
nvidia-smi

# SniÅ¾te gradient accumulation
# Upravte v kÃ³du: gradient_accumulation_steps=2
```

### Model nekonverguje
```bash
# SniÅ¾te learning rate
python finetune_babis_llama.py --learning_rate 1e-4

# ZvyÅ¡te poÄet epoch
python finetune_babis_llama.py --epochs 5
```

## ğŸ“ˆ OÄekÃ¡vanÃ© vÃ½sledky

Po fine-tuningu by model mÄ›l:

- âœ… Mluvit stylem Andreje BabiÅ¡e
- âœ… PouÅ¾Ã­vat jeho charakteristickÃ© frÃ¡ze
- âœ… OdpovÃ­dat v prvnÃ­ osobÄ›
- âœ… PÅ™idÃ¡vat podpis "Andrej BabiÅ¡."

### TestovacÃ­ prompty

```python
test_prompts = [
    "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?",
    "Co si myslÃ­te o opozici?",
    "Jak se vÃ¡m daÅ™Ã­ v Bruselu?",
    "MÅ¯Å¾ete vysvÄ›tlit vaÅ¡i roli v tÃ© chemiÄce?",
    "Jak hodnotÃ­te efektivizaci stÃ¡tnÃ­ sprÃ¡vy?"
]
```

## ğŸ’¾ UloÅ¾enÃ­ a sdÃ­lenÃ­

### LokÃ¡lnÃ­ uloÅ¾enÃ­
```bash
# Model se uloÅ¾Ã­ do ./babis-llama-finetuned-final/
```

### Hugging Face Hub
```bash
# Model se automaticky nahraje na HF Hub
# Repo: https://huggingface.co/your-username/babis-llama-3-8b-lora
```

## ğŸ§ª PouÅ¾itÃ­ fine-tuned modelu

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# NaÄtenÃ­ base modelu
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

# NaÄtenÃ­ LoRA adaptÃ©rÅ¯
model = PeftModel.from_pretrained(base_model, "your-username/babis-llama-3-8b-lora")

# GenerovÃ¡nÃ­
prompt = "Pane BabiÅ¡i, jak hodnotÃ­te inflaci?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## ğŸ’° OdhadovanÃ© nÃ¡klady

- **RTX 4090**: ~$0.60/hod
- **A100**: ~$1.20/hod
- **OÄekÃ¡vanÃ¡ doba trÃ©novÃ¡nÃ­**: 2-4 hodiny
- **CelkovÃ© nÃ¡klady**: $1.20 - $4.80

## ğŸ“ PoznÃ¡mky

1. **Data kvalita**: UjistÄ›te se, Å¾e vaÅ¡e data jsou kvalitnÃ­ a konzistentnÃ­
2. **Monitoring**: Sledujte loss bÄ›hem trÃ©novÃ¡nÃ­
3. **Backup**: PravidelnÄ› uklÃ¡dejte checkpointy
4. **TestovÃ¡nÃ­**: Testujte model bÄ›hem trÃ©novÃ¡nÃ­

## ğŸ¤ Kontakt

Pro problÃ©my nebo otÃ¡zky:
- GitHub Issues
- RunPod Discord
- Hugging Face Forums

## ğŸ“„ Licence

Tento projekt je urÄen pouze pro vzdÄ›lÃ¡vacÃ­ ÃºÄely. Respektujte autorskÃ¡ prÃ¡va a licenÄnÃ­ podmÃ­nky pouÅ¾itÃ½ch modelÅ¯ a dat. 