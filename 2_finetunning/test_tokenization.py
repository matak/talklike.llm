#!/usr/bin/env python3
"""
Test tokenizace pro fine-tuning data
"""

# Import setup_environment pro sprÃ¡vnÃ© nastavenÃ­ prostÅ™edÃ­
import setup_environment

import os
import json
import torch
from transformers import AutoTokenizer
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling
)

def setup_tokenizer_and_model(model_name, base_model):
    """NastavÃ­ tokenizer a model pro fine-tuning"""
    
    # 1. NaÄtenÃ­ tokenizeru
    base_tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(f"ğŸ“Š PÅ¯vodnÃ­ dÃ©lka tokenizeru: {len(base_tokenizer)}")
    
    # 2. Kontrola a pÅ™idÃ¡nÃ­ pad tokenu
    if base_tokenizer.pad_token is None:
        # ZkusÃ­me pouÅ¾Ã­t existujÃ­cÃ­ tokeny
        if base_tokenizer.eos_token:
            base_tokenizer.pad_token = base_tokenizer.eos_token
            print(f"âœ… PouÅ¾Ã­vÃ¡m EOS token jako PAD: {base_tokenizer.pad_token}")
        else:
            # PÅ™idÃ¡me novÃ½ pad token
            base_tokenizer.add_special_tokens({"pad_token": "<pad>"})
            print(f"âœ… PÅ™idÃ¡n novÃ½ pad token: {base_tokenizer.pad_token}")
            
            # DÅ¯leÅ¾itÃ©: Resize model embeddings
            base_model.resize_token_embeddings(len(base_tokenizer))
            print(f"ğŸ“Š Model embeddings resized na: {len(base_tokenizer)}")
    else:
        print(f"â„¹ï¸ Pad token uÅ¾ existuje: {base_tokenizer.pad_token}")
    
    # 3. Synchronizace s modelem
    if hasattr(base_model.config, 'pad_token_id'):
        old_pad_id = base_model.config.pad_token_id
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
        print(f"ğŸ”„ Pad token ID zmÄ›nÄ›n: {old_pad_id} â†’ {base_model.config.pad_token_id}")
    else:
        print("âš ï¸ Model nemÃ¡ pad_token_id v config")
    
    # 4. Kontrola konzistence
    try:
        assert base_tokenizer.pad_token_id == base_model.config.pad_token_id, \
            "Tokenizer a model majÃ­ rÅ¯znÃ© pad token ID!"
        print(f"âœ… Tokenizer a model synchronizovÃ¡ny")
    except AssertionError as e:
        print(f"âŒ Chyba synchronizace: {e}")
        # PokusÃ­me se opravit
        base_model.config.pad_token_id = base_tokenizer.pad_token_id
        print(f"ğŸ”§ Opraveno: pad_token_id nastaven na {base_tokenizer.pad_token_id}")
    
    return base_tokenizer, base_model

def test_tokenization():
    """Testuje tokenizaci a data collator"""
    
    # NaÄtenÃ­ modelu a tokenizeru
    model_name = "microsoft/DialoGPT-medium"  # PouÅ¾ijeme menÅ¡Ã­ model pro test
    
    print(f"ğŸ¤– NaÄÃ­tÃ¡m model: {model_name}")
    
    # NaÄtenÃ­ modelu (bez quantization pro test)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # PouÅ¾itÃ­ vylepÅ¡enÃ© metody pro nastavenÃ­ tokenizeru
    print("\nğŸ”§ Nastavuji tokenizer a model...")
    tokenizer, model = setup_tokenizer_and_model(model_name, model)
    
    # TestovacÃ­ data
    test_data = [
        {
            "text": "<|system|>\nJste Andrej BabiÅ¡, pÅ™edseda hnutÃ­ ANO a bÃ½valÃ½ premiÃ©r ÄŒeskÃ© republiky.<|end|>\n<|user|>\nJak hodnotÃ­te souÄasnou inflaci?<|end|>\n<|assistant|>\nInflace je vÃ¡Å¾nÃ½ problÃ©m, kterÃ½ postihuje vÅ¡echny obÄany.<|end|>\n"
        },
        {
            "text": "<|system|>\nJste Andrej BabiÅ¡, pÅ™edseda hnutÃ­ ANO a bÃ½valÃ½ premiÃ©r ÄŒeskÃ© republiky.<|end|>\n<|user|>\nCo si myslÃ­te o opozici?<|end|>\n<|assistant|>\nOpozice kritizuje, ale nemÃ¡ Å™eÅ¡enÃ­.<|end|>\n"
        }
    ]
    
    print("\nğŸ“Š TestovacÃ­ data:")
    for i, sample in enumerate(test_data):
        print(f"  Vzorek {i+1}: {len(sample['text'])} znakÅ¯")
    
    # VytvoÅ™enÃ­ datasetu
    dataset = Dataset.from_list(test_data)
    
    # Tokenizace
    def tokenize_function(examples, tokenizer, max_length=512):
        """Tokenizuje text pro fine-tuning"""
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors=None
        )
        
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized
    
    print("\nğŸ”¤ Tokenizuji data...")
    tokenize_func = lambda examples: tokenize_function(examples, tokenizer, 512)
    tokenized_dataset = dataset.map(
        tokenize_func,
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=100
    )
    
    # Oprava padding
    print("ğŸ”§ Opravuji padding...")
    def fix_padding(example):
        max_len = 512
        current_len = len(example['input_ids'])
        
        if current_len < max_len:
            padding_length = max_len - current_len
            example['input_ids'] = example['input_ids'] + [tokenizer.pad_token_id] * padding_length
            example['attention_mask'] = example['attention_mask'] + [0] * padding_length
            example['labels'] = example['labels'] + [-100] * padding_length
        elif current_len > max_len:
            example['input_ids'] = example['input_ids'][:max_len]
            example['attention_mask'] = example['attention_mask'][:max_len]
            example['labels'] = example['labels'][:max_len]
        
        return example
    
    tokenized_dataset = tokenized_dataset.map(fix_padding, desc="Opravuji padding")
    
    # Kontrola dÃ©lky
    print("\nğŸ“ Kontrola dÃ©lky tokenÅ¯:")
    for i, sample in enumerate(tokenized_dataset):
        print(f"  Vzorek {i+1}: {len(sample['input_ids'])} tokenÅ¯")
    
    # Test data collator
    print("\nğŸ”§ Testuji data collator...")
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        return_tensors="pt",
        pad_to_multiple_of=8,
    )
    
    try:
        test_batch = data_collator([tokenized_dataset[0], tokenized_dataset[1]])
        print("âœ… Data collator test ÃºspÄ›Å¡nÃ½!")
        print(f"ğŸ“Š Batch shape: {test_batch['input_ids'].shape}")
        print(f"ğŸ“Š Labels shape: {test_batch['labels'].shape}")
        
        # DekÃ³dovÃ¡nÃ­ pro kontrolu
        print("\nğŸ“ DekÃ³dovanÃ½ text:")
        decoded = tokenizer.decode(test_batch['input_ids'][0], skip_special_tokens=False)
        print(f"  Vzorek 1: {decoded[:200]}...")
        
        # DodateÄnÃ© informace o tokenizeru
        print(f"\nğŸ“‹ Tokenizer informace:")
        print(f"  Pad token: {tokenizer.pad_token}")
        print(f"  Pad token ID: {tokenizer.pad_token_id}")
        print(f"  EOS token: {tokenizer.eos_token}")
        print(f"  Vocab size: {len(tokenizer)}")
        print(f"  Model vocab size: {model.config.vocab_size}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Data collator test selhal: {e}")
        return False

if __name__ == "__main__":
    success = test_tokenization()
    if success:
        print("\nğŸ‰ Test ÃºspÄ›Å¡nÃ½! MÅ¯Å¾ete spustit fine-tuning.")
    else:
        print("\nâŒ Test selhal! Zkontrolujte konfiguraci.") 