# Funkce pro generov치n칤 odpov캩d칤
def generate_babis_response(prompt, max_length=100, temperature=0.7):
    """Vygeneruje odpov캩캞 ve stylu Babi코e"""
    
    # Tokenizace promptu
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=config.max_seq_length)
    
    # Generov치n칤
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Dek칩dov치n칤
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Odstran캩n칤 p콢vodn칤ho promptu
    response = response.replace(prompt, "").strip()
    
    return response

# Testovac칤 prompty
test_prompts = [
    "U쬴vatel: Jak칳 je v치코 n치zor na inflaci?\nAndrej Babi코: ",
    "U쬴vatel: Co si mysl칤te o Bruselu?\nAndrej Babi코: ",
    "U쬴vatel: Jak hodnot칤te opozici?\nAndrej Babi코: ",
    "U쬴vatel: Jak칠 m치te pl치ny?\nAndrej Babi코: "
]

print("游빍 Testov치n칤 fine-tuned modelu:")
print("=" * 50)

for prompt in test_prompts:
    response = generate_babis_response(prompt)
    print(f"\nPrompt: {prompt.strip()}")
    print(f"Odpov캩캞: {response}")
    print("-" * 30) 