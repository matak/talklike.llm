#!/usr/bin/env python3
"""
Fine-tuning script pro model s daty Andreje BabiÅ¡e
SpustitelnÃ½ na RunPod.io nebo lokÃ¡lnÄ›
"""

import os
import json
import torch
import numpy as np
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
from dotenv import load_dotenv
from huggingface_hub import login
import wandb
import argparse

def load_babis_data(file_path):
    """NaÄte data z JSONL souboru nebo jednoho velkÃ©ho JSON objektu"""
    conversations = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read().strip()
    
    try:
        # ZkusÃ­me parsovat jako jeden velkÃ½ JSON objekt
        data = json.loads(content)
        
        if 'messages' in data:
            # MÃ¡me jeden velkÃ½ objekt s messages - rozdÄ›lÃ­me na konverzace
            messages = data['messages']
            print(f"ğŸ“Š NaÄteno {len(messages)} zprÃ¡v v jednom objektu")
            
            # RozdÄ›lenÃ­ na konverzace (kaÅ¾dÃ½ch 3 zprÃ¡vy = 1 konverzace)
            i = 0
            while i < len(messages):
                # Najdeme system zprÃ¡vu
                if i < len(messages) and messages[i]['role'] == 'system':
                    system_msg = messages[i]
                    i += 1
                    
                    # Najdeme user a assistant zprÃ¡vy
                    conv_messages = [system_msg]
                    while i < len(messages) and messages[i]['role'] in ['user', 'assistant']:
                        conv_messages.append(messages[i])
                        i += 1
                    
                    # VytvoÅ™Ã­me konverzaci
                    if len(conv_messages) >= 3:  # system + user + assistant
                        conversations.append({
                            "messages": conv_messages
                        })
                else:
                    i += 1
            
            print(f"âœ… VytvoÅ™eno {len(conversations)} konverzacÃ­")
            return conversations
            
    except json.JSONDecodeError:
        # NenÃ­ jeden velkÃ½ JSON objekt, zkusÃ­me JSONL formÃ¡t
        print("ğŸ“Š ZkouÅ¡Ã­m JSONL formÃ¡t...")
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        conversations.append(data)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ Chyba pÅ™i parsovÃ¡nÃ­ Å™Ã¡dku: {e}")
                        continue
        
        print(f"âœ… NaÄteno {len(conversations)} konverzacÃ­ z JSONL")
        return conversations
    
    return conversations

def prepare_training_data(conversations):
    """PÅ™ipravÃ­ data pro fine-tuning"""
    training_data = []
    
    for conv in conversations:
        messages = conv['messages']
        
        # PÅ™eskoÄÃ­me konverzace bez assistant zprÃ¡v
        if not any(msg['role'] == 'assistant' for msg in messages):
            continue
            
        # VytvoÅ™Ã­me text pro fine-tuning
        text = ""
        for msg in messages:
            if msg['role'] == 'system':
                text += f"<|system|>\n{msg['content']}<|end|>\n"
            elif msg['role'] == 'user':
                text += f"<|user|>\n{msg['content']}<|end|>\n"
            elif msg['role'] == 'assistant':
                text += f"<|assistant|>\n{msg['content']}<|end|>\n"
        
        training_data.append({"text": text})
    
    return training_data

def tokenize_function(examples, tokenizer, max_length=2048):
    """Tokenizuje text pro fine-tuning"""
    # Tokenizace
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding=False,
        max_length=max_length,
        return_tensors=None
    )
    
    # NastavenÃ­ labels stejnÃ© jako input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

def main():
    parser = argparse.ArgumentParser(description='Fine-tuning 3 8B pro Andreje BabiÅ¡e')
    parser.add_argument('--data_path', type=str, default='data/all.jsonl', help='Cesta k datÅ¯m')
    parser.add_argument('--output_dir', type=str, default='./babis-finetuned', help='VÃ½stupnÃ­ adresÃ¡Å™')
    parser.add_argument('--model_name', type=str, default='mistralai/Mistral-7B-Instruct-v0.3', help='NÃ¡zev base modelu')
    parser.add_argument('--epochs', type=int, default=3, help='PoÄet epoch')
    parser.add_argument('--batch_size', type=int, default=2, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=2e-4, help='Learning rate')
    parser.add_argument('--max_length', type=int, default=2048, help='MaximÃ¡lnÃ­ dÃ©lka sekvence')
    parser.add_argument('--use_wandb', action='store_true', help='PouÅ¾Ã­t Weights & Biases')
    parser.add_argument('--push_to_hub', action='store_true', help='NahrÃ¡t model na HF Hub')
    parser.add_argument('--hub_model_id', type=str, default='babis-lora', help='NÃ¡zev modelu na HF Hub')
    
    args = parser.parse_args()
    
    print("ğŸš€ SpouÅ¡tÃ­m fine-tuning pro Andreje BabiÅ¡e")
    print(f"ğŸ“ Data: {args.data_path}")
    print(f"ğŸ“ VÃ½stup: {args.output_dir}")
    print(f"ğŸ¤– Model: {args.model_name}")
    
    # NaÄtenÃ­ promÄ›nnÃ½ch prostÅ™edÃ­
    load_dotenv()
    
    # Hugging Face token
    HF_TOKEN = os.getenv("HF_TOKEN")
    if HF_TOKEN:
        login(token=HF_TOKEN)
        print("âœ… Hugging Face login ÃºspÄ›Å¡nÃ½")
    else:
        print("âš ï¸ HF_TOKEN nebyl nalezen")
    
    # Weights & Biases
    if args.use_wandb:
        WANDB_API_KEY = os.getenv("WANDB_API_KEY")
        if WANDB_API_KEY:
            os.environ["WANDB_API_KEY"] = WANDB_API_KEY
            wandb.login()
            wandb.init(project="babis-finetune", name=args.model_name)
            print("âœ… W&B login ÃºspÄ›Å¡nÃ½")
        else:
            print("âš ï¸ WANDB_API_KEY nebyl nalezen")
    
    # 1. NaÄtenÃ­ dat
    print("\nğŸ“Š NaÄÃ­tÃ¡m data...")
    conversations = load_babis_data(args.data_path)
    print(f"âœ… NaÄteno {len(conversations)} konverzacÃ­")
    
    # 2. PÅ™Ã­prava dat
    print("ğŸ”§ PÅ™ipravuji data...")
    training_data = prepare_training_data(conversations)
    print(f"âœ… PÅ™ipraveno {len(training_data)} trÃ©novacÃ­ch vzorkÅ¯")
    
    # 3. VytvoÅ™enÃ­ Dataset
    dataset = Dataset.from_list(training_data)
    
    # 4. NaÄtenÃ­ modelu
    print(f"\nğŸ¤– NaÄÃ­tÃ¡m model: {args.model_name}")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    
    # PÅ™idÃ¡nÃ­ pad tokenu
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.pad_token_id
    
    print(f"âœ… Model naÄten. Vocab size: {model.config.vocab_size}")
    
    # 5. Konfigurace LoRA
    print("\nğŸ”§ Konfiguruji LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=[
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"
        ]
    )
    
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 6. Tokenizace dat
    print("\nğŸ”¤ Tokenizuji data...")
    tokenize_func = lambda examples: tokenize_function(examples, tokenizer, args.max_length)
    tokenized_dataset = dataset.map(
        tokenize_func,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    # RozdÄ›lenÃ­ na train/validation
    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = split_dataset["train"]
    eval_dataset = split_dataset["test"]
    
    print(f"âœ… Train dataset: {len(train_dataset)} vzorkÅ¯")
    print(f"âœ… Validation dataset: {len(eval_dataset)} vzorkÅ¯")
    
    # 7. Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # 8. Training Arguments
    print("\nâš™ï¸ Nastavuji training arguments...")
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        learning_rate=args.learning_rate,
        fp16=True,
        logging_steps=10,
        save_steps=500,
        eval_steps=500,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="wandb" if args.use_wandb else "none",
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        weight_decay=0.01,
        max_grad_norm=0.3,
        push_to_hub=args.push_to_hub,
        hub_model_id=args.hub_model_id,
        hub_token=HF_TOKEN if args.push_to_hub else None,
    )
    
    # 9. Trainer
    print("\nğŸ‹ï¸ VytvÃ¡Å™Ã­m Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # 10. Fine-tuning
    print("\nğŸš€ SpouÅ¡tÃ­m fine-tuning...")
    trainer.train()
    
    # 11. UloÅ¾enÃ­ modelu
    print("\nğŸ’¾ UklÃ¡dÃ¡m model...")
    trainer.save_model(f"{args.output_dir}-final")
    tokenizer.save_pretrained(f"{args.output_dir}-final")
    
    if args.push_to_hub and HF_TOKEN:
        print("ğŸ“¤ NahrÃ¡vÃ¡m model na Hugging Face Hub...")
        model.push_to_hub(args.hub_model_id, token=HF_TOKEN)
        tokenizer.push_to_hub(args.hub_model_id, token=HF_TOKEN)
        print(f"âœ… Model nahrÃ¡n: https://huggingface.co/{args.hub_model_id}")
    
    # 12. TestovÃ¡nÃ­
    print("\nğŸ§ª Testuji model...")
    def generate_response(prompt, max_length=200):
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
    
    test_prompts = [
        "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?",
        "Co si myslÃ­te o opozici?",
        "Jak se vÃ¡m daÅ™Ã­ v Bruselu?"
    ]
    
    print("\nğŸ“ TestovacÃ­ odpovÄ›di:")
    print("=" * 50)
    for prompt in test_prompts:
        print(f"\nPrompt: {prompt}")
        response = generate_response(prompt)
        print(f"OdpovÄ›Ä: {response}")
        print("-" * 30)
    
    # 13. UkonÄenÃ­
    if args.use_wandb:
        wandb.finish()
    
    print("\nğŸ‰ Fine-tuning dokonÄen!")
    print(f"ğŸ“ Model uloÅ¾en v: {args.output_dir}-final")
    if args.push_to_hub:
        print(f"ğŸŒ Model dostupnÃ½ na: https://huggingface.co/{args.hub_model_id}")

if __name__ == "__main__":
    main() 