#!/usr/bin/env python3
"""
Fine-tuning script pro model s daty Andreje BabiÅ¡e
SpustitelnÃ½ na RunPod.io nebo lokÃ¡lnÄ›
"""

import os
import json
import torch
import numpy as np
import shutil
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
from dotenv import load_dotenv
from huggingface_hub import login
import wandb
import argparse

# Import disk manager knihovny
from disk_manager import DiskManager, setup_for_ml_project, check_and_cleanup

def load_babis_data(file_path):
    """NaÄte data z JSONL souboru nebo jednoho velkÃ©ho JSON objektu"""
    conversations = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read().strip()
    
    try:
        # ZkusÃ­me parsovat jako jeden velkÃ½ JSON objekt
        data = json.loads(content)
        
        if 'messages' in data:
            # MÃ¡me jeden velkÃ½ objekt s messages - rozdÄ›lÃ­me na konverzace
            messages = data['messages']
            print(f"ğŸ“Š NaÄteno {len(messages)} zprÃ¡v v jednom objektu")
            
            # Najdeme system zprÃ¡vu (mÄ›la by bÃ½t prvnÃ­)
            system_msg = None
            for msg in messages:
                if msg['role'] == 'system':
                    system_msg = msg
                    break
            
            if not system_msg:
                print("âŒ Nenalezena system zprÃ¡va!")
                return conversations
            
            # Projdeme vÅ¡echny zprÃ¡vy a najdeme user-assistant pÃ¡ry
            i = 0
            while i < len(messages):
                # HledÃ¡me user zprÃ¡vu
                if i < len(messages) and messages[i]['role'] == 'user':
                    user_msg = messages[i]
                    i += 1
                    
                    # HledÃ¡me nÃ¡sledujÃ­cÃ­ assistant zprÃ¡vu
                    if i < len(messages) and messages[i]['role'] == 'assistant':
                        assistant_msg = messages[i]
                        i += 1
                        
                        # VytvoÅ™Ã­me konverzaci s system + user + assistant
                        conv_messages = [system_msg, user_msg, assistant_msg]
                        conversations.append({
                            "messages": conv_messages
                        })
                    else:
                        # ChybÃ­ assistant zprÃ¡va, pÅ™eskoÄÃ­me user zprÃ¡vu
                        i += 1
                else:
                    # NenÃ­ user zprÃ¡va, pÅ™eskoÄÃ­me
                    i += 1
            
            print(f"âœ… VytvoÅ™eno {len(conversations)} konverzacÃ­")
            
            # Debug informace
            if len(conversations) > 0:
                print(f"ğŸ“ UkÃ¡zka prvnÃ­ konverzace:")
                first_conv = conversations[0]
                for msg in first_conv['messages']:
                    print(f"  {msg['role']}: {msg['content'][:100]}...")
                
                if len(conversations) > 1:
                    print(f"ğŸ“ UkÃ¡zka druhÃ© konverzace:")
                    second_conv = conversations[1]
                    for msg in second_conv['messages']:
                        print(f"  {msg['role']}: {msg['content'][:100]}...")
            
            return conversations
            
    except json.JSONDecodeError:
        # NenÃ­ jeden velkÃ½ JSON objekt, zkusÃ­me JSONL formÃ¡t
        print("ğŸ“Š ZkouÅ¡Ã­m JSONL formÃ¡t...")
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        conversations.append(data)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ Chyba pÅ™i parsovÃ¡nÃ­ Å™Ã¡dku: {e}")
                        continue
        
        print(f"âœ… NaÄteno {len(conversations)} konverzacÃ­ z JSONL")
        return conversations
    
    return conversations

def prepare_training_data(conversations):
    """PÅ™ipravÃ­ data pro fine-tuning"""
    training_data = []
    
    for conv in conversations:
        messages = conv['messages']
        
        # PÅ™eskoÄÃ­me konverzace bez assistant zprÃ¡v
        if not any(msg['role'] == 'assistant' for msg in messages):
            continue
            
        # VytvoÅ™Ã­me text pro fine-tuning
        text = ""
        for msg in messages:
            if msg['role'] == 'system':
                text += f"<|system|>\n{msg['content']}<|end|>\n"
            elif msg['role'] == 'user':
                text += f"<|user|>\n{msg['content']}<|end|>\n"
            elif msg['role'] == 'assistant':
                text += f"<|assistant|>\n{msg['content']}<|end|>\n"
        
        training_data.append({"text": text})
    
    return training_data

def tokenize_function(examples, tokenizer, max_length=2048):
    """Tokenizuje text pro fine-tuning"""
    # Tokenizace s padding pro konzistentnÃ­ dÃ©lky
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding=True,  # PovolÃ­me padding
        max_length=max_length,
        return_tensors=None
    )
    
    # NastavenÃ­ labels stejnÃ© jako input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

def main():
    parser = argparse.ArgumentParser(description='Fine-tuning 3 8B pro Andreje BabiÅ¡e')
    parser.add_argument('--data_path', type=str, default='data/all.jsonl', help='Cesta k datÅ¯m')
    parser.add_argument('--output_dir', type=str, default='/workspace/babis-finetuned', help='VÃ½stupnÃ­ adresÃ¡Å™')
    parser.add_argument('--model_name', type=str, default='microsoft/DialoGPT-medium', help='NÃ¡zev base modelu')
    parser.add_argument('--epochs', type=int, default=3, help='PoÄet epoch')
    parser.add_argument('--batch_size', type=int, default=2, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=2e-4, help='Learning rate')
    parser.add_argument('--max_length', type=int, default=1024, help='MaximÃ¡lnÃ­ dÃ©lka sekvence')
    parser.add_argument('--use_wandb', action='store_true', help='PouÅ¾Ã­t Weights & Biases')
    parser.add_argument('--push_to_hub', action='store_true', help='NahrÃ¡t model na HF Hub')
    parser.add_argument('--hub_model_id', type=str, default='babis-lora', help='NÃ¡zev modelu na HF Hub')
    parser.add_argument('--cleanup_cache', action='store_true', help='VyÄistit cache pÅ™ed spuÅ¡tÄ›nÃ­m')
    parser.add_argument('--aggressive_cleanup', action='store_true', help='AgresivnÃ­ vyÄiÅ¡tÄ›nÃ­ pro velkÃ© modely')
    
    args = parser.parse_args()
    
    # ZajistÃ­me, Å¾e vÃ½stupnÃ­ adresÃ¡Å™ je na network storage
    if not args.output_dir.startswith('/workspace'):
        args.output_dir = f'/workspace/{args.output_dir.lstrip("./")}'
    
    print("ğŸš€ SpouÅ¡tÃ­m fine-tuning pro Andreje BabiÅ¡e")
    print(f"ğŸ“ Data: {args.data_path}")
    print(f"ğŸ“ VÃ½stup: {args.output_dir}")
    print(f"ğŸ¤– Model: {args.model_name}")
    
    # Inicializace disk manageru a nastavenÃ­ pro ML projekt
    dm = setup_for_ml_project("/workspace")
    
    # Kontrola mÃ­sta a vyÄiÅ¡tÄ›nÃ­ pokud je potÅ™eba
    if not check_and_cleanup(threshold=95):
        print("âŒ StÃ¡le nenÃ­ dost mÃ­sta. PouÅ¾ijte menÅ¡Ã­ model nebo vyÄistÄ›te disk.")
        return
    
    # VyÄiÅ¡tÄ›nÃ­ cache pokud poÅ¾adovÃ¡no
    if args.cleanup_cache:
        dm.cleanup_cache()
    
    # Optimalizace pro velkÃ© modely
    if args.aggressive_cleanup or "mistral" in args.model_name.lower() or "llama" in args.model_name.lower():
        print("ğŸ§¹ Optimalizace pro velkÃ½ model...")
        if not dm.optimize_for_large_models(args.model_name):
            print("âŒ Nedost mÃ­sta pro velkÃ½ model. Zkuste menÅ¡Ã­ model.")
            return
    
    # NaÄtenÃ­ promÄ›nnÃ½ch prostÅ™edÃ­
    load_dotenv()
    
    # Hugging Face token
    HF_TOKEN = os.getenv("HF_TOKEN")
    if HF_TOKEN:
        login(token=HF_TOKEN)
        print("âœ… Hugging Face login ÃºspÄ›Å¡nÃ½")
    else:
        print("âš ï¸ HF_TOKEN nebyl nalezen")
    
    # Weights & Biases
    if args.use_wandb:
        WANDB_API_KEY = os.getenv("WANDB_API_KEY")
        if WANDB_API_KEY:
            os.environ["WANDB_API_KEY"] = WANDB_API_KEY
            wandb.login()
            wandb.init(project="babis-finetune", name=args.model_name)
            print("âœ… W&B login ÃºspÄ›Å¡nÃ½")
        else:
            print("âš ï¸ WANDB_API_KEY nebyl nalezen")
    
    # 1. NaÄtenÃ­ dat
    print("\nğŸ“Š NaÄÃ­tÃ¡m data...")
    conversations = load_babis_data(args.data_path)
    print(f"âœ… NaÄteno {len(conversations)} konverzacÃ­")
    
    # 2. PÅ™Ã­prava dat
    print("ğŸ”§ PÅ™ipravuji data...")
    training_data = prepare_training_data(conversations)
    print(f"âœ… PÅ™ipraveno {len(training_data)} trÃ©novacÃ­ch vzorkÅ¯")
    
    # 3. VytvoÅ™enÃ­ Dataset
    dataset = Dataset.from_list(training_data)
    
    # Debug: Kontrola struktury dat
    print(f"\nğŸ” DEBUG: Kontrola struktury dat")
    print(f"ğŸ“Š CelkovÃ½ poÄet vzorkÅ¯: {len(dataset)}")
    
    if len(dataset) > 0:
        print(f"ğŸ“ UkÃ¡zka prvnÃ­ho vzorku:")
        first_sample = dataset[0]
        print(f"Text (prvnÃ­ch 200 znakÅ¯): {first_sample['text'][:200]}...")
        
        # Kontrola pÅ™Ã­tomnosti system, user, assistant tagÅ¯
        text = first_sample['text']
        has_system = "<|system|>" in text
        has_user = "<|user|>" in text
        has_assistant = "<|assistant|>" in text
        has_end = "<|end|>" in text
        
        print(f"âœ… System tag: {has_system}")
        print(f"âœ… User tag: {has_user}")
        print(f"âœ… Assistant tag: {has_assistant}")
        print(f"âœ… End tag: {has_end}")
        
        # PoÄÃ­tÃ¡nÃ­ tagÅ¯ v celÃ©m datasetu
        system_count = sum(1 for sample in dataset if "<|system|>" in sample['text'])
        user_count = sum(1 for sample in dataset if "<|user|>" in sample['text'])
        assistant_count = sum(1 for sample in dataset if "<|assistant|>" in sample['text'])
        
        print(f"ğŸ“Š Statistiky tagÅ¯ v celÃ©m datasetu:")
        print(f"  System messages: {system_count}")
        print(f"  User messages: {user_count}")
        print(f"  Assistant messages: {assistant_count}")
        
        # Kontrola dÃ©lky textÅ¯
        lengths = [len(sample['text']) for sample in dataset]
        print(f"ğŸ“ DÃ©lka textÅ¯: min={min(lengths)}, max={max(lengths)}, avg={sum(lengths)/len(lengths):.1f}")
    
    # 4. NaÄtenÃ­ modelu
    print(f"\nğŸ¤– NaÄÃ­tÃ¡m model: {args.model_name}")
    
    # PouÅ¾itÃ­ menÅ¡Ã­ho modelu pro Ãºsporu mÃ­sta
    if "mistral" in args.model_name.lower() or "llama" in args.model_name.lower():
        print("âš ï¸ DetekovÃ¡n velkÃ½ model. PouÅ¾Ã­vÃ¡m agresivnÃ­ optimalizaci.")
        print("ğŸ’¡ DostupnÃ© menÅ¡Ã­ modely:")
        print("   - microsoft/DialoGPT-medium (355M)")
        print("   - microsoft/DialoGPT-large (774M)")
        print("   - gpt2-medium (355M)")
        print("   - distilgpt2 (82M)")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
    )
    
    # Pokus o naÄtenÃ­ modelu s retry logikou
    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ Pokus {attempt + 1}/{max_retries} naÄtenÃ­ modelu...")
            
            model = AutoModelForCausalLM.from_pretrained(
                args.model_name,
                quantization_config=bnb_config,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                cache_dir='/workspace/.cache/huggingface/transformers',
                local_files_only=False,
                resume_download=True,
                force_download=False
            )
            print("âœ… Model ÃºspÄ›Å¡nÄ› naÄten!")
            break
            
        except OSError as e:
            if "No space left on device" in str(e):
                print(f"âŒ Pokus {attempt + 1} selhal - nenÃ­ dost mÃ­sta")
                if attempt < max_retries - 1:
                    print("ğŸ§¹ ZkouÅ¡Ã­m dalÅ¡Ã­ vyÄiÅ¡tÄ›nÃ­...")
                    dm.aggressive_cleanup()
                    # PoÄkÃ¡me chvÃ­li
                    import time
                    time.sleep(5)
                else:
                    print("âŒ VÅ¡echny pokusy selhaly. Zkuste:")
                    print("   1. PouÅ¾Ã­t menÅ¡Ã­ model: --model_name microsoft/DialoGPT-medium")
                    print("   2. Restartovat kontejner")
                    print("   3. ZvÃ½Å¡it velikost root filesystem")
                    return
            else:
                raise e
        except Exception as e:
            print(f"âŒ NeoÄekÃ¡vanÃ¡ chyba pÅ™i naÄÃ­tÃ¡nÃ­ modelu: {e}")
            if attempt < max_retries - 1:
                print("ğŸ”„ ZkouÅ¡Ã­m znovu...")
                import time
                time.sleep(10)
            else:
                raise e
    
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name,
        cache_dir='/workspace/.cache/huggingface/transformers',
        local_files_only=False,
        resume_download=True,
        force_download=False
    )
    
    # PÅ™idÃ¡nÃ­ pad tokenu
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.pad_token_id
    
    print(f"âœ… Model naÄten. Vocab size: {model.config.vocab_size}")
    
    # 5. Konfigurace LoRA
    print("\nğŸ”§ Konfiguruji LoRA...")
    
    # DynamickÃ© nastavenÃ­ target_modules podle typu modelu
    if "dialogpt" in args.model_name.lower():
        target_modules = ["c_attn", "c_proj", "wte", "wpe"]
    elif "gpt2" in args.model_name.lower():
        target_modules = ["c_attn", "c_proj", "wte", "wpe"]
    else:
        target_modules = [
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"
        ]
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,  # MenÅ¡Ã­ r pro Ãºsporu pamÄ›ti
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=target_modules
    )
    
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 6. Tokenizace dat
    print("\nğŸ”¤ Tokenizuji data...")
    tokenize_func = lambda examples: tokenize_function(examples, tokenizer, args.max_length)
    tokenized_dataset = dataset.map(
        tokenize_func,
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=100  # MenÅ¡Ã­ batch size pro lepÅ¡Ã­ kontrolu
    )
    
    # Kontrola a oprava padding po tokenizaci
    print("ğŸ”§ Kontroluji a opravuji padding...")
    def fix_padding(example):
        """ZajistÃ­, Å¾e vÅ¡echny sekvence majÃ­ stejnou dÃ©lku"""
        max_len = args.max_length
        current_len = len(example['input_ids'])
        
        if current_len < max_len:
            # PÅ™idÃ¡me padding
            padding_length = max_len - current_len
            example['input_ids'] = example['input_ids'] + [tokenizer.pad_token_id] * padding_length
            example['attention_mask'] = example['attention_mask'] + [0] * padding_length
            example['labels'] = example['labels'] + [-100] * padding_length  # -100 pro ignorovÃ¡nÃ­ v loss
        elif current_len > max_len:
            # OÅ™Ã­zneme na max_length
            example['input_ids'] = example['input_ids'][:max_len]
            example['attention_mask'] = example['attention_mask'][:max_len]
            example['labels'] = example['labels'][:max_len]
        
        return example
    
    # Aplikujeme opravu padding na celÃ½ dataset
    tokenized_dataset = tokenized_dataset.map(
        fix_padding,
        desc="Opravuji padding"
    )
    
    # RozdÄ›lenÃ­ na train/validation s kontrolou velikosti
    print(f"ğŸ“Š CelkovÃ½ poÄet vzorkÅ¯: {len(tokenized_dataset)}")
    
    if len(tokenized_dataset) < 5:
        print("âš ï¸ MÃ¡lo vzorkÅ¯ pro rozdÄ›lenÃ­. PouÅ¾Ã­vÃ¡m celÃ½ dataset pro trÃ©novÃ¡nÃ­.")
        train_dataset = tokenized_dataset
        eval_dataset = tokenized_dataset  # PouÅ¾ijeme stejnÃ½ dataset pro evaluaci
    elif len(tokenized_dataset) < 10:
        # Pro velmi malÃ© datasety pouÅ¾ijeme 80/20 split
        split_ratio = 0.2
        split_dataset = tokenized_dataset.train_test_split(test_size=split_ratio, seed=42)
        train_dataset = split_dataset["train"]
        eval_dataset = split_dataset["test"]
        print(f"âœ… Train dataset: {len(train_dataset)} vzorkÅ¯ ({100-split_ratio*100:.0f}%)")
        print(f"âœ… Validation dataset: {len(eval_dataset)} vzorkÅ¯ ({split_ratio*100:.0f}%)")
    else:
        # StandardnÃ­ 90/10 split
        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)
        train_dataset = split_dataset["train"]
        eval_dataset = split_dataset["test"]
        print(f"âœ… Train dataset: {len(train_dataset)} vzorkÅ¯ (90%)")
        print(f"âœ… Validation dataset: {len(eval_dataset)} vzorkÅ¯ (10%)")
    
    # Kontrola minimÃ¡lnÃ­ velikosti datasetu
    if len(train_dataset) == 0:
        print("âŒ Train dataset je prÃ¡zdnÃ½! Zkontrolujte data.")
        return
    
    if len(eval_dataset) == 0:
        print("âš ï¸ Validation dataset je prÃ¡zdnÃ½. PouÅ¾Ã­vÃ¡m train dataset pro evaluaci.")
        eval_dataset = train_dataset
    
    # Debug: Kontrola train/validation split
    print(f"\nğŸ” DEBUG: Kontrola train/validation split")
    print(f"ğŸ“Š Train dataset: {len(train_dataset)} vzorkÅ¯")
    print(f"ğŸ“Š Validation dataset: {len(eval_dataset)} vzorkÅ¯")
    
    # DetailnÃ­ debug informace o train datasetu
    if len(train_dataset) > 0:
        print(f"\nğŸ“‹ DETAILNÃ DEBUG - TRAIN DATASET:")
        print(f"ğŸ“Š CelkovÃ½ poÄet vzorkÅ¯: {len(train_dataset)}")
        
        # UkÃ¡zka prvnÃ­ch 3 vzorkÅ¯
        for i in range(min(3, len(train_dataset))):
            print(f"\nğŸ“ Train vzorek {i+1}:")
            sample = train_dataset[i]
            decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
            print(f"  DÃ©lka tokenÅ¯: {len(sample['input_ids'])}")
            print(f"  Text (prvnÃ­ch 300 znakÅ¯): {decoded_text[:300]}...")
            
            # Kontrola pÅ™Ã­tomnosti tagÅ¯
            has_system = "<|system|>" in decoded_text
            has_user = "<|user|>" in decoded_text
            has_assistant = "<|assistant|>" in decoded_text
            has_end = "<|end|>" in decoded_text
            print(f"  Tagy: System={has_system}, User={has_user}, Assistant={has_assistant}, End={has_end}")
        
        # Statistiky dÃ©lky tokenÅ¯ v train datasetu
        train_lengths = [len(sample['input_ids']) for sample in train_dataset]
        print(f"\nğŸ“ Statistiky dÃ©lky tokenÅ¯ v train datasetu:")
        print(f"  Min: {min(train_lengths)}")
        print(f"  Max: {max(train_lengths)}")
        print(f"  PrÅ¯mÄ›r: {sum(train_lengths)/len(train_lengths):.1f}")
        print(f"  MediÃ¡n: {sorted(train_lengths)[len(train_lengths)//2]}")
        
        # Kontrola pÅ™Ã­tomnosti tagÅ¯ v celÃ©m train datasetu
        train_texts = [tokenizer.decode(sample['input_ids'], skip_special_tokens=False) for sample in train_dataset]
        train_system_count = sum(1 for text in train_texts if "<|system|>" in text)
        train_user_count = sum(1 for text in train_texts if "<|user|>" in text)
        train_assistant_count = sum(1 for text in train_texts if "<|assistant|>" in text)
        train_end_count = sum(1 for text in train_texts if "<|end|>" in text)
        
        print(f"\nğŸ“Š Tagy v celÃ©m train datasetu:")
        print(f"  System: {train_system_count}/{len(train_dataset)} ({train_system_count/len(train_dataset)*100:.1f}%)")
        print(f"  User: {train_user_count}/{len(train_dataset)} ({train_user_count/len(train_dataset)*100:.1f}%)")
        print(f"  Assistant: {train_assistant_count}/{len(train_dataset)} ({train_assistant_count/len(train_dataset)*100:.1f}%)")
        print(f"  End: {train_end_count}/{len(train_dataset)} ({train_end_count/len(train_dataset)*100:.1f}%)")
    
    # DetailnÃ­ debug informace o validation datasetu
    if len(eval_dataset) > 0:
        print(f"\nğŸ“‹ DETAILNÃ DEBUG - VALIDATION DATASET:")
        print(f"ğŸ“Š CelkovÃ½ poÄet vzorkÅ¯: {len(eval_dataset)}")
        
        # UkÃ¡zka prvnÃ­ch 3 vzorkÅ¯
        for i in range(min(3, len(eval_dataset))):
            print(f"\nğŸ“ Validation vzorek {i+1}:")
            sample = eval_dataset[i]
            decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
            print(f"  DÃ©lka tokenÅ¯: {len(sample['input_ids'])}")
            print(f"  Text (prvnÃ­ch 300 znakÅ¯): {decoded_text[:300]}...")
            
            # Kontrola pÅ™Ã­tomnosti tagÅ¯
            has_system = "<|system|>" in decoded_text
            has_user = "<|user|>" in decoded_text
            has_assistant = "<|assistant|>" in decoded_text
            has_end = "<|end|>" in decoded_text
            print(f"  Tagy: System={has_system}, User={has_user}, Assistant={has_assistant}, End={has_end}")
        
        # Statistiky dÃ©lky tokenÅ¯ v validation datasetu
        eval_lengths = [len(sample['input_ids']) for sample in eval_dataset]
        print(f"\nğŸ“ Statistiky dÃ©lky tokenÅ¯ v validation datasetu:")
        print(f"  Min: {min(eval_lengths)}")
        print(f"  Max: {max(eval_lengths)}")
        print(f"  PrÅ¯mÄ›r: {sum(eval_lengths)/len(eval_lengths):.1f}")
        print(f"  MediÃ¡n: {sorted(eval_lengths)[len(eval_lengths)//2]}")
        
        # Kontrola pÅ™Ã­tomnosti tagÅ¯ v celÃ©m validation datasetu
        eval_texts = [tokenizer.decode(sample['input_ids'], skip_special_tokens=False) for sample in eval_dataset]
        eval_system_count = sum(1 for text in eval_texts if "<|system|>" in text)
        eval_user_count = sum(1 for text in eval_texts if "<|user|>" in text)
        eval_assistant_count = sum(1 for text in eval_texts if "<|assistant|>" in text)
        eval_end_count = sum(1 for text in eval_texts if "<|end|>" in text)
        
        print(f"\nğŸ“Š Tagy v celÃ©m validation datasetu:")
        print(f"  System: {eval_system_count}/{len(eval_dataset)} ({eval_system_count/len(eval_dataset)*100:.1f}%)")
        print(f"  User: {eval_user_count}/{len(eval_dataset)} ({eval_user_count/len(eval_dataset)*100:.1f}%)")
        print(f"  Assistant: {eval_assistant_count}/{len(eval_dataset)} ({eval_assistant_count/len(eval_dataset)*100:.1f}%)")
        print(f"  End: {eval_end_count}/{len(eval_dataset)} ({eval_end_count/len(eval_dataset)*100:.1f}%)")
    
    # PorovnÃ¡nÃ­ train vs validation
    print(f"\nğŸ” POROVNÃNÃ TRAIN vs VALIDATION:")
    print(f"ğŸ“Š PomÄ›r velikostÃ­: {len(train_dataset)}:{len(eval_dataset)} ({len(train_dataset)/len(eval_dataset):.1f}:1)")
    
    if len(train_dataset) > 0 and len(eval_dataset) > 0:
        train_avg_length = sum(len(sample['input_ids']) for sample in train_dataset) / len(train_dataset)
        eval_avg_length = sum(len(sample['input_ids']) for sample in eval_dataset) / len(eval_dataset)
        print(f"ğŸ“ PrÅ¯mÄ›rnÃ¡ dÃ©lka: Train={train_avg_length:.1f}, Validation={eval_avg_length:.1f}")
        
        # Kontrola, zda jsou data podobnÃ¡
        train_sample = tokenizer.decode(train_dataset[0]['input_ids'], skip_special_tokens=False)
        eval_sample = tokenizer.decode(eval_dataset[0]['input_ids'], skip_special_tokens=False)
        
        print(f"ğŸ“ UkÃ¡zka struktury:")
        print(f"  Train prvnÃ­ vzorek: {train_sample[:100]}...")
        print(f"  Validation prvnÃ­ vzorek: {eval_sample[:100]}...")
    
    print(f"\nâœ… System messages jsou v obou datasetech - model se uÄÃ­ na kompletnÃ­ch konverzacÃ­ch")
    print(f"âœ… KaÅ¾dÃ¡ konverzace obsahuje: system + user + assistant")
    print(f"âœ… Data jsou pÅ™ipravena pro fine-tuning")
    
    # 7. Data Collator
    print("\nğŸ”§ Konfiguruji data collator...")
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        return_tensors="pt",
        pad_to_multiple_of=8,  # Padding na nÃ¡sobky 8 pro lepÅ¡Ã­ vÃ½kon
    )
    
    # Test data collator na jednom vzorku
    if len(train_dataset) > 0:
        try:
            test_batch = data_collator([train_dataset[0]])
            print(f"âœ… Data collator test ÃºspÄ›Å¡nÃ½")
            print(f"ğŸ“Š Batch keys: {list(test_batch.keys())}")
            print(f"ğŸ“Š Input shape: {test_batch['input_ids'].shape}")
            print(f"ğŸ“Š Labels shape: {test_batch['labels'].shape}")
        except Exception as e:
            print(f"âš ï¸ Data collator test selhal: {e}")
            print("ğŸ” Debugging informace:")
            print(f"  Sample keys: {list(train_dataset[0].keys())}")
            print(f"  Input IDs length: {len(train_dataset[0]['input_ids'])}")
            print(f"  Labels length: {len(train_dataset[0]['labels'])}")
            print(f"  Sample type: {type(train_dataset[0]['input_ids'])}")
            
            # ZkusÃ­me opravit problÃ©m s padding
            print("ğŸ”§ ZkouÅ¡Ã­m opravit padding...")
            try:
                # VytvoÅ™Ã­me novÃ½ data collator s explicitnÃ­m padding
                fixed_collator = DataCollatorForLanguageModeling(
                    tokenizer=tokenizer,
                    mlm=False,
                    return_tensors="pt",
                    pad_to_multiple_of=8,
                    padding=True,
                )
                test_batch = fixed_collator([train_dataset[0]])
                print(f"âœ… OpravenÃ½ data collator test ÃºspÄ›Å¡nÃ½")
                data_collator = fixed_collator
            except Exception as e2:
                print(f"âŒ Oprava selhala: {e2}")
                print("â„¹ï¸ PokraÄuji s vÃ½chozÃ­m nastavenÃ­m")
    
    # 8. Training Arguments - nastavenÃ­ na network storage
    print("\nâš™ï¸ Nastavuji training arguments...")
    
    # DynamickÃ© nastavenÃ­ podle velikosti datasetu
    if len(train_dataset) < 10:
        # Pro malÃ© datasety
        save_steps = max(1, len(train_dataset) // 2)
        eval_steps = max(1, len(train_dataset) // 2)
        logging_steps = 1
        print(f"ğŸ“Š MalÃ½ dataset - save_steps: {save_steps}, eval_steps: {eval_steps}")
    else:
        # Pro vÄ›tÅ¡Ã­ datasety
        save_steps = 500
        eval_steps = 500
        logging_steps = 10
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=4,
        warmup_steps=min(100, len(train_dataset) // 2),
        learning_rate=args.learning_rate,
        fp16=True,
        logging_steps=logging_steps,
        save_steps=save_steps,
        eval_steps=eval_steps,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="wandb" if args.use_wandb else "none",
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        weight_decay=0.01,
        max_grad_norm=0.3,
        push_to_hub=args.push_to_hub,
        hub_model_id=args.hub_model_id,
        hub_token=HF_TOKEN if args.push_to_hub else None,
        save_total_limit=2,
        logging_dir=f"{args.output_dir}/logs",
        dataloader_num_workers=0,
        dataloader_drop_last=True,  # PÅ™idÃ¡no pro lepÅ¡Ã­ handling batchÅ¯
        group_by_length=True,  # PÅ™idÃ¡no pro lepÅ¡Ã­ padding
    )
    
    # 9. Trainer
    print("\nğŸ‹ï¸ VytvÃ¡Å™Ã­m Trainer...")
    
    # NastavenÃ­ label_names pro PeftModel - robustnÄ›jÅ¡Ã­ pÅ™Ã­stup
    try:
        # ZkusÃ­me nastavit label_names na modelu
        if hasattr(model, 'label_names'):
            model.label_names = ['labels']
        elif hasattr(model, 'config') and hasattr(model.config, 'label_names'):
            model.config.label_names = ['labels']
        
        # Pro PeftModel mÅ¯Å¾eme takÃ© nastavit na base modelu
        if hasattr(model, 'base_model') and hasattr(model.base_model, 'config'):
            model.base_model.config.label_names = ['labels']
        
        print("âœ… Label names nastaveny pro model")
    except Exception as e:
        print(f"âš ï¸ Nelze nastavit label_names: {e}")
        print("â„¹ï¸ PokraÄuji bez explicitnÃ­ho nastavenÃ­ label_names")
    
    # ZajistÃ­me, Å¾e model je v training mÃ³du
    model.train()
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # 10. Fine-tuning
    print("\nğŸš€ SpouÅ¡tÃ­m fine-tuning...")
    trainer.train()
    
    # 11. UloÅ¾enÃ­ modelu na network storage
    print("\nğŸ’¾ UklÃ¡dÃ¡m model na network storage...")
    final_model_path = f"{args.output_dir}-final"
    
    # VytvoÅ™enÃ­ adresÃ¡Å™e pokud neexistuje
    os.makedirs(final_model_path, exist_ok=True)
    
    trainer.save_model(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    
    # VÃ½pis velikosti uloÅ¾enÃ©ho modelu
    try:
        import subprocess
        result = subprocess.run(['du', '-sh', final_model_path], capture_output=True, text=True)
        if result.stdout:
            print(f"ğŸ“Š Velikost modelu: {result.stdout.strip()}")
    except:
        pass
    
    if args.push_to_hub and HF_TOKEN:
        print("ğŸ“¤ NahrÃ¡vÃ¡m model na Hugging Face Hub...")
        model.push_to_hub(args.hub_model_id, token=HF_TOKEN)
        tokenizer.push_to_hub(args.hub_model_id, token=HF_TOKEN)
        print(f"âœ… Model nahrÃ¡n: https://huggingface.co/{args.hub_model_id}")
    
    # 12. TestovÃ¡nÃ­
    print("\nğŸ§ª Testuji model...")
    def generate_response(prompt, max_length=200):
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
    
    test_prompts = [
        "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?",
        "Co si myslÃ­te o opozici?",
        "Jak se vÃ¡m daÅ™Ã­ v Bruselu?"
    ]
    
    print("\nğŸ“ TestovacÃ­ odpovÄ›di:")
    print("=" * 50)
    for prompt in test_prompts:
        print(f"\nPrompt: {prompt}")
        response = generate_response(prompt)
        print(f"OdpovÄ›Ä: {response}")
        print("-" * 30)
    
    # 13. UkonÄenÃ­
    if args.use_wandb:
        wandb.finish()
    
    print("\nğŸ‰ Fine-tuning dokonÄen!")
    print(f"ğŸ“ Model uloÅ¾en v: {final_model_path}")
    print(f"ğŸ’¾ Network storage: {args.output_dir}")
    if args.push_to_hub:
        print(f"ğŸŒ Model dostupnÃ½ na: https://huggingface.co/{args.hub_model_id}")
    
    # VÃ½pis informacÃ­ o uloÅ¾enÃ½ch souborech
    print(f"\nğŸ“‹ UloÅ¾enÃ© soubory:")
    try:
        for root, dirs, files in os.walk(final_model_path):
            level = root.replace(final_model_path, '').count(os.sep)
            indent = ' ' * 2 * level
            print(f"{indent}{os.path.basename(root)}/")
            subindent = ' ' * 2 * (level + 1)
            for file in files[:5]:  # ZobrazÃ­me pouze prvnÃ­ch 5 souborÅ¯
                print(f"{subindent}{file}")
            if len(files) > 5:
                print(f"{subindent}... a dalÅ¡Ã­ch {len(files) - 5} souborÅ¯")
    except Exception as e:
        print(f"âš ï¸ Nelze zobrazit seznam souborÅ¯: {e}")

if __name__ == "__main__":
    main() 