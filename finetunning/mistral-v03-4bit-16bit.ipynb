{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained on RunPod.io\n",
    "\n",
    "- GPU - RTX 3090 24GB / A5000 24 GB VRAM\n",
    "- RAM - 21 GB \n",
    "- HDD - 200 GB\n",
    "\n",
    "Price 0.44$/hod\n",
    "\n",
    "## 4-bit training\n",
    "\n",
    "- training took cca. 120 minutes = 0.88 $\n",
    "\n",
    "## 16-bit merged model\n",
    "\n",
    "- merge took cca. 2 minute = 0.02 $\n",
    "- push took cca. 2 minute = 0.02 $\n",
    "\n",
    "\n",
    "# Inference on TGI \n",
    "https://ui.endpoints.huggingface.co/\n",
    "\n",
    "GPU - L4 24GB VRAM\n",
    "\n",
    "Price 0.8$/hod\n",
    "\n",
    "# Inference on Runpod.io \n",
    "https://ui.endpoints.huggingface.co/\n",
    "\n",
    "GPU - RTX 3090 24GB / A5000 24 GB VRAM\n",
    "\n",
    "Price 0.44$/hod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.1.0+cu118)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.7.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.31.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.24.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.31.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.7.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trl) (3.6.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.51.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.16.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.13.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (6.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "%pip install transformers\n",
    "%pip install peft\n",
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "%pip install trl\n",
    "%pip install matplotlib\n",
    "%pip install tensorboard\n",
    "%pip install sentencepiece\n",
    "%pip install typing_extensions>=4.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: typing_extensions\n",
      "Version: 4.13.2\n",
      "Summary: Backported and Experimental Type Hints for Python 3.8+\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \"Guido van Rossum, Jukka Lehtosalo, ≈Åukasz Langa, Michael Lee\" <levkivskyi@gmail.com>\n",
      "License: \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: \n",
      "Required-by: async-lru, huggingface-hub, multidict, rich, torch\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "API_TOKEN = \"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\"\n",
    "login(token=API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e69f2c2825246c38dc33df1124603d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff475c9740c4ba18eb424d22762a8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30af12ec16a46f6b9150cd97d91a71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f239d76e32f471ca27a82ce26137818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e969d711d72744cfb4cb2ccc644a1f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22514a66cc5c43dba759105e927e2777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95701bcd0c904f07b6dc5c720b677c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings:  Embedding(32768, 4096)\n",
      "Output embeddings:  Linear(in_features=4096, out_features=32768, bias=False)\n",
      "Model Vocabulary Size:  32768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2520673303b42deb34d6658cfc6bfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1599eff562406db9c32813bb699f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c2f87b53d143db9dbad250ec94149c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b0779d2b8040d7aaf007adadcfacd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before add token to tokenizer - tokenizer length:  32768\n",
      "After add token to tokenizer - tokenizer length:  32769\n",
      "Before add pad token to model - pad token Id:  None\n",
      "After add pad token to model - pad token Id:  32768\n",
      "Before resizing Model Vocabulary - Size:  32768\n",
      "After  resizing Model Vocabulary - Size:  32769\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"Input embeddings: \", base_model.get_input_embeddings())\n",
    "print(\"Output embeddings: \", base_model.get_output_embeddings())\n",
    "print(\"Model Vocabulary Size: \", base_model.config.vocab_size)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Before add token to tokenizer - tokenizer length: \", len(base_tokenizer))\n",
    "base_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "print(\"After add token to tokenizer - tokenizer length: \", len(base_tokenizer))\n",
    "\n",
    "print(\"Before add pad token to model - pad token Id: \", base_model.config.pad_token_id)\n",
    "base_model.config.pad_token_id = base_tokenizer.pad_token_id\n",
    "print(\"After add pad token to model - pad token Id: \", base_model.config.pad_token_id)\n",
    "\n",
    "print(\"Before resizing Model Vocabulary - Size: \", base_model.config.vocab_size)\n",
    "base_model.resize_token_embeddings(len(base_tokenizer))\n",
    "print(\"After  resizing Model Vocabulary - Size: \", base_model.config.vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n",
      "Architecture: MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32769, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 32769\n",
      "Input embeddings:\n",
      "Embedding(32769, 4096)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=32769, bias=False)\n",
      "Model device: cuda:0\n",
      "Model is CUDA:  True\n",
      "---Tokenizer---\n",
      "Type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "All tokens count: 32769\n",
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(base_model))\n",
    "print(\"Architecture:\", base_model)\n",
    "print(\"Config:\", base_model.config)\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(base_model.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(base_model.get_output_embeddings())\n",
    "print(\"Model device:\", base_model.device)\n",
    "print(\"Model is CUDA: \", next(base_model.parameters()).is_cuda)\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenizer---\")\n",
    "print(\"Type:\", type(base_tokenizer))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", base_tokenizer.special_tokens_map)\n",
    "print(\"All tokens count:\", len(base_tokenizer))\n",
    "print(\"Padding side:\", base_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472dfe2b758a4abcab52e784b29ec482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/341 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a3d04df3e14fa48dd3c26492c52410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15987555017a456581b9c59af94e517d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 31\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"lukaskellerstein/joe\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(dataset: Dataset):\n",
    "    lengths = [len(x[\"text\"]) for x in dataset]\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color=\"blue\")\n",
    "    plt.xlabel(\"Length of text\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Lengths of text\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_data_lengths(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualize dataset with chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAARrlJREFUeJzt3X98z/X+//H7e2Y/sBmxzY9hMZPfP6tJUWjYEenkx9HBjnQqTkSdjuoT+rV++dWPQ50OqyRSfnQKWWw5oR9kIUUTRm2jZLOVYXt+/+jrfbxt87S397zfuF0vl/fl0vv5er5er8fruedw7/V+Pd8OY4wRAAAAAKBMft4uAAAAAAB8HcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAztHkyZPlcDjOy7m6deumbt26Od+npaXJ4XDonXfeOS/nHzFihBo1anRezuWu/Px83X777YqMjJTD4dC4ceO8XZLHne+fu83KlSvVtm1bBQUFyeFw6PDhw94uCQA8juAEAKdITk6Ww+FwvoKCglS3bl3Fx8fr+eef15EjRzxynh9//FGTJ09Wenq6R47nSb5c29l48sknlZycrLvuuktvvPGG/vznP5fZt1GjRvrDH/5wHqsrn/nz52vGjBneLuOMfv75Zw0cOFDBwcF66aWX9MYbb6hq1aql9l2/fr0mT55c4cHq119/1eTJk5WWllah5wFwafH3dgEA4IseffRRRUdH6/jx48rOzlZaWprGjRunadOm6b333lPr1q2dfR9++GH94x//KNfxf/zxR02ZMkWNGjVS27Ztz3q/VatWles87jhTbf/6179UXFxc4TWcizVr1ujqq6/WpEmTvF3KOZs/f762bdvm03fNvvjiCx05ckSPPfaYevTocca+69ev15QpUzRixAiFhYVVWE2//vqrpkyZIkkud2gB4FwQnACgFL1791bHjh2d7ydOnKg1a9boD3/4g2666SZ98803Cg4OliT5+/vL379i/zj99ddfVaVKFQUEBFToeWwqV67s1fOfjQMHDqh58+beLuOSceDAAUmq0CAEAL6Aj+oBwFm64YYb9H//93/au3ev5s2b52wv7RmnlJQUdenSRWFhYapWrZpiY2P14IMPSvr9+ZROnTpJkhITE50fC0xOTpb0+/8hb9mypTZt2qTrrrtOVapUce57+jNOJxUVFenBBx9UZGSkqlatqptuukn79u1z6dOoUSONGDGixL6nHtNWW2nPOBUUFGjChAmKiopSYGCgYmNj9dxzz8kY49LP4XBozJgxWrp0qVq2bKnAwEC1aNFCK1euLH3AT3PgwAGNHDlSERERCgoKUps2bfTaa685t5987mf37t364IMPnLXv2bPnrI5/JvPmzVOHDh0UHBysmjVravDgwSXG9+TPbfv27br++utVpUoV1atXT88880yJ4+3du1c33XSTqlatqvDwcN1777368MMP5XA4nB8v69atmz744APt3bvXeS2nj31xcbGeeOIJ1a9fX0FBQerevbsyMjJc+nz33Xe65ZZbFBkZqaCgINWvX1+DBw9Wbm6u9boXLVrkvO5atWrptttu0w8//OByzcOHD5ckderUSQ6Ho9Q5Jv3+e3L//fdLkqKjo0v9+djGee7cuXI4HJozZ47LsZ988kk5HA4tX75ce/bsUe3atSVJU6ZMcZ5n8uTJ1usFgDPhjhMAlMOf//xnPfjgg1q1apVGjRpVap+vv/5af/jDH9S6dWs9+uijCgwMVEZGhtatWydJuuKKK/Too4/qkUce0R133KFrr71WktS5c2fnMX7++Wf17t1bgwcP1m233aaIiIgz1vXEE0/I4XDogQce0IEDBzRjxgz16NFD6enpzjtjZ+NsajuVMUY33XSTUlNTNXLkSLVt21Yffvih7r//fv3www+aPn26S/9PPvlEixcv1t13362QkBA9//zzuuWWW5SZmanLLruszLp+++03devWTRkZGRozZoyio6O1aNEijRgxQocPH9bYsWN1xRVX6I033tC9996r+vXra8KECZLk/Ee0u5544gn93//9nwYOHKjbb79dBw8e1AsvvKDrrrtOmzdvdrnT8ssvv6hXr14aMGCABg4cqHfeeUcPPPCAWrVqpd69e0v6PWjecMMNysrK0tixYxUZGan58+crNTXV5bwPPfSQcnNztX//fuc4VqtWzaXPU089JT8/P913333Kzc3VM888o6FDh+qzzz6TJB07dkzx8fEqLCzU3/72N0VGRuqHH37Q+++/r8OHD6t69eplXndycrISExPVqVMnJSUlKScnRzNnztS6deuc1/3QQw8pNjZWr7zyivPjrY0bNy71eAMGDNDOnTv11ltvafr06apVq5ak//18zmacExMTtXjxYo0fP149e/ZUVFSUtm7dqilTpmjkyJHq06ePCgoKNGvWLN111126+eabNWDAAEly+XgtALjFAACc5s6daySZL774osw+1atXN+3atXO+nzRpkjn1j9Pp06cbSebgwYNlHuOLL74wkszcuXNLbOvatauRZGbPnl3qtq5duzrfp6amGkmmXr16Ji8vz9n+9ttvG0lm5syZzraGDRua4cOHW495ptqGDx9uGjZs6Hy/dOlSI8k8/vjjLv3++Mc/GofDYTIyMpxtkkxAQIBL21dffWUkmRdeeKHEuU41Y8YMI8nMmzfP2Xbs2DETFxdnqlWr5nLtDRs2NAkJCWc83tn23bNnj6lUqZJ54oknXNq3bt1q/P39XdpP/txef/11Z1thYaGJjIw0t9xyi7Nt6tSpRpJZunSps+23334zzZo1M5JMamqqsz0hIcFlvE86+XO/4oorTGFhobN95syZRpLZunWrMcaYzZs3G0lm0aJF9sE4xbFjx0x4eLhp2bKl+e2335zt77//vpFkHnnkEWfb2fzOnPTss88aSWb37t0u7eUZ56ysLFOzZk3Ts2dPU1hYaNq1a2caNGhgcnNznX0OHjxoJJlJkyaV67oB4Ez4qB4AlFO1atXOuLreyTsQy5Ytc3shhcDAQCUmJp51/2HDhikkJMT5/o9//KPq1Kmj5cuXu3X+s7V8+XJVqlRJ99xzj0v7hAkTZIzRihUrXNp79OjhckeidevWCg0N1ffff289T2RkpIYMGeJsq1y5su655x7l5+fr448/9sDVlLR48WIVFxdr4MCB+umnn5yvyMhIxcTElLhLVK1aNd12223O9wEBAbryyitdrm/lypWqV6+ebrrpJmdbUFBQmXcwzyQxMdHlubeTdwhPnu/kHaUPP/xQv/7661kfd+PGjTpw4IDuvvtuBQUFOdsTEhLUrFkzffDBB+Wu9UzKM86RkZF66aWXlJKSomuvvVbp6emaM2eOQkNDPVoTAJyO4AQA5ZSfn+8SUk43aNAgXXPNNbr99tsVERGhwYMH6+233y5XiKpXr165FoKIiYlxee9wONSkSROPPN9zJnv37lXdunVLjMcVV1zh3H6qBg0alDhGjRo19Msvv1jPExMTIz8/17+2yjqPp3z33XcyxigmJka1a9d2eX3zzTfOhRFOql+/fonn3U6/vr1796px48Yl+jVp0qTc9Z0+njVq1JAk5/mio6M1fvx4vfrqq6pVq5bi4+P10ksvWZ9vOjmesbGxJbY1a9bM4+Nd3nEePHiwEhIS9Pnnn2vUqFHq3r27R+sBgNLwjBMAlMP+/fuVm5t7xn/kBgcHa+3atUpNTdUHH3yglStXauHChbrhhhu0atUqVapUyXqe8jyXdLbK+pLeoqKis6rJE8o6jzltIQlfUVxcLIfDoRUrVpRa++nPHJ3v6zub802dOlUjRozQsmXLtGrVKt1zzz1KSkrSp59+qvr161dIXeVV3nH++eeftXHjRknS9u3bVVxcXCJUA4CnEZwAoBzeeOMNSVJ8fPwZ+/n5+al79+7q3r27pk2bpieffFIPPfSQUlNT1aNHjzJDjLu+++47l/fGGGVkZLg8EF+jRo1Sv3h07969uvzyy53vy1Nbw4YN9dFHH+nIkSMud52+/fZb53ZPaNiwobZs2VLiH8iePs/pGjduLGOMoqOj1bRpU48cs2HDhtq+fbuMMS5jffpqeFL5fhZn0qpVK7Vq1UoPP/yw1q9fr2uuuUazZ8/W448/XmaNkrRjxw7dcMMNLtt27Njh9niXdT3lHefRo0fryJEjSkpK0sSJEzVjxgyNHz/eeh4AOBf87xkAOEtr1qzRY489pujoaA0dOrTMfocOHSrRdvKLZAsLCyVJVatWlaRSg4w7Xn/9dZfnrt555x1lZWU5V3KTfv/H6aeffqpjx445295///0Sy2qXp7Y+ffqoqKhIL774okv79OnT5XA4XM5/Lvr06aPs7GwtXLjQ2XbixAm98MILqlatmrp27eqR85xuwIABqlSpkqZMmVLirpExRj///HO5jxkfH68ffvhB7733nrPt6NGj+te//lWib9WqVc9q2fCy5OXl6cSJEy5trVq1kp+fn3MulqZjx44KDw/X7NmzXfqtWLFC33zzjRISEtyqp6y5VZ5xfuedd7Rw4UI99dRT+sc//qHBgwfr4Ycf1s6dO519qlSpUup5AOBccMcJAEqxYsUKffvttzpx4oRycnK0Zs0apaSkqGHDhnrvvfdcHpg/3aOPPqq1a9cqISFBDRs21IEDB/TPf/5T9evXV5cuXST9HmLCwsI0e/ZshYSEqGrVqrrqqqsUHR3tVr01a9ZUly5dlJiYqJycHM2YMUNNmjRxWXDg9ttv1zvvvKNevXpp4MCB2rVrl+bNm1di+ejy1Na3b19df/31euihh7Rnzx61adNGq1at0rJlyzRu3Lgyl6YurzvuuEMvv/yyRowYoU2bNqlRo0Z65513tG7dOs2YMeOMz5zZZGRklHrnpV27dkpISNDjjz+uiRMnas+ePerfv79CQkK0e/duLVmyRHfccYfuu+++cp3vr3/9q1588UUNGTJEY8eOVZ06dfTmm28659Spd0s6dOighQsXavz48erUqZOqVaumvn37nvW51qxZozFjxujWW29V06ZNdeLECb3xxhuqVKmSbrnlljL3q1y5sp5++mklJiaqa9euGjJkiHM58kaNGunee+8t1zWfej3S70utDx48WJUrV1bfvn3VuHHjsxrnAwcO6K677tL111+vMWPGSJJefPFFpaamasSIEfrkk0/k5+en4OBgNW/eXAsXLlTTpk1Vs2ZNtWzZUi1btnSrbgCQxHLkAHCqk0srn3wFBASYyMhI07NnTzNz5kyXZa9POn058tWrV5t+/fqZunXrmoCAAFO3bl0zZMgQs3PnTpf9li1bZpo3b278/f1dlv/u2rWradGiRan1lbUc+VtvvWUmTpxowsPDTXBwsElISDB79+4tsf/UqVNNvXr1TGBgoLnmmmvMxo0bSxzzTLWdvhy5McYcOXLE3HvvvaZu3bqmcuXKJiYmxjz77LOmuLjYpZ8kM3r06BI1lbVM+ulycnJMYmKiqVWrlgkICDCtWrUqdcn08i5HfurP+9TXyJEjnf3effdd06VLF1O1alVTtWpV06xZMzN69GizY8cOZ5+yfm6ljdn3339vEhISTHBwsKldu7aZMGGCeffdd40k8+mnnzr75efnmz/96U8mLCzMSHIe5+TP/fRlxnfv3u3y8/r+++/NX/7yF9O4cWMTFBRkatasaa6//nrz0UcfndX4LFy40LRr184EBgaamjVrmqFDh5r9+/e79CnPcuTGGPPYY4+ZevXqGT8/vxJLk9vGecCAASYkJMTs2bPH5ZjLli0zkszTTz/tbFu/fr3p0KGDCQgIYGlyAB7hMMZHn8gFAOASMmPGDN17773av3+/6tWr5+1yAACnITgBAHCe/fbbby4rJx49elTt2rVTUVGRy7M6AADfwTNOAACcZwMGDFCDBg3Utm1b5ebmat68efr222/15ptvers0AEAZCE4AAJxn8fHxevXVV/Xmm2+qqKhIzZs314IFCzRo0CBvlwYAKAMf1QMAAAAAC77HCQAAAAAsCE4AAAAAYHHJPeNUXFysH3/8USEhIS5fMggAAADg0mKM0ZEjR1S3bl35+Z35ntIlF5x+/PFHRUVFebsMAAAAAD5i3759ql+//hn7XHLBKSQkRNLvgxMaGurlagAAAAB4S15enqKiopwZ4UwuueB08uN5oaGhBCcAAAAAZ/UID4tDAAAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFl4NTrNmzVLr1q0VGhqq0NBQxcXFacWKFWfcZ9GiRWrWrJmCgoLUqlUrLV++/DxVCwAAAOBS5dXgVL9+fT311FPatGmTNm7cqBtuuEH9+vXT119/XWr/9evXa8iQIRo5cqQ2b96s/v37q3///tq2bdt5rhwAAADApcRhjDHeLuJUNWvW1LPPPquRI0eW2DZo0CAVFBTo/fffd7ZdffXVatu2rWbPnn1Wx8/Ly1P16tWVm5ur0NBQj9UNAAAA4MJSnmzgM884FRUVacGCBSooKFBcXFypfTZs2KAePXq4tMXHx2vDhg1lHrewsFB5eXkuLwAAAAAoD39vF7B161bFxcXp6NGjqlatmpYsWaLmzZuX2jc7O1sREREubREREcrOzi7z+ElJSZoyZYpHa/a0vn29XcH//Oc/3q4AcB+/S6XzpXGRfGtsAAA4W16/4xQbG6v09HR99tlnuuuuuzR8+HBt377dY8efOHGicnNzna99+/Z57NgAAAAALg1ev+MUEBCgJk2aSJI6dOigL774QjNnztTLL79com9kZKRycnJc2nJychQZGVnm8QMDAxUYGOjZogEAAABcUrx+x+l0xcXFKiwsLHVbXFycVq9e7dKWkpJS5jNRAAAAAOAJXr3jNHHiRPXu3VsNGjTQkSNHNH/+fKWlpenDDz+UJA0bNkz16tVTUlKSJGns2LHq2rWrpk6dqoSEBC1YsEAbN27UK6+84s3LAAAAAHCR82pwOnDggIYNG6asrCxVr15drVu31ocffqiePXtKkjIzM+Xn97+bYp07d9b8+fP18MMP68EHH1RMTIyWLl2qli1beusSAAAAAFwCvBqc/v3vf59xe1paWom2W2+9VbfeemsFVQQAAAAAJfncM04AAAAA4GsITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABZeDU5JSUnq1KmTQkJCFB4erv79+2vHjh1n3Cc5OVkOh8PlFRQUdJ4qBgAAAHAp8mpw+vjjjzV69Gh9+umnSklJ0fHjx3XjjTeqoKDgjPuFhoYqKyvL+dq7d+95qhgAAADApcjfmydfuXKly/vk5GSFh4dr06ZNuu6668rcz+FwKDIysqLLAwAAAABJPvaMU25uriSpZs2aZ+yXn5+vhg0bKioqSv369dPXX39dZt/CwkLl5eW5vAAAAACgPHwmOBUXF2vcuHG65ppr1LJlyzL7xcbGas6cOVq2bJnmzZun4uJide7cWfv37y+1f1JSkqpXr+58RUVFVdQlAAAAALhI+UxwGj16tLZt26YFCxacsV9cXJyGDRumtm3bqmvXrlq8eLFq166tl19+udT+EydOVG5urvO1b9++iigfAAAAwEXMq884nTRmzBi9//77Wrt2rerXr1+ufStXrqx27dopIyOj1O2BgYEKDAz0RJkAAAAALlFeveNkjNGYMWO0ZMkSrVmzRtHR0eU+RlFRkbZu3ao6depUQIUAAAAA4OU7TqNHj9b8+fO1bNkyhYSEKDs7W5JUvXp1BQcHS5KGDRumevXqKSkpSZL06KOP6uqrr1aTJk10+PBhPfvss9q7d69uv/12r10HAAAAgIubV4PTrFmzJEndunVzaZ87d65GjBghScrMzJSf3/9ujP3yyy8aNWqUsrOzVaNGDXXo0EHr169X8+bNz1fZAAAAAC4xXg1Oxhhrn7S0NJf306dP1/Tp0yuoIgAAAAAoyWdW1QMAAAAAX0VwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsPBqcEpKSlKnTp0UEhKi8PBw9e/fXzt27LDut2jRIjVr1kxBQUFq1aqVli9ffh6qBQAAAHCp8mpw+vjjjzV69Gh9+umnSklJ0fHjx3XjjTeqoKCgzH3Wr1+vIUOGaOTIkdq8ebP69++v/v37a9u2beexcgAAAACXEocxxni7iJMOHjyo8PBwffzxx7ruuutK7TNo0CAVFBTo/fffd7ZdffXVatu2rWbPnm09R15enqpXr67c3FyFhoZ6rPZz0bevtyv4n//8x9sVAO7jd6l0vjQukm+NDQDg0laebOBTzzjl5uZKkmrWrFlmnw0bNqhHjx4ubfHx8dqwYUOp/QsLC5WXl+fyAgAAAIDy8Pd2AScVFxdr3Lhxuuaaa9SyZcsy+2VnZysiIsKlLSIiQtnZ2aX2T0pK0pQpUzxaKy49vvZ/7OH7mDNlY2xKx5240vnafOHnhAuZL/0+XYi/Sz5zx2n06NHatm2bFixY4NHjTpw4Ubm5uc7Xvn37PHp8AAAAABc/n7jjNGbMGL3//vtau3at6tevf8a+kZGRysnJcWnLyclRZGRkqf0DAwMVGBjosVoBAAAAXHq8esfJGKMxY8ZoyZIlWrNmjaKjo637xMXFafXq1S5tKSkpiouLq6gyAQAAAFzivHrHafTo0Zo/f76WLVumkJAQ53NK1atXV3BwsCRp2LBhqlevnpKSkiRJY8eOVdeuXTV16lQlJCRowYIF2rhxo1555RWvXQcAAACAi5tX7zjNmjVLubm56tatm+rUqeN8LVy40NknMzNTWVlZzvedO3fW/Pnz9corr6hNmzZ65513tHTp0jMuKAEAAAAA58Krd5zO5iuk0tLSSrTdeuutuvXWWyugIgAAAAAoyWdW1QMAAAAAX0VwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgIVbwen777/3dB0AAAAA4LPcCk5NmjTR9ddfr3nz5uno0aOergkAAAAAfIpbwenLL79U69atNX78eEVGRuqvf/2rPv/8c0/XBgAAAAA+wa3g1LZtW82cOVM//vij5syZo6ysLHXp0kUtW7bUtGnTdPDgQU/XCQAAAABec06LQ/j7+2vAgAFatGiRnn76aWVkZOi+++5TVFSUhg0bpqysLE/VCQAAAABec07BaePGjbr77rtVp04dTZs2Tffdd5927dqllJQU/fjjj+rXr5+n6gQAAAAAr/F3Z6dp06Zp7ty52rFjh/r06aPXX39dffr0kZ/f7zksOjpaycnJatSokSdrBQAAAACvcCs4zZo1S3/5y180YsQI1alTp9Q+4eHh+ve//31OxQEAAACAL3ArOH333XfWPgEBARo+fLg7hwcAAAAAn+LWM05z587VokWLSrQvWrRIr7322jkXBQAAAAC+xK3glJSUpFq1apVoDw8P15NPPnnORQEAAACAL3ErOGVmZio6OrpEe8OGDZWZmXnORQEAAACAL3ErOIWHh2vLli0l2r/66itddtll51wUAAAAAPgSt4LTkCFDdM899yg1NVVFRUUqKirSmjVrNHbsWA0ePNjTNQIAAACAV7m1qt5jjz2mPXv2qHv37vL3//0QxcXFGjZsGM84AQAAALjouBWcAgICtHDhQj322GP66quvFBwcrFatWqlhw4aerg8AAAAAvM6t4HRS06ZN1bRpU0/VAgAAAAA+ya3gVFRUpOTkZK1evVoHDhxQcXGxy/Y1a9Z4pDgAAAAA8AVuBaexY8cqOTlZCQkJatmypRwOh6frAgAAAACf4VZwWrBggd5++2316dPH0/UAAAAAgM9xaznygIAANWnSxNO1AAAAAIBPcis4TZgwQTNnzpQxxtP1AAAAAIDPceujep988olSU1O1YsUKtWjRQpUrV3bZvnjxYo8UBwAAAAC+wK3gFBYWpptvvtnTtQAAAACAT3IrOM2dO9fTdQAAAACAz3LrGSdJOnHihD766CO9/PLLOnLkiCTpxx9/VH5+vseKAwAAAABf4NYdp71796pXr17KzMxUYWGhevbsqZCQED399NMqLCzU7NmzPV0nAAAAAHiNW3ecxo4dq44dO+qXX35RcHCws/3mm2/W6tWrPVYcAAAAAPgCt+44/fe//9X69esVEBDg0t6oUSP98MMPHikMAAAAAHyFW3eciouLVVRUVKJ9//79CgkJOeeiAAAAAMCXuBWcbrzxRs2YMcP53uFwKD8/X5MmTVKfPn08VRsAAAAA+AS3Pqo3depUxcfHq3nz5jp69Kj+9Kc/6bvvvlOtWrX01ltvebpGAAAAAPAqt4JT/fr19dVXX2nBggXasmWL8vPzNXLkSA0dOtRlsQgAAAAAuBi4FZwkyd/fX7fddpsnawEAAAAAn+RWcHr99dfPuH3YsGFuFQMAAAAAvsit4DR27FiX98ePH9evv/6qgIAAValSheAEAAAA4KLi1qp6v/zyi8srPz9fO3bsUJcuXVgcAgAAAMBFx63gVJqYmBg99dRTJe5GAQAAAMCFzmPBSfp9wYgff/zRk4cEAAAAAK9z6xmn9957z+W9MUZZWVl68cUXdc0113ikMAAAAADwFW4Fp/79+7u8dzgcql27tm644QZNnTr1rI+zdu1aPfvss9q0aZOysrK0ZMmSEsc+VVpamq6//voS7VlZWYqMjDzr8wIAAABAebgVnIqLiz1y8oKCArVp00Z/+ctfNGDAgLPeb8eOHQoNDXW+Dw8P90g9AAAAAFAat78A1xN69+6t3r17l3u/8PBwhYWFeb4gAAAAACiFW8Fp/PjxZ9132rRp7pzijNq2bavCwkK1bNlSkydPPuNzVYWFhSosLHS+z8vL83g9AAAAAC5ubgWnzZs3a/PmzTp+/LhiY2MlSTt37lSlSpXUvn17Zz+Hw+GZKv+/OnXqaPbs2erYsaMKCwv16quvqlu3bvrss89cznuqpKQkTZkyxaN1AAAAALi0uBWc+vbtq5CQEL322muqUaOGpN+/FDcxMVHXXnutJkyY4NEiT4qNjXUGNUnq3Lmzdu3apenTp+uNN94odZ+JEye63CHLy8tTVFRUhdQHAAAA4OLkVnCaOnWqVq1a5QxNklSjRg09/vjjuvHGGyssOJXmyiuv1CeffFLm9sDAQAUGBp63egAAAABcfNz6Aty8vDwdPHiwRPvBgwd15MiRcy6qPNLT01WnTp3zek4AAAAAlxa37jjdfPPNSkxM1NSpU3XllVdKkj777DPdf//95VpWPD8/XxkZGc73u3fvVnp6umrWrKkGDRpo4sSJ+uGHH/T6669LkmbMmKHo6Gi1aNFCR48e1auvvqo1a9Zo1apV7lwGAAAAAJwVt4LT7Nmzdd999+lPf/qTjh8//vuB/P01cuRIPfvss2d9nI0bN7p8oe3JZ5GGDx+u5ORkZWVlKTMz07n92LFjmjBhgn744QdVqVJFrVu31kcffVTql+ICAAAAgKc4jDHG3Z0LCgq0a9cuSVLjxo1VtWpVjxVWUfLy8lS9enXl5ua6fImuN/Xt6+0K/uc///F2Bb7Jl35GAC5O/PlbOl/785efEy5kvvT75Cu/S+XJBm4943RSVlaWsrKyFBMTo6pVq+ocMhgAAAAA+Cy3gtPPP/+s7t27q2nTpurTp4+ysrIkSSNHjjyvK+oBAAAAwPngVnC69957VblyZWVmZqpKlSrO9kGDBmnlypUeKw4AAAAAfIFbi0OsWrVKH374oerXr+/SHhMTo71793qkMAAAAADwFW7dcSooKHC503TSoUOH+LJZAAAAABcdt4LTtdde6/xuJUlyOBwqLi7WM888w9LgAAAAAC46bn1U75lnnlH37t21ceNGHTt2TH//+9/19ddf69ChQ1q3bp2nawQAAAAAr3LrjlPLli21c+dOdenSRf369VNBQYEGDBigzZs3q3Hjxp6uEQAAAAC8qtx3nI4fP65evXpp9uzZeuihhyqiJgAAAADwKeW+41S5cmVt2bKlImoBAAAAAJ/k1kf1brvtNv373//2dC0AAAAA4JPcWhzixIkTmjNnjj766CN16NBBVatWddk+bdo0jxQHAAAAAL6gXMHp+++/V6NGjbRt2za1b99ekrRz506XPg6Hw3PVAQAAAIAPKFdwiomJUVZWllJTUyVJgwYN0vPPP6+IiIgKKQ4AAAAAfEG5nnEyxri8X7FihQoKCjxaEAAAAAD4GrcWhzjp9CAFAAAAABejcgUnh8NR4hkmnmkCAAAAcLEr1zNOxhiNGDFCgYGBkqSjR4/qzjvvLLGq3uLFiz1XIQAAAAB4WbmC0/Dhw13e33bbbR4tBgAAAAB8UbmC09y5cyuqDgAAAADwWee0OAQAAAAAXAoITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYOHV4LR27Vr17dtXdevWlcPh0NKlS637pKWlqX379goMDFSTJk2UnJxc4XUCAAAAuLR5NTgVFBSoTZs2eumll86q/+7du5WQkKDrr79e6enpGjdunG6//XZ9+OGHFVwpAAAAgEuZvzdP3rt3b/Xu3fus+8+ePVvR0dGaOnWqJOmKK67QJ598ounTpys+Pr6iygQAAABwibugnnHasGGDevTo4dIWHx+vDRs2lLlPYWGh8vLyXF4AAAAAUB5eveNUXtnZ2YqIiHBpi4iIUF5enn777TcFBweX2CcpKUlTpkw5XyVe8Pr29XYFAABv4++CsjE2vu8///F2BbhYXVB3nNwxceJE5ebmOl/79u3zdkkAAAAALjAX1B2nyMhI5eTkuLTl5OQoNDS01LtNkhQYGKjAwMDzUR4AAACAi9QFdccpLi5Oq1evdmlLSUlRXFyclyoCAAAAcCnwanDKz89Xenq60tPTJf2+3Hh6eroyMzMl/f4xu2HDhjn733nnnfr+++/197//Xd9++63++c9/6u2339a9997rjfIBAAAAXCK8Gpw2btyodu3aqV27dpKk8ePHq127dnrkkUckSVlZWc4QJUnR0dH64IMPlJKSojZt2mjq1Kl69dVXWYocAAAAQIXy6jNO3bp1kzGmzO3Jycml7rN58+YKrAoAAAAAXF1QzzgBAAAAgDcQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACx8Iji99NJLatSokYKCgnTVVVfp888/L7NvcnKyHA6HyysoKOg8VgsAAADgUuP14LRw4UKNHz9ekyZN0pdffqk2bdooPj5eBw4cKHOf0NBQZWVlOV979+49jxUDAAAAuNR4PThNmzZNo0aNUmJiopo3b67Zs2erSpUqmjNnTpn7OBwORUZGOl8RERHnsWIAAAAAlxqvBqdjx45p06ZN6tGjh7PNz89PPXr00IYNG8rcLz8/Xw0bNlRUVJT69eunr7/+usy+hYWFysvLc3kBAAAAQHl4NTj99NNPKioqKnHHKCIiQtnZ2aXuExsbqzlz5mjZsmWaN2+eiouL1blzZ+3fv7/U/klJSapevbrzFRUV5fHrAAAAAHBx8/pH9corLi5Ow4YNU9u2bdW1a1ctXrxYtWvX1ssvv1xq/4kTJyo3N9f52rdv33muGAAAAMCFzt+bJ69Vq5YqVaqknJwcl/acnBxFRkae1TEqV66sdu3aKSMjo9TtgYGBCgwMPOdaAQAAAFy6vHrHKSAgQB06dNDq1audbcXFxVq9erXi4uLO6hhFRUXaunWr6tSpU1FlAgAAALjEefWOkySNHz9ew4cPV8eOHXXllVdqxowZKigoUGJioiRp2LBhqlevnpKSkiRJjz76qK6++mo1adJEhw8f1rPPPqu9e/fq9ttv9+ZlAAAAALiIeT04DRo0SAcPHtQjjzyi7OxstW3bVitXrnQuGJGZmSk/v//dGPvll180atQoZWdnq0aNGurQoYPWr1+v5s2be+sSAAAAAFzkHMYY4+0izqe8vDxVr15dubm5Cg0N9XY5kqS+fb1dAQDA2/7zH29X8D/8vYQLmS/9LvkaX/rd9pWfU3mywQW3qh4AAAAAnG8EJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABYEJwAAAAAwILgBAAAAAAWBCcAAAAAsCA4AQAAAIAFwQkAAAAALAhOAAAAAGBBcAIAAAAAC4ITAAAAAFgQnAAAAADAguAEAAAAABYEJwAAAACwIDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABY+ERweumll9SoUSMFBQXpqquu0ueff37G/osWLVKzZs0UFBSkVq1aafny5eepUgAAAACXIq8Hp4ULF2r8+PGaNGmSvvzyS7Vp00bx8fE6cOBAqf3Xr1+vIUOGaOTIkdq8ebP69++v/v37a9u2bee5cgAAAACXCq8Hp2nTpmnUqFFKTExU8+bNNXv2bFWpUkVz5swptf/MmTPVq1cv3X///briiiv02GOPqX379nrxxRfPc+UAAAAALhX+3jz5sWPHtGnTJk2cONHZ5ufnpx49emjDhg2l7rNhwwaNHz/epS0+Pl5Lly4ttX9hYaEKCwud73NzcyVJeXl551i95xw/7u0KAADe5kN/LfH3Ei5ovvS75Gt86XfbV35OJzOBMcba16vB6aefflJRUZEiIiJc2iMiIvTtt9+Wuk92dnap/bOzs0vtn5SUpClTppRoj4qKcrNqAAA8r3p1b1cAXBz4Xbow+NrP6ciRI6puKcqrwel8mDhxossdquLiYh06dEiXXXaZHA6H8vLyFBUVpX379ik0NNSLlV5cGNeKwbhWHMa2YjCuFYNxrRiMa8VgXCsG4+oZxhgdOXJEdevWtfb1anCqVauWKlWqpJycHJf2nJwcRUZGlrpPZGRkufoHBgYqMDDQpS0sLKxEv9DQUCZdBWBcKwbjWnEY24rBuFYMxrViMK4Vg3GtGIzrubPdaTrJq4tDBAQEqEOHDlq9erWzrbi4WKtXr1ZcXFyp+8TFxbn0l6SUlJQy+wMAAADAufL6R/XGjx+v4cOHq2PHjrryyis1Y8YMFRQUKDExUZI0bNgw1atXT0lJSZKksWPHqmvXrpo6daoSEhK0YMECbdy4Ua+88oo3LwMAAADARczrwWnQoEE6ePCgHnnkEWVnZ6tt27ZauXKlcwGIzMxM+fn978ZY586dNX/+fD388MN68MEHFRMTo6VLl6ply5ZunT8wMFCTJk0q8XE+nBvGtWIwrhWHsa0YjGvFYFwrBuNaMRjXisG4nn8OczZr7wEAAADAJczrX4ALAAAAAL6O4AQAAAAAFgQnAAAAALAgOAEAAACAxQUZnNauXau+ffuqbt26cjgcWrp0qcv2ESNGyOFwuLx69erl0ufQoUMaOnSoQkNDFRYWppEjRyo/P9+lz5YtW3TttdcqKChIUVFReuaZZ0rUsmjRIjVr1kxBQUFq1aqVli9f7vHrPR+SkpLUqVMnhYSEKDw8XP3799eOHTtc+hw9elSjR4/WZZddpmrVqumWW24p8WXEmZmZSkhIUJUqVRQeHq77779fJ06ccOmTlpam9u3bKzAwUE2aNFFycnKJel566SU1atRIQUFBuuqqq/T55597/JrPl7MZ227dupWYs3feeadLH8bW1axZs9S6dWvnF//FxcVpxYoVzu3MV/fYxpW56hlPPfWUHA6Hxo0b52xjzp670saVOVt+kydPLjFmzZo1c25nrrrPNrbMVx9nLkDLly83Dz30kFm8eLGRZJYsWeKyffjw4aZXr14mKyvL+Tp06JBLn169epk2bdqYTz/91Pz3v/81TZo0MUOGDHFuz83NNREREWbo0KFm27Zt5q233jLBwcHm5ZdfdvZZt26dqVSpknnmmWfM9u3bzcMPP2wqV65stm7dWqHXXxHi4+PN3LlzzbZt20x6errp06ePadCggcnPz3f2ufPOO01UVJRZvXq12bhxo7n66qtN586dndtPnDhhWrZsaXr06GE2b95sli9fbmrVqmUmTpzo7PP999+bKlWqmPHjx5vt27ebF154wVSqVMmsXLnS2WfBggUmICDAzJkzx3z99ddm1KhRJiwszOTk5JyfwfCwsxnbrl27mlGjRrnM2dzcXOd2xrak9957z3zwwQdm586dZseOHebBBx80lStXNtu2bTPGMF/dZRtX5uq5+/zzz02jRo1M69atzdixY53tzNlzU9a4MmfLb9KkSaZFixYuY3bw4EHnduaq+2xjy3z1bRdkcDpVWcGpX79+Ze6zfft2I8l88cUXzrYVK1YYh8NhfvjhB2OMMf/85z9NjRo1TGFhobPPAw88YGJjY53vBw4caBISElyOfdVVV5m//vWv53BFvuHAgQNGkvn444+NMcYcPnzYVK5c2SxatMjZ55tvvjGSzIYNG4wxvwdaPz8/k52d7ewza9YsExoa6hzHv//976ZFixYu5xo0aJCJj493vr/yyivN6NGjne+LiopM3bp1TVJSkucv1AtOH1tjfv+D8tS/6E/H2J6dGjVqmFdffZX56mEnx9UY5uq5OnLkiImJiTEpKSkuY8mcPTdljasxzFl3TJo0ybRp06bUbczVc3OmsTWG+errLsiP6p2NtLQ0hYeHKzY2VnfddZd+/vln57YNGzYoLCxMHTt2dLb16NFDfn5++uyzz5x9rrvuOgUEBDj7xMfHa8eOHfrll1+cfXr06OFy3vj4eG3YsKEiL+28yM3NlSTVrFlTkrRp0yYdP37c5XqbNWumBg0aOK93w4YNatWqlfPLi6XfxyMvL09ff/21s8+ZxuzYsWPatGmTSx8/Pz/16NHjohhXqeTYnvTmm2+qVq1aatmypSZOnKhff/3VuY2xPbOioiItWLBABQUFiouLY756yOnjehJz1X2jR49WQkJCietnzp6bssb1JOZs+X333XeqW7euLr/8cg0dOlSZmZmSmKueUNbYnsR89V3+3i6gIvTq1UsDBgxQdHS0du3apQcffFC9e/fWhg0bVKlSJWVnZys8PNxlH39/f9WsWVPZ2dmSpOzsbEVHR7v0OTlJs7OzVaNGDWVnZ7tM3JN9Th7jQlVcXKxx48bpmmuuUcuWLSX9fs0BAQEKCwtz6Xvq9ZY1Hie3nalPXl6efvvtN/3yyy8qKioqtc+3337rsWv0ltLGVpL+9Kc/qWHDhqpbt662bNmiBx54QDt27NDixYslMbZl2bp1q+Li4nT06FFVq1ZNS5YsUfPmzZWens58PQdljavEXD0XCxYs0JdffqkvvviixDb+jHXfmcZVYs6646qrrlJycrJiY2OVlZWlKVOm6Nprr9W2bduYq+foTGMbEhLCfPVxF2VwGjx4sPO/W7VqpdatW6tx48ZKS0tT9+7dvVjZhWH06NHatm2bPvnkE2+XctEpa2zvuOMO53+3atVKderUUffu3bVr1y41btz4fJd5wYiNjVV6erpyc3P1zjvvaPjw4fr444+9XdYFr6xxbd68OXPVTfv27dPYsWOVkpKioKAgb5dz0TibcWXOll/v3r2d/926dWtdddVVatiwod5++20FBwd7sbIL35nGduTIkcxXH3fRflTvVJdffrlq1aqljIwMSVJkZKQOHDjg0ufEiRM6dOiQIiMjnX1OXyHm5Htbn5PbL0RjxozR+++/r9TUVNWvX9/ZHhkZqWPHjunw4cMu/U+93nMZs9DQUAUHB6tWrVqqVKnSRTeuUtljW5qrrrpKklzmLGNbUkBAgJo0aaIOHTooKSlJbdq00cyZM5mv56iscS0Nc/XsbNq0SQcOHFD79u3l7+8vf39/ffzxx3r++efl7++viIgI5qwbbONaVFRUYh/mbPmFhYWpadOmysjI4M9XDzt1bEvDfPUtl0Rw2r9/v37++WfVqVNHkhQXF6fDhw9r06ZNzj5r1qxRcXGxc4LGxcVp7dq1On78uLNPSkqKYmNjVaNGDWef1atXu5wrJSXF5VmAC4UxRmPGjNGSJUu0Zs2aEh9T7NChgypXruxyvTt27FBmZqbzeuPi4rR161aXUJqSkqLQ0FDnx3xsYxYQEKAOHTq49CkuLtbq1asvyHGV7GNbmvT0dElymbOMrV1xcbEKCwuZrx52clxLw1w9O927d9fWrVuVnp7ufHXs2FFDhw51/jdztvxs41qpUqUS+zBnyy8/P1+7du1SnTp1+PPVw04d29IwX32Mt1encMeRI0fM5s2bzebNm40kM23aNLN582azd+9ec+TIEXPfffeZDRs2mN27d5uPPvrItG/f3sTExJijR486j9GrVy/Trl0789lnn5lPPvnExMTEuCxHfvjwYRMREWH+/Oc/m23btpkFCxaYKlWqlFiO3N/f3zz33HPmm2++MZMmTbpglyO/6667TPXq1U1aWprLEpi//vqrs8+dd95pGjRoYNasWWM2btxo4uLiTFxcnHP7ySUyb7zxRpOenm5WrlxpateuXeoSmffff7/55ptvzEsvvVTqEpmBgYEmOTnZbN++3dxxxx0mLCzMZQWZC4ltbDMyMsyjjz5qNm7caHbv3m2WLVtmLr/8cnPdddc5j8HYlvSPf/zDfPzxx2b37t1my5Yt5h//+IdxOBxm1apVxhjmq7vONK7MVc86ffUs5qxnnDquzFn3TJgwwaSlpZndu3ebdevWmR49ephatWqZAwcOGGOYq+fiTGPLfPV9F2RwSk1NNZJKvIYPH25+/fVXc+ONN5ratWubypUrm4YNG5pRo0aVmAg///yzGTJkiKlWrZoJDQ01iYmJ5siRIy59vvrqK9OlSxcTGBho6tWrZ5566qkStbz99tumadOmJiAgwLRo0cJ88MEHFXrtFaW08ZRk5s6d6+zz22+/mbvvvtvUqFHDVKlSxdx8880mKyvL5Th79uwxvXv3NsHBwaZWrVpmwoQJ5vjx4y59UlNTTdu2bU1AQIC5/PLLXc5x0gsvvGAaNGhgAgICzJVXXmk+/fTTirjs88I2tpmZmea6664zNWvWNIGBgaZJkybm/vvvd/neBmMY29P95S9/MQ0bNjQBAQGmdu3apnv37s7QZAzz1V1nGlfmqmedHpyYs55x6rgyZ90zaNAgU6dOHRMQEGDq1atnBg0aZDIyMpzbmavuO9PYMl99n8MYY873XS4AAAAAuJBcEs84AQAAAMC5IDgBAAAAgAXBCQAAAAAsCE4AAAAAYEFwAgAAAAALghMAAAAAWBCcAAAAAMCC4AQAAAAAFgQnAMBFacSIEerfv7/Hj5udna2ePXuqatWqCgsL8/jxAQC+ieAEAHBbRYWT8tizZ48cDofS09PPy/mmT5+urKwspaena+fOnaX2qahx8YXxBoBLlb+3CwAA4EKya9cudejQQTExMd4uBQBwHnHHCQBQYbZt26bevXurWrVqioiI0J///Gf99NNPzu3dunXTPffco7///e+qWbOmIiMjNXnyZJdjfPvtt+rSpYuCgoLUvHlzffTRR3I4HFq6dKkkKTo6WpLUrl07ORwOdevWzWX/5557TnXq1NFll12m0aNH6/jx42esedasWWrcuLECAgIUGxurN954w7mtUaNGevfdd/X666/L4XBoxIgRJfafPHmyXnvtNS1btkwOh0MOh0NpaWmSpH379mngwIEKCwtTzZo11a9fP+3Zs8d5nVWqVNH8+fOdx3r77bcVHBys7du3n/G4AICKR3ACAFSIw4cP64YbblC7du20ceNGrVy5Ujk5ORo4cKBLv9dee01Vq1bVZ599pmeeeUaPPvqoUlJSJElFRUXq37+/qlSpos8++0yvvPKKHnroIZf9P//8c0nSRx99pKysLC1evNi5LTU1Vbt27VJqaqpee+01JScnKzk5ucyalyxZorFjx2rChAnatm2b/vrXvyoxMVGpqamSpC+++EK9evXSwIEDlZWVpZkzZ5Y4xn333aeBAweqV69eysrKUlZWljp37qzjx48rPj5eISEh+u9//6t169apWrVq6tWrl44dO6ZmzZrpueee0913363MzEzt379fd955p55++mk1b968zOMCAM4PPqoHAKgQL774otq1a6cnn3zS2TZnzhxFRUVp586datq0qSSpdevWmjRpkiQpJiZGL774olavXq2ePXsqJSVFu3btUlpamiIjIyVJTzzxhHr27Ok8Zu3atSVJl112mbPPSTVq1NCLL76oSpUqqVmzZkpISNDq1as1atSoUmt+7rnnNGLECN19992SpPHjx+vTTz/Vc889p+uvv161a9dWYGCggoODS5zrpGrVqik4OFiFhYUufebNm6fi4mK9+uqrcjgckqS5c+cqLCxMaWlpuvHGG3X33Xdr+fLluu222xQQEKBOnTrpb3/72xmPCwA4PwhOAIAK8dVXXyk1NVXVqlUrsW3Xrl0uwelUderU0YEDByRJO3bsUFRUlEtQuPLKK8+6hhYtWqhSpUoux966dWuZ/b/55hvdcccdLm3XXHNNqXeWyuurr75SRkaGQkJCXNqPHj2qXbt2Od/PmTNHTZs2lZ+fn77++mtnyAIAeBfBCQBQIfLz89W3b189/fTTJbbVqVPH+d+VK1d22eZwOFRcXOyRGiry2OWVn5+vDh066M033yyx7eRdM+n3gFVQUCA/Pz9lZWW5jBUAwHsITgCACtG+fXu9++67atSokfz93fvrJjY2Vvv27VNOTo4iIiIk/f6c0akCAgIk/f481Lm64oortG7dOg0fPtzZtm7dOjVv3rxcxwkICChRT/v27bVw4UKFh4crNDS01P0OHTqkESNG6KGHHlJWVpaGDh2qL7/8UsHBwWUeFwBwfrA4BADgnOTm5io9Pd3ltW/fPo0ePVqHDh3SkCFD9MUXX2jXrl368MMPlZiYeNb/+O/Zs6caN26s4cOHa8uWLVq3bp0efvhhSXJ+hC08PFzBwcHOxSdyc3Pdvpb7779fycnJmjVrlr777jtNmzZNixcv1n333Veu4zRq1EhbtmzRjh079NNPP+n48eMaOnSoatWqpX79+um///2vdu/erbS0NN1zzz3av3+/JOnOO+9UVFSUHn74YU2bNk1FRUUu5y7tuACA84PgBAA4J2lpaWrXrp3La8qUKapbt67WrVunoqIi3XjjjWrVqpXGjRunsLAw+fmd3V8/lSpV0tKlS5Wfn69OnTrp9ttvd66qFxQUJEny9/fX888/r5dffll169ZVv3793L6W/v37a+bMmXruuefUokULvfzyy5o7d26JJc5tRo0apdjYWHXs2FG1a9fWunXrVKVKFa1du1YNGjTQgAEDdMUVV2jkyJE6evSoQkND9frrr2v58uV644035O/vr6pVq2revHn617/+pRUrVpR5XADA+eEwxhhvFwEAwNlat26dunTpooyMDDVu3Njb5QAALhEEJwCAT1uyZImqVaummJgYZWRkaOzYsapRo4Y++eQTb5cGALiEsDgEAMCnHTlyRA888IAyMzNVq1Yt9ejRQ1OnTvV2WQCASwx3nAAAAADAgsUhAAAAAMCC4AQAAAAAFgQnAAAAALAgOAEAAACABcEJAAAAACwITgAAAABgQXACAAAAAAuCEwAAAABY/D9zrQJTsh2cnQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(dataset: Dataset, tokenize: bool):\n",
    "\n",
    "    # 1. Apply chat template to each row\n",
    "    tokenized_result = []\n",
    "    for row in dataset:\n",
    "        # apply a chat template\n",
    "        formatted_text = base_tokenizer.apply_chat_template(\n",
    "            row[\"text\"], tokenize=tokenize, add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        tokenized_result.append(formatted_text)\n",
    "\n",
    "    lengths = [len(x) for x in tokenized_result]\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color=\"blue\")\n",
    "    plt.xlabel(\"Length of text\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Lengths of text\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_data_lengths(dataset, False)\n",
    "plot_data_lengths(dataset, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset to smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes:  31\n",
      "------------Episode 0------------\n",
      "Number of (user/assistant) messages: 940\n",
      "Split into number of chunks (of messages):  18\n",
      "------------Episode 1------------\n",
      "Number of (user/assistant) messages: 661\n",
      "Split into number of chunks (of messages):  16\n",
      "------------Episode 2------------\n",
      "Number of (user/assistant) messages: 373\n",
      "Split into number of chunks (of messages):  14\n",
      "------------Episode 3------------\n",
      "Number of (user/assistant) messages: 996\n",
      "Split into number of chunks (of messages):  28\n",
      "------------Episode 4------------\n",
      "Number of (user/assistant) messages: 707\n",
      "Split into number of chunks (of messages):  16\n",
      "------------Episode 5------------\n",
      "Number of (user/assistant) messages: 1154\n",
      "Split into number of chunks (of messages):  23\n",
      "------------Episode 6------------\n",
      "Number of (user/assistant) messages: 819\n",
      "Split into number of chunks (of messages):  21\n",
      "------------Episode 7------------\n",
      "Number of (user/assistant) messages: 880\n",
      "Split into number of chunks (of messages):  18\n",
      "------------Episode 8------------\n",
      "Number of (user/assistant) messages: 1955\n",
      "Split into number of chunks (of messages):  35\n",
      "------------Episode 9------------\n",
      "Number of (user/assistant) messages: 1852\n",
      "Split into number of chunks (of messages):  36\n",
      "------------Episode 10------------\n",
      "Number of (user/assistant) messages: 207\n",
      "Split into number of chunks (of messages):  11\n",
      "------------Episode 11------------\n",
      "Number of (user/assistant) messages: 1105\n",
      "Split into number of chunks (of messages):  32\n",
      "------------Episode 12------------\n",
      "Number of (user/assistant) messages: 445\n",
      "Split into number of chunks (of messages):  20\n",
      "------------Episode 13------------\n",
      "Number of (user/assistant) messages: 965\n",
      "Split into number of chunks (of messages):  23\n",
      "------------Episode 14------------\n",
      "Number of (user/assistant) messages: 636\n",
      "Split into number of chunks (of messages):  17\n",
      "------------Episode 15------------\n",
      "Number of (user/assistant) messages: 1742\n",
      "Split into number of chunks (of messages):  28\n",
      "------------Episode 16------------\n",
      "Number of (user/assistant) messages: 346\n",
      "Split into number of chunks (of messages):  24\n",
      "------------Episode 17------------\n",
      "Number of (user/assistant) messages: 862\n",
      "Split into number of chunks (of messages):  23\n",
      "------------Episode 18------------\n",
      "Number of (user/assistant) messages: 249\n",
      "Split into number of chunks (of messages):  12\n",
      "------------Episode 19------------\n",
      "Number of (user/assistant) messages: 844\n",
      "Split into number of chunks (of messages):  30\n",
      "------------Episode 20------------\n",
      "Number of (user/assistant) messages: 633\n",
      "Split into number of chunks (of messages):  19\n",
      "------------Episode 21------------\n",
      "Number of (user/assistant) messages: 921\n",
      "Split into number of chunks (of messages):  19\n",
      "------------Episode 22------------\n",
      "Number of (user/assistant) messages: 910\n",
      "Split into number of chunks (of messages):  29\n",
      "------------Episode 23------------\n",
      "Number of (user/assistant) messages: 320\n",
      "Split into number of chunks (of messages):  20\n",
      "------------Episode 24------------\n",
      "Number of (user/assistant) messages: 1086\n",
      "Split into number of chunks (of messages):  24\n",
      "------------Episode 25------------\n",
      "Number of (user/assistant) messages: 299\n",
      "Split into number of chunks (of messages):  11\n",
      "------------Episode 26------------\n",
      "Number of (user/assistant) messages: 610\n",
      "Split into number of chunks (of messages):  29\n",
      "------------Episode 27------------\n",
      "Number of (user/assistant) messages: 362\n",
      "Split into number of chunks (of messages):  21\n",
      "------------Episode 28------------\n",
      "Number of (user/assistant) messages: 662\n",
      "Split into number of chunks (of messages):  17\n",
      "------------Episode 29------------\n",
      "Number of (user/assistant) messages: 540\n",
      "Split into number of chunks (of messages):  22\n",
      "------------Episode 30------------\n",
      "Number of (user/assistant) messages: 113\n",
      "Split into number of chunks (of messages):  9\n",
      "Result chunks of episodes length:\n",
      "665\n",
      "--------------Chunk 0-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 1-----------\n",
      "Number of (user/assistant) messages:  78\n",
      "--------------Chunk 2-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 3-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 4-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 5-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 6-----------\n",
      "Number of (user/assistant) messages:  72\n",
      "--------------Chunk 7-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 8-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 9-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 10-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 11-----------\n",
      "Number of (user/assistant) messages:  62\n",
      "--------------Chunk 12-----------\n",
      "Number of (user/assistant) messages:  76\n",
      "--------------Chunk 13-----------\n",
      "Number of (user/assistant) messages:  64\n",
      "--------------Chunk 14-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 15-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 16-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 17-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 18-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 19-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 20-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 21-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 22-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 23-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 24-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 25-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 26-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 27-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 28-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 29-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 30-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 31-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 32-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 33-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 34-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 35-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 36-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 37-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 38-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 39-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 40-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 41-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 42-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 43-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 44-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 45-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 46-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 47-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 48-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 49-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 50-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 51-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 52-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 53-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 54-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 55-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 56-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 57-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 58-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 59-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 60-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 61-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 62-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 63-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 64-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 65-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 66-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 67-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 68-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 69-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 70-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 71-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 72-----------\n",
      "Number of (user/assistant) messages:  64\n",
      "--------------Chunk 73-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 74-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 75-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 76-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 77-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 78-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 79-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 80-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 81-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 82-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 83-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 84-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 85-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 86-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 87-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 88-----------\n",
      "Number of (user/assistant) messages:  66\n",
      "--------------Chunk 89-----------\n",
      "Number of (user/assistant) messages:  82\n",
      "--------------Chunk 90-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 91-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 92-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 93-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 94-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 95-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 96-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 97-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 98-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 99-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 100-----------\n",
      "Number of (user/assistant) messages:  74\n",
      "--------------Chunk 101-----------\n",
      "Number of (user/assistant) messages:  72\n",
      "--------------Chunk 102-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 103-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 104-----------\n",
      "Number of (user/assistant) messages:  68\n",
      "--------------Chunk 105-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 106-----------\n",
      "Number of (user/assistant) messages:  62\n",
      "--------------Chunk 107-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 108-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 109-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 110-----------\n",
      "Number of (user/assistant) messages:  74\n",
      "--------------Chunk 111-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 112-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 113-----------\n",
      "Number of (user/assistant) messages:  68\n",
      "--------------Chunk 114-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 115-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 116-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 117-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 118-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 119-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 120-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 121-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 122-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 123-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 124-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 125-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 126-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 127-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 128-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 129-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 130-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 131-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 132-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 133-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 134-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 135-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 136-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 137-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 138-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 139-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 140-----------\n",
      "Number of (user/assistant) messages:  78\n",
      "--------------Chunk 141-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 142-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 143-----------\n",
      "Number of (user/assistant) messages:  66\n",
      "--------------Chunk 144-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 145-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 146-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 147-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 148-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 149-----------\n",
      "Number of (user/assistant) messages:  62\n",
      "--------------Chunk 150-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 151-----------\n",
      "Number of (user/assistant) messages:  94\n",
      "--------------Chunk 152-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 153-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 154-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 155-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 156-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 157-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 158-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 159-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 160-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 161-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 162-----------\n",
      "Number of (user/assistant) messages:  80\n",
      "--------------Chunk 163-----------\n",
      "Number of (user/assistant) messages:  68\n",
      "--------------Chunk 164-----------\n",
      "Number of (user/assistant) messages:  68\n",
      "--------------Chunk 165-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 166-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 167-----------\n",
      "Number of (user/assistant) messages:  70\n",
      "--------------Chunk 168-----------\n",
      "Number of (user/assistant) messages:  68\n",
      "--------------Chunk 169-----------\n",
      "Number of (user/assistant) messages:  66\n",
      "--------------Chunk 170-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 171-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 172-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 173-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 174-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 175-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 176-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 177-----------\n",
      "Number of (user/assistant) messages:  74\n",
      "--------------Chunk 178-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 179-----------\n",
      "Number of (user/assistant) messages:  64\n",
      "--------------Chunk 180-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 181-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 182-----------\n",
      "Number of (user/assistant) messages:  66\n",
      "--------------Chunk 183-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 184-----------\n",
      "Number of (user/assistant) messages:  70\n",
      "--------------Chunk 185-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 186-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 187-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 188-----------\n",
      "Number of (user/assistant) messages:  70\n",
      "--------------Chunk 189-----------\n",
      "Number of (user/assistant) messages:  62\n",
      "--------------Chunk 190-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 191-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 192-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 193-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 194-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 195-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 196-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 197-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 198-----------\n",
      "Number of (user/assistant) messages:  72\n",
      "--------------Chunk 199-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 200-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 201-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 202-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 203-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 204-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 205-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 206-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 207-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 208-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 209-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 210-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 211-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 212-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 213-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 214-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 215-----------\n",
      "Number of (user/assistant) messages:  66\n",
      "--------------Chunk 216-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 217-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 218-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 219-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 220-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 221-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 222-----------\n",
      "Number of (user/assistant) messages:  70\n",
      "--------------Chunk 223-----------\n",
      "Number of (user/assistant) messages:  80\n",
      "--------------Chunk 224-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 225-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 226-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 227-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 228-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 229-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 230-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 231-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 232-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 233-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 234-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 235-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 236-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 237-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 238-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 239-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 240-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 241-----------\n",
      "Number of (user/assistant) messages:  68\n",
      "--------------Chunk 242-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 243-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 244-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 245-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 246-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 247-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 248-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 249-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 250-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 251-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 252-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 253-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 254-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 255-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 256-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 257-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 258-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 259-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 260-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 261-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 262-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 263-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 264-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 265-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 266-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 267-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 268-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 269-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 270-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 271-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 272-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 273-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 274-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 275-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 276-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 277-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 278-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 279-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 280-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 281-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 282-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 283-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 284-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 285-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 286-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 287-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 288-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 289-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 290-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 291-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 292-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 293-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 294-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 295-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 296-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 297-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 298-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 299-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 300-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 301-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 302-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 303-----------\n",
      "Number of (user/assistant) messages:  112\n",
      "--------------Chunk 304-----------\n",
      "Number of (user/assistant) messages:  88\n",
      "--------------Chunk 305-----------\n",
      "Number of (user/assistant) messages:  64\n",
      "--------------Chunk 306-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 307-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 308-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 309-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 310-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 311-----------\n",
      "Number of (user/assistant) messages:  66\n",
      "--------------Chunk 312-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 313-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 314-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 315-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 316-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 317-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 318-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 319-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 320-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 321-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 322-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 323-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 324-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 325-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 326-----------\n",
      "Number of (user/assistant) messages:  62\n",
      "--------------Chunk 327-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 328-----------\n",
      "Number of (user/assistant) messages:  110\n",
      "--------------Chunk 329-----------\n",
      "Number of (user/assistant) messages:  80\n",
      "--------------Chunk 330-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 331-----------\n",
      "Number of (user/assistant) messages:  70\n",
      "--------------Chunk 332-----------\n",
      "Number of (user/assistant) messages:  66\n",
      "--------------Chunk 333-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 334-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 335-----------\n",
      "Number of (user/assistant) messages:  64\n",
      "--------------Chunk 336-----------\n",
      "Number of (user/assistant) messages:  74\n",
      "--------------Chunk 337-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 338-----------\n",
      "Number of (user/assistant) messages:  70\n",
      "--------------Chunk 339-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 340-----------\n",
      "Number of (user/assistant) messages:  78\n",
      "--------------Chunk 341-----------\n",
      "Number of (user/assistant) messages:  66\n",
      "--------------Chunk 342-----------\n",
      "Number of (user/assistant) messages:  68\n",
      "--------------Chunk 343-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 344-----------\n",
      "Number of (user/assistant) messages:  64\n",
      "--------------Chunk 345-----------\n",
      "Number of (user/assistant) messages:  62\n",
      "--------------Chunk 346-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 347-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 348-----------\n",
      "Number of (user/assistant) messages:  78\n",
      "--------------Chunk 349-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 350-----------\n",
      "Number of (user/assistant) messages:  76\n",
      "--------------Chunk 351-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 352-----------\n",
      "Number of (user/assistant) messages:  64\n",
      "--------------Chunk 353-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 354-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 355-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 356-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 357-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 358-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 359-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 360-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 361-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 362-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 363-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 364-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 365-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 366-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 367-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 368-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 369-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 370-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 371-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 372-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 373-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 374-----------\n",
      "Number of (user/assistant) messages:  6\n",
      "--------------Chunk 375-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 376-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 377-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 378-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 379-----------\n",
      "Number of (user/assistant) messages:  6\n",
      "--------------Chunk 380-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 381-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 382-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 383-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 384-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 385-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 386-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 387-----------\n",
      "Number of (user/assistant) messages:  62\n",
      "--------------Chunk 388-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 389-----------\n",
      "Number of (user/assistant) messages:  62\n",
      "--------------Chunk 390-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 391-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 392-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 393-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 394-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 395-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 396-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 397-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 398-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 399-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 400-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 401-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 402-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 403-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 404-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 405-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 406-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 407-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 408-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 409-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 410-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 411-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 412-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 413-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 414-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 415-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 416-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 417-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 418-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 419-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 420-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 421-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 422-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 423-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 424-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 425-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 426-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 427-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 428-----------\n",
      "Number of (user/assistant) messages:  6\n",
      "--------------Chunk 429-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 430-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 431-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 432-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 433-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 434-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 435-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 436-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 437-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 438-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 439-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 440-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 441-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 442-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 443-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 444-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 445-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 446-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 447-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 448-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 449-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 450-----------\n",
      "Number of (user/assistant) messages:  60\n",
      "--------------Chunk 451-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 452-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 453-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 454-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 455-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 456-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 457-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 458-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 459-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 460-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 461-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 462-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 463-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 464-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 465-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 466-----------\n",
      "Number of (user/assistant) messages:  68\n",
      "--------------Chunk 467-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 468-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 469-----------\n",
      "Number of (user/assistant) messages:  64\n",
      "--------------Chunk 470-----------\n",
      "Number of (user/assistant) messages:  70\n",
      "--------------Chunk 471-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 472-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 473-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 474-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 475-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 476-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 477-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 478-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 479-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 480-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 481-----------\n",
      "Number of (user/assistant) messages:  78\n",
      "--------------Chunk 482-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 483-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 484-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 485-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 486-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 487-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 488-----------\n",
      "Number of (user/assistant) messages:  74\n",
      "--------------Chunk 489-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 490-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 491-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 492-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 493-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 494-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 495-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 496-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 497-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 498-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 499-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 500-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 501-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 502-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 503-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 504-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 505-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 506-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 507-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 508-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 509-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 510-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 511-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 512-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 513-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 514-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 515-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 516-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 517-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 518-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 519-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 520-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 521-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 522-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 523-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 524-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 525-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 526-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 527-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 528-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 529-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 530-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 531-----------\n",
      "Number of (user/assistant) messages:  4\n",
      "--------------Chunk 532-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 533-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 534-----------\n",
      "Number of (user/assistant) messages:  56\n",
      "--------------Chunk 535-----------\n",
      "Number of (user/assistant) messages:  72\n",
      "--------------Chunk 536-----------\n",
      "Number of (user/assistant) messages:  52\n",
      "--------------Chunk 537-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 538-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 539-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 540-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 541-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 542-----------\n",
      "Number of (user/assistant) messages:  76\n",
      "--------------Chunk 543-----------\n",
      "Number of (user/assistant) messages:  58\n",
      "--------------Chunk 544-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 545-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 546-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 547-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 548-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 549-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 550-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 551-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 552-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 553-----------\n",
      "Number of (user/assistant) messages:  50\n",
      "--------------Chunk 554-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 555-----------\n",
      "Number of (user/assistant) messages:  6\n",
      "--------------Chunk 556-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 557-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 558-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 559-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 560-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 561-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 562-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 563-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 564-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 565-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 566-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 567-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 568-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 569-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 570-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 571-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 572-----------\n",
      "Number of (user/assistant) messages:  2\n",
      "--------------Chunk 573-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 574-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 575-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 576-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 577-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 578-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 579-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 580-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 581-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 582-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 583-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 584-----------\n",
      "Number of (user/assistant) messages:  16\n",
      "--------------Chunk 585-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 586-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 587-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 588-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 589-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 590-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 591-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 592-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 593-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 594-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 595-----------\n",
      "Number of (user/assistant) messages:  36\n",
      "--------------Chunk 596-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 597-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 598-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 599-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 600-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 601-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 602-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 603-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 604-----------\n",
      "Number of (user/assistant) messages:  6\n",
      "--------------Chunk 605-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 606-----------\n",
      "Number of (user/assistant) messages:  20\n",
      "--------------Chunk 607-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 608-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 609-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 610-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 611-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 612-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 613-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 614-----------\n",
      "Number of (user/assistant) messages:  6\n",
      "--------------Chunk 615-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 616-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 617-----------\n",
      "Number of (user/assistant) messages:  54\n",
      "--------------Chunk 618-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 619-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 620-----------\n",
      "Number of (user/assistant) messages:  40\n",
      "--------------Chunk 621-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 622-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 623-----------\n",
      "Number of (user/assistant) messages:  44\n",
      "--------------Chunk 624-----------\n",
      "Number of (user/assistant) messages:  42\n",
      "--------------Chunk 625-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 626-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 627-----------\n",
      "Number of (user/assistant) messages:  46\n",
      "--------------Chunk 628-----------\n",
      "Number of (user/assistant) messages:  48\n",
      "--------------Chunk 629-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 630-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 631-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 632-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 633-----------\n",
      "Number of (user/assistant) messages:  30\n",
      "--------------Chunk 634-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 635-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 636-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 637-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 638-----------\n",
      "Number of (user/assistant) messages:  34\n",
      "--------------Chunk 639-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 640-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 641-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 642-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 643-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 644-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 645-----------\n",
      "Number of (user/assistant) messages:  24\n",
      "--------------Chunk 646-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 647-----------\n",
      "Number of (user/assistant) messages:  28\n",
      "--------------Chunk 648-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 649-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 650-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "--------------Chunk 651-----------\n",
      "Number of (user/assistant) messages:  18\n",
      "--------------Chunk 652-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 653-----------\n",
      "Number of (user/assistant) messages:  38\n",
      "--------------Chunk 654-----------\n",
      "Number of (user/assistant) messages:  32\n",
      "--------------Chunk 655-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 656-----------\n",
      "Number of (user/assistant) messages:  14\n",
      "--------------Chunk 657-----------\n",
      "Number of (user/assistant) messages:  8\n",
      "--------------Chunk 658-----------\n",
      "Number of (user/assistant) messages:  4\n",
      "--------------Chunk 659-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 660-----------\n",
      "Number of (user/assistant) messages:  12\n",
      "--------------Chunk 661-----------\n",
      "Number of (user/assistant) messages:  26\n",
      "--------------Chunk 662-----------\n",
      "Number of (user/assistant) messages:  10\n",
      "--------------Chunk 663-----------\n",
      "Number of (user/assistant) messages:  4\n",
      "--------------Chunk 664-----------\n",
      "Number of (user/assistant) messages:  22\n",
      "60\n",
      "[{'content': \"Guys, the rumors are true. Bert is undergoing a double kidney transplant and we are sending him our best. In lieu of flowers, please check out his new episode of something's burning on the Bert Kreischer YouTube page. Thanks for sitting in, Joe.\", 'role': 'user'}, {'content': 'My pleasure.', 'role': 'assistant'}, {'content': 'I hope he does well. I hope he recovers.', 'role': 'user'}, {'content': \"He'll be fine. Yeah, they can do that now. They're good at it.\", 'role': 'assistant'}, {'content': 'They are good at it.', 'role': 'user'}, {'content': 'Yeah, they fix everything.', 'role': 'assistant'}, {'content': 'I think they found a young brazilian girl and they took her stuff and now.', 'role': 'user'}, {'content': 'Brazilian.', 'role': 'assistant'}, {'content': \"Yeah. And now they're going to give it to Bert. So same blood type. That's all that matters. Good luck, Bert. You got here. You checked out the new studio.\", 'role': 'user'}, {'content': \"It's awesome.\", 'role': 'assistant'}, {'content': 'Thank you.', 'role': 'user'}, {'content': 'Very cool.', 'role': 'assistant'}, {'content': \"And then one of the staff, as you were pulling up, said, I can't lose in arm wrestling. And I was like, oh, that's cool. And he goes, I want to arm wrestle him. And I was like, yeah, you should. And I go, you're going to lose. And he goes, I can't lose. And I go, you want to bet? I'll bet you. And he goes, how much? I go, whatever you want. And he goes, how much? I go, how about a grand? He was like, all right, I'll take that act.\", 'role': 'user'}, {'content': 'Why is he so confident?', 'role': 'assistant'}, {'content': \"It's part of his whole Persona. And he is like. He has looked at me before and goes, if somebody challenges me to something, I cannot lose. I cannot lose.\", 'role': 'user'}, {'content': \"And I said, okay, that's a crazy, delusional form of confidence.\", 'role': 'assistant'}, {'content': 'He is so delusional.', 'role': 'user'}, {'content': \"That wasn't even a little difficult. Wait, say that again. It wasn't even a little difficult. I'll let him try for a while.\", 'role': 'assistant'}, {'content': 'He needs to be humbled.', 'role': 'user'}, {'content': 'I gave him a few seconds.', 'role': 'assistant'}, {'content': 'Yeah.', 'role': 'user'}, {'content': 'I was like, come on.', 'role': 'assistant'}, {'content': 'He wants to arm wrestle Bruce too.', 'role': 'user'}, {'content': \"Oh, that's hilarious. Bruce is a bear. I know. A literal bear.\", 'role': 'assistant'}, {'content': \"Yeah, you could try him next if he's good. I'm good. I'm good. Just try, man. I'm good.\", 'role': 'user'}, {'content': 'Thanks.', 'role': 'assistant'}, {'content': \"Isn't it a good way, though? Don't you think it's good to have for you to come in here and kind of begin the day? It's not the beginning of the day. But have a win at the beginning of part of the day.\", 'role': 'user'}, {'content': 'I feel good that you won $1,000.', 'role': 'assistant'}, {'content': 'Yeah. But I mean that you won that. Somebody goes, I challenge you to something. Like, to have a win. Like, those wins kind of how.', 'role': 'user'}, {'content': \"And it's like, yeah, but it was too easy. I was going to win.\", 'role': 'assistant'}, {'content': 'Yeah.', 'role': 'user'}, {'content': \"It. I haven't arm wrestled anybody in, like a fucking decade.\", 'role': 'assistant'}, {'content': \"Yeah, you're pretty good.\", 'role': 'user'}, {'content': \"Yeah, I'm not, though.\", 'role': 'assistant'}, {'content': 'But you just knew.', 'role': 'user'}, {'content': \"Yeah. What are the odds? I don't know what the ODs are.\", 'role': 'assistant'}, {'content': 'But, no, I had full confidence. I had full confidence.', 'role': 'user'}, {'content': 'Thank you.', 'role': 'assistant'}, {'content': 'Yeah. Oh, my God. It was so necessary. Now we can always cite this. When he goes, it is impossible for me to lose.', 'role': 'user'}, {'content': 'Some people have crazy confidence. Yeah, very true. That gets them killed.', 'role': 'assistant'}, {'content': \"It's funny because we talk about it sometimes how our good budy, who usually sits where you sit, is a crazy confident guy, too.\", 'role': 'user'}, {'content': 'Yeah. I always wonder how much of what.', 'role': 'assistant'}, {'content': 'Bert does is performance. Yeah.', 'role': 'user'}, {'content': \"It's hard to tell.\", 'role': 'assistant'}, {'content': \"It is. I do remember sitting on your set one time and he was like, I'm going to run the La marathon. And we were all like, shut up. No, you're not.\", 'role': 'user'}, {'content': 'He had no training.', 'role': 'assistant'}, {'content': \"And everybody who runs marathons was like, you don't understand how to prepare for a marathon. You don't just go. You take your time. It's weeks, and you work up in mileage, then you kind of work down, and then you do like a half. You do like ten or 12 miles, like the week before. Then they have the whole strategy. And he was like, I'm just going to do it. And we were like, you're dumb. And then he went and he did it.\", 'role': 'user'}, {'content': \"Well, he does. I mean, he regularly, back then at least, was running like a couple of miles every now. And he's just. Burt's unusual. You could count him out and it would be a mistake.\", 'role': 'assistant'}, {'content': 'I agree.', 'role': 'user'}, {'content': \"Yeah. He's like, when you and him played tennis. That makes sense to me. That's who he is.\", 'role': 'assistant'}, {'content': \"He's a weird guy, but he has this very bizarre skill set for, like. So you do archery regularly? Yeah. And then there's people who regularly do it. And you guys have a certain comfort with the equipment and how. But if you were to grab all your friends who don't shoot regularly, like, they don't practice regularly, and you go, try to do this, I'm 100% certain that of that group, he would do the best of people that don't practice. If you were just like, this is how you shoot it, because he has really impressive, I'm telling you, hand eye coordination. He has really good. Like anything like shooting, throwing a dart, throwing a ball, hitting a baseball. All that stuff is what he actually excels in.\", 'role': 'user'}, {'content': \"I've seen him play pool. He can play pool.\", 'role': 'assistant'}, {'content': \"I didn't know that.\", 'role': 'user'}, {'content': 'Yeah. Not bad.', 'role': 'assistant'}, {'content': \"Yeah. And by the way, he doesn't shoot pool regularly. Yeah.\", 'role': 'user'}, {'content': \"No. You could tell. Yeah. If he wasn't such a drunk, he'd probably be an amazing athlete. Right? Don't you think? Yeah.\", 'role': 'assistant'}, {'content': \"Yes. Which is why this kind of brings me to. We skipped it last year. I don't know if we skipped it the year before, but we've been talking about doing sober October again, and I like the idea that you mentioned. I think we should talk about it briefly, which is obviously, we do sober for the month.\", 'role': 'user'}, {'content': 'Right.', 'role': 'assistant'}, {'content': 'Which is two birds benefit, and then it can go crazy if we go challenge style. Right. You particularly go into your dark places in your head if we make competition.', 'role': 'user'}, {'content': 'Well, we did that that one year. It went so crazy.', 'role': 'assistant'}]\n",
      "----- Chunk 0 -----\n",
      "[\n",
      "    {\n",
      "        \"content\": \"Guys, the rumors are true. Bert is undergoing a double kidney transplant and we are sending him our best. In lieu of flowers, please check out his new episode of something's burning on the Bert Kreischer YouTube page. Thanks for sitting in, Joe.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"My pleasure.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"I hope he does well. I hope he recovers.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"He'll be fine. Yeah, they can do that now. They're good at it.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"They are good at it.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah, they fix everything.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"I think they found a young brazilian girl and they took her stuff and now.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Brazilian.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah. And now they're going to give it to Bert. So same blood type. That's all that matters. Good luck, Bert. You got here. You checked out the new studio.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"It's awesome.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Thank you.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Very cool.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"And then one of the staff, as you were pulling up, said, I can't lose in arm wrestling. And I was like, oh, that's cool. And he goes, I want to arm wrestle him. And I was like, yeah, you should. And I go, you're going to lose. And he goes, I can't lose. And I go, you want to bet? I'll bet you. And he goes, how much? I go, whatever you want. And he goes, how much? I go, how about a grand? He was like, all right, I'll take that act.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Why is he so confident?\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"It's part of his whole Persona. And he is like. He has looked at me before and goes, if somebody challenges me to something, I cannot lose. I cannot lose.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"And I said, okay, that's a crazy, delusional form of confidence.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"He is so delusional.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"That wasn't even a little difficult. Wait, say that again. It wasn't even a little difficult. I'll let him try for a while.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"He needs to be humbled.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"I gave him a few seconds.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"I was like, come on.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"He wants to arm wrestle Bruce too.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Oh, that's hilarious. Bruce is a bear. I know. A literal bear.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah, you could try him next if he's good. I'm good. I'm good. Just try, man. I'm good.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Thanks.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Isn't it a good way, though? Don't you think it's good to have for you to come in here and kind of begin the day? It's not the beginning of the day. But have a win at the beginning of part of the day.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"I feel good that you won $1,000.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah. But I mean that you won that. Somebody goes, I challenge you to something. Like, to have a win. Like, those wins kind of how.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"And it's like, yeah, but it was too easy. I was going to win.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"It. I haven't arm wrestled anybody in, like a fucking decade.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah, you're pretty good.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah, I'm not, though.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"But you just knew.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah. What are the odds? I don't know what the ODs are.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"But, no, I had full confidence. I had full confidence.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Thank you.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah. Oh, my God. It was so necessary. Now we can always cite this. When he goes, it is impossible for me to lose.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Some people have crazy confidence. Yeah, very true. That gets them killed.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"It's funny because we talk about it sometimes how our good budy, who usually sits where you sit, is a crazy confident guy, too.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah. I always wonder how much of what.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Bert does is performance. Yeah.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"It's hard to tell.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"It is. I do remember sitting on your set one time and he was like, I'm going to run the La marathon. And we were all like, shut up. No, you're not.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"He had no training.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"And everybody who runs marathons was like, you don't understand how to prepare for a marathon. You don't just go. You take your time. It's weeks, and you work up in mileage, then you kind of work down, and then you do like a half. You do like ten or 12 miles, like the week before. Then they have the whole strategy. And he was like, I'm just going to do it. And we were like, you're dumb. And then he went and he did it.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Well, he does. I mean, he regularly, back then at least, was running like a couple of miles every now. And he's just. Burt's unusual. You could count him out and it would be a mistake.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"I agree.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah. He's like, when you and him played tennis. That makes sense to me. That's who he is.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"He's a weird guy, but he has this very bizarre skill set for, like. So you do archery regularly? Yeah. And then there's people who regularly do it. And you guys have a certain comfort with the equipment and how. But if you were to grab all your friends who don't shoot regularly, like, they don't practice regularly, and you go, try to do this, I'm 100% certain that of that group, he would do the best of people that don't practice. If you were just like, this is how you shoot it, because he has really impressive, I'm telling you, hand eye coordination. He has really good. Like anything like shooting, throwing a dart, throwing a ball, hitting a baseball. All that stuff is what he actually excels in.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"I've seen him play pool. He can play pool.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"I didn't know that.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah. Not bad.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yeah. And by the way, he doesn't shoot pool regularly. Yeah.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"No. You could tell. Yeah. If he wasn't such a drunk, he'd probably be an amazing athlete. Right? Don't you think? Yeah.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Yes. Which is why this kind of brings me to. We skipped it last year. I don't know if we skipped it the year before, but we've been talking about doing sober October again, and I like the idea that you mentioned. I think we should talk about it briefly, which is obviously, we do sober for the month.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Right.\",\n",
      "        \"role\": \"assistant\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Which is two birds benefit, and then it can go crazy if we go challenge style. Right. You particularly go into your dark places in your head if we make competition.\",\n",
      "        \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "        \"content\": \"Well, we did that that one year. It went so crazy.\",\n",
      "        \"role\": \"assistant\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def split_episodes(data, max_words=2048):\n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    # Function to count words in a sentence\n",
    "    def count_words(sentence):\n",
    "        return len(sentence.split())\n",
    "\n",
    "    # Iterate over all entries in the data\n",
    "    for entry in data:\n",
    "        role = entry['role']\n",
    "        content = entry['content']\n",
    "        word_count = count_words(content)\n",
    "\n",
    "        # Check if adding this entry would exceed the max word count\n",
    "        if current_word_count + word_count > max_words:\n",
    "            # Ensure last role is 'assistant' before cutting off\n",
    "            if current_chunk and current_chunk[-1]['role'] == 'assistant':\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = []\n",
    "                current_word_count = 0\n",
    "            else:\n",
    "                # Add to current chunk until 'assistant' entry is found\n",
    "                continue\n",
    "\n",
    "        # Add current entry to the chunk\n",
    "        current_chunk.append(entry)\n",
    "        current_word_count += word_count\n",
    "\n",
    "        # Ensure the last entry in a complete chunk is from 'assistant'\n",
    "        if role == 'assistant' and current_word_count > max_words / 2:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "\n",
    "    # Add the last chunk if it ends correctly\n",
    "    if current_chunk and current_chunk[-1]['role'] == 'assistant':\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    if len(chunks) == 0:\n",
    "        print(\"EMPTY CHUNKS\")\n",
    "        print(len(data))\n",
    "        print(data)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "result = []\n",
    "print(\"Number of episodes: \", len(dataset))\n",
    "for i in range(len(dataset)): \n",
    "    row = dataset[i]\n",
    "    print(f\"------------Episode {i}------------\")\n",
    "    # print(row)\n",
    "    print(\"Number of (user/assistant) messages:\", len(row[\"text\"]))\n",
    "    # print(row[\"text\"])\n",
    "    text = row[\"text\"]\n",
    "    all_chunks = split_episodes(text)\n",
    "    print(\"Split into number of chunks (of messages): \", len(all_chunks))\n",
    "    # print(all_chunks)\n",
    "    result.extend(all_chunks)\n",
    "\n",
    "\n",
    "print(\"Result chunks of episodes length:\")\n",
    "print(len(result))\n",
    "for i in range(len(result)):\n",
    "    print(f\"--------------Chunk {i}-----------\")\n",
    "    print(\"Number of (user/assistant) messages: \", len(result[i]))\n",
    "\n",
    "\n",
    "print(len(result[0]))    \n",
    "print(result[0])  \n",
    "print(\"----- Chunk 0 -----\")  \n",
    "print(json.dumps(result[0], indent=4))\n",
    "\n",
    "\n",
    "newChunksDataset = Dataset.from_dict({\"text\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_data_lengths(dataset: Dataset, tokenize: bool):\n",
    "\n",
    "    # 1. Apply chat template to each row\n",
    "    tokenized_result = []\n",
    "    for row in dataset:\n",
    "        # apply a chat template\n",
    "        formatted_text = base_tokenizer.apply_chat_template(\n",
    "            row[\"text\"], tokenize=tokenize, add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        tokenized_result.append(formatted_text)\n",
    "\n",
    "    lengths = [len(x) for x in tokenized_result]\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color=\"blue\")\n",
    "    plt.xlabel(\"Length of text\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Lengths of text\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_data_lengths(newChunksDataset, False)\n",
    "plot_data_lengths(newChunksDataset, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 665\n",
      "attention_mask 665\n",
      "labels 665\n",
      "[32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 1, 3, 1763, 1422, 18713, 1056, 9828, 29491, 2305, 2114, 29510, 29481, 1227, 1279, 1137, 29491, 1860, 1146, 1171, 29491, 2370, 1452, 2313, 1137, 1117, 1032, 8803, 1330, 1032, 4406, 1137, 1504, 1117, 1476, 2880, 1070, 14610, 29572, 2155, 29510, 29481, 1544, 29493, 1279, 1136, 1279, 1146, 1210, 1227, 29572, 4, 10322, 29491, 2, 3, 1783, 1535, 1136, 9518, 1171, 5125, 1246, 8424, 1420, 1885, 2650, 1844, 1072, 13566, 1427, 1066, 1279, 1032, 29473, 29550, 29502, 29502, 3068, 12835, 1032, 2138, 26807, 29491, 4, 9120, 29491, 4971, 2138, 29493, 7439, 2970, 1032, 2587, 29491, 2, 3, 4771, 29510, 29481, 1354, 3764, 29491, 3937, 29510, 29481, 2083, 1136, 1344, 1065, 1040, 4738, 1072, 1136, 1393, 1509, 8586, 1065, 29493, 1072, 1146, 6466, 29481, 29491, 3937, 29510, 29481, 1544, 2083, 29473, 29518, 29502, 29502, 25104, 1279, 29491, 1763, 2307, 29510, 29475, 1274, 1066, 1544, 1279, 29473, 29538, 29502, 29502, 1448, 29491, 20579, 29491, 2305, 1146, 29510, 29481, 1544, 29473, 29550, 29502, 29502, 29491, 4, 29473, 29550, 29502, 29502, 1065, 1032, 2138, 29491, 20579, 29491, 1083, 2369, 1505, 1137, 29510, 29481, 5736, 29491, 2, 3, 1083, 1841, 1146, 29510, 29481, 5736, 29491, 4, 3048, 2136, 10048, 29493, 1864, 1083, 1641, 1136, 1505, 1066, 1279, 1757, 1539, 9276, 1032, 2138, 29491, 2305, 1281, 1136, 1631, 29493, 1505, 29493, 1342, 23621, 1072, 1636, 1136, 1631, 1342, 4916, 1459, 29493, 1136, 1393, 1146, 1065, 29491, 2, 3, 1763, 1309, 1393, 1146, 1065, 1040, 2738, 29491, 4, 1183, 23842, 1117, 29493, 1505, 7439, 2970, 1032, 2587, 29491, 1429, 29510, 29481, 1505, 1507, 1246, 1631, 1040, 4064, 22383, 8803, 29493, 1246, 1321, 1066, 1279, 29473, 29508, 29550, 1065, 1040, 2870, 29493, 1458, 29491, 24879, 12069, 29491, 2, 3, 2066, 2180, 1290, 29491, 4, 1860, 5851, 1040, 1716, 29493, 1083, 1841, 1083, 1057, 14042, 29493, 1505, 29493, 10310, 29491, 1783, 1347, 1083, 1321, 1066, 1279, 29493, 1505, 29493, 10310, 2970, 1065, 1032, 5918, 29491, 1783, 1792, 1032, 2080, 29493, 1136, 1544, 1393, 1065, 1224, 10048, 6745, 1519, 1738, 1136, 3547, 2880, 1070, 1505, 1040, 2407, 1137, 1146, 29510, 29481, 26794, 29491, 1763, 4226, 1040, 2407, 1137, 1136, 2201, 1066, 1279, 1146, 29491, 2, 3, 1083, 1171, 4963, 2200, 2893, 1452, 1224, 29473, 29550, 29502, 29502, 3068, 12835, 1032, 2138, 2738, 29493, 1072, 1083, 2422, 29493, 1083, 3547, 7550, 1678, 1146, 5627, 29493, 1458, 1117, 1040, 1675, 2587, 1136, 1274, 1040, 23867, 1072, 1040, 14787, 1137, 1136, 29510, 1035, 10302, 29491, 10322, 29491, 1183, 2444, 2587, 1040, 23867, 9151, 1066, 27298, 29493, 1072, 1040, 4776, 1072, 10804, 2587, 1228, 2296, 1040, 1615, 1275, 1070, 1146, 1738, 1136, 1344, 1505, 29493, 1083, 2201, 1066, 8482, 6260, 29491, 1429, 29510, 29481, 2138, 29473, 29518, 29538, 29491, 1083, 2201, 1066, 1279, 1224, 1124, 1040, 2200, 2138, 29491, 4, 1763, 29510, 1035, 2172, 1066, 3946, 1146, 29491, 2, 3, 1763, 29510, 1035, 2172, 1066, 1115, 1505, 29493, 12944, 29493, 1864, 29491, 4, 1763, 29510, 2693, 2172, 1066, 1115, 28523, 29491, 2, 3, 1763, 29510, 1035, 2172, 1066, 1115, 1065, 10048, 6611, 1072, 1136, 2369, 1137, 4135, 1070, 10373, 13011, 29491, 9120, 29493, 1083, 1841, 1146, 29510, 29481, 1032, 2296, 1947, 29491, 1783, 1122, 1296, 29493, 1083, 1274, 1040, 8717, 1070, 1083, 8214, 5295, 1072, 1636, 1083, 6809, 1354, 3609, 29493, 1347, 1146, 29510, 29481, 2296, 1947, 1122, 1296, 29491, 4, 6202, 29493, 1136, 29510, 29483, 1115, 1248, 1661, 4570, 29491, 1083, 1171, 2880, 1070, 7138, 1065, 1354, 3609, 1864, 1083, 1544, 2201, 1620, 1245, 11596, 29491, 2, 3, 6202, 29493, 12944, 29491, 4, 1083, 2201, 1350, 1066, 1505, 29473, 29518, 29508, 29508, 1065, 11596, 29491, 2, 3, 20161, 29572, 4, 9120, 29491, 1083, 29510, 29487, 1505, 29473, 29508, 29542, 29555, 29493, 29473, 29508, 29542, 29551, 29491, 3729, 1083, 2201, 1350, 1066, 29473, 29518, 29508, 29508, 29491, 2, 3, 1783, 1631, 1136, 6856, 3704, 29572, 4, 9120, 29491, 2, 3, 1763, 1544, 16500, 29572, 4, 1083, 1544, 6078, 4355, 1072, 1539, 1343, 1032, 2823, 29491, 2, 3, 2370, 4355, 1631, 1136, 1227, 6078, 1065, 11596, 29572, 4, 1083, 16500, 1505, 1032, 19726, 29491, 1429, 29510, 29481, 3377, 29476, 29491, 1083, 2201, 13499, 1780, 2893, 29491, 2, 3, 1429, 29510, 29481, 1040, 2257, 29491, 2493, 1117, 1040, 2257, 29491, 4, 1429, 1171, 1514, 29491, 2, 3, 1083, 1717, 29510, 29475, 1841, 1504, 29510, 29481, 1032, 2641, 24923, 12754, 1589, 29493, 27837, 29493, 1146, 29510, 29481, 1040, 2257, 29491, 4, 1429, 29510, 29481, 1040, 2257, 29491, 1783, 29493, 1136, 1641, 29493, 1083, 1717, 29510, 29475, 6078, 1146, 1848, 1070, 1040, 1495, 29491, 1783, 1083, 1171, 1065, 11596, 29491, 2, 3, 1328, 11596, 29493, 6906, 29491, 3677, 8482, 4424, 1323, 17620, 3260, 1146, 1065, 1040, 1620, 29491, 4, 9120, 29493, 5436, 29491, 2, 3, 9120, 29491, 4, 2305, 1083, 2836, 5623, 1245, 1137, 1871, 1066, 1354, 3609, 29491, 2, 3, 3729, 29493, 1631, 1136, 3856, 1738, 1136, 29510, 1035, 2172, 1066, 6382, 29572, 1083, 7021, 29510, 29475, 5264, 3551, 29491, 9058, 1136, 10177, 1452, 1535, 1136, 1631, 29572, 4, 2538, 29491, 2, 3, 2305, 1146, 29510, 29481, 1032, 8046, 29491, 4, 1083, 1985, 2359, 1065, 1757, 6331, 29491, 1083, 29510, 1352, 2083, 1146, 1124, 1224, 2115, 29491, 2, 3, 20579, 29491, 4, 1083, 1985, 2359, 1206, 1040, 9217, 18131, 1072, 1636, 1083, 1985, 2359, 1206, 2644, 1350, 3711, 29491, 2, 3, 2305, 1136, 2836, 1245, 6331, 1505, 1392, 1070, 1040, 1848, 5440, 29493, 3243, 29491, 1429, 29510, 29481, 1032, 3243, 29473, 29538, 29552, 29502, 29502, 8046, 18131, 1066, 1032, 19755, 5257, 29491, 4, 9120, 29491, 2, 3, 2305, 1136, 29510, 1035, 6433, 1845, 29572, 4, 9120, 29493, 1083, 29510, 29487, 1544, 3710, 1066, 6016, 1343, 1535, 1083, 1505, 1448, 29491, 1183, 3468, 1163, 1040, 19755, 5257, 1117, 1504, 1171, 2136, 2055, 1673, 5202, 1066, 1296, 29491, 2074, 1422, 2136, 3616, 29491, 2074, 3381, 1066, 1115, 1505, 1032, 1512, 1070, 1040, 2115, 29491, 1584, 1321, 1066, 7177, 1350, 2480, 2349, 5138, 29491, 1783, 1083, 3008, 1474, 1927, 1040, 2115, 29493, 1083, 29510, 29487, 1505, 29493, 5433, 29493, 1083, 29510, 29487, 3734, 1056, 29491, 3957, 29510, 29475, 2753, 29491, 2, 3, 2074, 1717, 29510, 29475, 2424, 29491, 4, 6387, 2077, 2753, 1864, 1146, 29510, 29481, 1673, 1065, 1040, 3546, 5918, 29491, 2074, 29510, 1035, 1505, 29493, 1136, 29510, 1035, 1871, 1504, 29491, 2, 3, 1183, 3546, 5918, 29491, 1083, 6378, 1343, 1350, 1354, 6198, 1065, 24766, 1124, 8497, 1065, 1040, 3546, 5918, 29491, 1619, 5417, 3542, 1505, 29493, 1105, 4340, 1343, 21527, 29493, 1072, 1083, 1344, 29493, 1146, 29510, 29481, 2366, 1066, 1641, 1738, 1040, 16243, 1142, 2106, 1065, 1040, 8706, 1117, 29572, 9636, 1122, 7166, 1350, 3546, 29491, 1783, 1168, 2836, 1505, 1224, 29491, 4, 9120, 29491, 24766, 1117, 2511, 16243, 29491, 2, 3, 1763, 2511, 6260, 1124, 24766, 29491, 4, 1152, 3683, 1142, 2401, 1065, 1040, 3707, 29491, 1429, 29510, 29481, 1227, 1032, 2121, 2433, 29491, 2155, 29510, 29481, 1032, 20895, 2212, 9911, 1072, 2218, 3494, 29493, 1072, 1146, 29510, 29481, 7542, 1163, 1673, 1137, 1274, 1476, 4085, 29491, 2074, 1274, 1476, 4085, 29491, 2, 3, 1083, 29510, 1101, 2252, 4132, 13189, 1344, 12551, 1124, 24766, 29491, 4, 5441, 29493, 1083, 8876, 1350, 1065, 17023, 29491, 2, 3, 9120, 29491, 4, 2305, 1083, 4966, 1065, 24766, 1312, 1040, 1495, 29491, 1783, 1780, 1495, 1083, 29510, 29483, 4298, 1504, 29493, 1083, 29510, 29483, 1115, 1505, 29493, 1535, 1040, 4927, 29572, 2, 3, 1083, 29510, 1352, 2680, 1136, 1224, 29493, 2838, 29491, 1083, 1631, 7653, 1033, 2030, 7825, 24827, 29493, 1072, 1146, 1171, 1032, 8482, 23864, 29491, 4, 9120, 29491, 5590, 1114, 1673, 1245, 2218, 3494, 29491, 2, 3, 1098, 2823, 1070, 3810, 1673, 29491, 9120, 29493, 1504, 1136, 1344, 29491, 4, 2493, 3638, 4135, 29491, 2, 3, 9120, 29491, 4, 1183, 24766, 1673, 1228, 3817, 1505, 29493, 1535, 29510, 29481, 13566, 15595, 1206, 29572, 3155, 1070, 1354, 2257, 4050, 29493, 21661, 13466, 29491, 1418, 2077, 5389, 1504, 29491, 2, 3, 2493, 29510, 29481, 7081, 5153, 29491, 9120, 29491, 4, 1083, 29510, 1101, 1518, 5719, 1122, 29473, 29538, 29502, 2035, 29491, 2, 3, 1418, 5389, 1504, 1072, 1168, 29510, 29481, 1245, 1504, 29491, 4, 9120, 29491, 1083, 6260, 1124, 1481, 1780, 1495, 1083, 1802, 1481, 1350, 29491, 2, 3, 1429, 29510, 29481, 1347, 1514, 1066, 6260, 1124, 1032, 1924, 29491, 1083, 2840, 29493, 1505, 29493, 1083, 8433, 1181, 4320, 29493, 16240, 1347, 1956, 29491, 4, 1181, 4320, 29493, 16240, 29491, 2, 3, 11141, 1032, 8482, 11100, 29491, 1098, 12531, 1070, 2283, 1172, 29491, 1429, 29510, 29481, 1032, 28464, 4504, 1163, 1032, 28464, 9493, 29493, 1072, 1281, 1136, 3711, 1504, 29493, 1136, 1641, 29493, 1136, 29510, 1035, 1032, 8482, 6897, 29491, 4, 3962, 1393, 1343, 29491, 2, 3, 3251, 1343, 29491, 4, 1860, 1281, 1136, 29510, 1035, 8482, 6149, 1032, 2401, 1505, 1137, 29493, 1136, 29510, 1035, 1279, 17580, 29491, 2592, 1040, 4927, 1279, 1136, 1279, 29572, 28794, 29510, 29475, 1137, 5610, 1163, 1420, 10013, 1137, 1544, 1274, 1032, 4928, 4006, 1522, 1504, 29572, 6178, 25037, 29493, 1505, 2218, 22372, 29491, 2, 3, 6202, 29493, 12944, 29491, 4, 22697, 4006, 1070, 1639, 1984, 29491, 2, 3, 13477, 26012, 29491, 1763, 2427, 1158, 1930, 1115, 1065, 2466, 3707, 1065, 17700, 29491, 1783, 1083, 1603, 2369, 1505, 4420, 10454, 29491, 2452, 1136, 1393, 1546, 1040, 2121, 3758, 1070, 4420, 10454, 29491, 4, 9120, 29491, 7694, 1312, 1040, 3698, 6260, 1117, 29491, 2, 3, 9120, 29491, 4, 1429, 29510, 29481, 1032, 9681, 2349, 1055, 1151, 6260, 29491, 2074, 8482, 25919, 1137, 29491, 2493, 29510, 29481, 1040, 3758, 1137, 3843, 1673, 25919, 29491, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 32768, 1, 3, 1763, 1422, 18713, 1056, 9828, 29491, 2305, 2114, 29510, 29481, 1227, 1279, 1137, 29491, 1860, 1146, 1171, 29491, 2370, 1452, 2313, 1137, 1117, 1032, 8803, 1330, 1032, 4406, 1137, 1504, 1117, 1476, 2880, 1070, 14610, 29572, 2155, 29510, 29481, 1544, 29493, 1279, 1136, 1279, 1146, 1210, 1227, 29572, 4, 10322, 29491, 2, 3, 1783, 1535, 1136, 9518, 1171, 5125, 1246, 8424, 1420, 1885, 2650, 1844, 1072, 13566, 1427, 1066, 1279, 1032, 29473, 29550, 29502, 29502, 3068, 12835, 1032, 2138, 26807, 29491, 4, 9120, 29491, 4971, 2138, 29493, 7439, 2970, 1032, 2587, 29491, 2, 3, 4771, 29510, 29481, 1354, 3764, 29491, 3937, 29510, 29481, 2083, 1136, 1344, 1065, 1040, 4738, 1072, 1136, 1393, 1509, 8586, 1065, 29493, 1072, 1146, 6466, 29481, 29491, 3937, 29510, 29481, 1544, 2083, 29473, 29518, 29502, 29502, 25104, 1279, 29491, 1763, 2307, 29510, 29475, 1274, 1066, 1544, 1279, 29473, 29538, 29502, 29502, 1448, 29491, 20579, 29491, 2305, 1146, 29510, 29481, 1544, 29473, 29550, 29502, 29502, 29491, 4, 29473, 29550, 29502, 29502, 1065, 1032, 2138, 29491, 20579, 29491, 1083, 2369, 1505, 1137, 29510, 29481, 5736, 29491, 2, 3, 1083, 1841, 1146, 29510, 29481, 5736, 29491, 4, 3048, 2136, 10048, 29493, 1864, 1083, 1641, 1136, 1505, 1066, 1279, 1757, 1539, 9276, 1032, 2138, 29491, 2305, 1281, 1136, 1631, 29493, 1505, 29493, 1342, 23621, 1072, 1636, 1136, 1631, 1342, 4916, 1459, 29493, 1136, 1393, 1146, 1065, 29491, 2, 3, 1763, 1309, 1393, 1146, 1065, 1040, 2738, 29491, 4, 1183, 23842, 1117, 29493, 1505, 7439, 2970, 1032, 2587, 29491, 1429, 29510, 29481, 1505, 1507, 1246, 1631, 1040, 4064, 22383, 8803, 29493, 1246, 1321, 1066, 1279, 29473, 29508, 29550, 1065, 1040, 2870, 29493, 1458, 29491, 24879, 12069, 29491, 2, 3, 2066, 2180, 1290, 29491, 4, 1860, 5851, 1040, 1716, 29493, 1083, 1841, 1083, 1057, 14042, 29493, 1505, 29493, 10310, 29491, 1783, 1347, 1083, 1321, 1066, 1279, 29493, 1505, 29493, 10310, 2970, 1065, 1032, 5918, 29491, 1783, 1792, 1032, 2080, 29493, 1136, 1544, 1393, 1065, 1224, 10048, 6745, 1519, 1738, 1136, 3547, 2880, 1070, 1505, 1040, 2407, 1137, 1146, 29510, 29481, 26794, 29491, 1763, 4226, 1040, 2407, 1137, 1136, 2201, 1066, 1279, 1146, 29491, 2, 3, 1083, 1171, 4963, 2200, 2893, 1452, 1224, 29473, 29550, 29502, 29502, 3068, 12835, 1032, 2138, 2738, 29493, 1072, 1083, 2422, 29493, 1083, 3547, 7550, 1678, 1146, 5627, 29493, 1458, 1117, 1040, 1675, 2587, 1136, 1274, 1040, 23867, 1072, 1040, 14787, 1137, 1136, 29510, 1035, 10302, 29491, 10322, 29491, 1183, 2444, 2587, 1040, 23867, 9151, 1066, 27298, 29493, 1072, 1040, 4776, 1072, 10804, 2587, 1228, 2296, 1040, 1615, 1275, 1070, 1146, 1738, 1136, 1344, 1505, 29493, 1083, 2201, 1066, 8482, 6260, 29491, 1429, 29510, 29481, 2138, 29473, 29518, 29538, 29491, 1083, 2201, 1066, 1279, 1224, 1124, 1040, 2200, 2138, 29491, 4, 1763, 29510, 1035, 2172, 1066, 3946, 1146, 29491, 2, 3, 1763, 29510, 1035, 2172, 1066, 1115, 1505, 29493, 12944, 29493, 1864, 29491, 4, 1763, 29510, 2693, 2172, 1066, 1115, 28523, 29491, 2, 3, 1763, 29510, 1035, 2172, 1066, 1115, 1065, 10048, 6611, 1072, 1136, 2369, 1137, 4135, 1070, 10373, 13011, 29491, 9120, 29493, 1083, 1841, 1146, 29510, 29481, 1032, 2296, 1947, 29491, 1783, 1122, 1296, 29493, 1083, 1274, 1040, 8717, 1070, 1083, 8214, 5295, 1072, 1636, 1083, 6809, 1354, 3609, 29493, 1347, 1146, 29510, 29481, 2296, 1947, 1122, 1296, 29491, 4, 6202, 29493, 1136, 29510, 29483, 1115, 1248, 1661, 4570, 29491, 1083, 1171, 2880, 1070, 7138, 1065, 1354, 3609, 1864, 1083, 1544, 2201, 1620, 1245, 11596, 29491, 2, 3, 6202, 29493, 12944, 29491, 4, 1083, 2201, 1350, 1066, 1505, 29473, 29518, 29508, 29508, 1065, 11596, 29491, 2, 3, 20161, 29572, 4, 9120, 29491, 1083, 29510, 29487, 1505, 29473, 29508, 29542, 29555, 29493, 29473, 29508, 29542, 29551, 29491, 3729, 1083, 2201, 1350, 1066, 29473, 29518, 29508, 29508, 29491, 2, 3, 1783, 1631, 1136, 6856, 3704, 29572, 4, 9120, 29491, 2, 3, 1763, 1544, 16500, 29572, 4, 1083, 1544, 6078, 4355, 1072, 1539, 1343, 1032, 2823, 29491, 2, 3, 2370, 4355, 1631, 1136, 1227, 6078, 1065, 11596, 29572, 4, 1083, 16500, 1505, 1032, 19726, 29491, 1429, 29510, 29481, 3377, 29476, 29491, 1083, 2201, 13499, 1780, 2893, 29491, 2, 3, 1429, 29510, 29481, 1040, 2257, 29491, 2493, 1117, 1040, 2257, 29491, 4, 1429, 1171, 1514, 29491, 2, 3, 1083, 1717, 29510, 29475, 1841, 1504, 29510, 29481, 1032, 2641, 24923, 12754, 1589, 29493, 27837, 29493, 1146, 29510, 29481, 1040, 2257, 29491, 4, 1429, 29510, 29481, 1040, 2257, 29491, 1783, 29493, 1136, 1641, 29493, 1083, 1717, 29510, 29475, 6078, 1146, 1848, 1070, 1040, 1495, 29491, 1783, 1083, 1171, 1065, 11596, 29491, 2, 3, 1328, 11596, 29493, 6906, 29491, 3677, 8482, 4424, 1323, 17620, 3260, 1146, 1065, 1040, 1620, 29491, 4, 9120, 29493, 5436, 29491, 2, 3, 9120, 29491, 4, 2305, 1083, 2836, 5623, 1245, 1137, 1871, 1066, 1354, 3609, 29491, 2, 3, 3729, 29493, 1631, 1136, 3856, 1738, 1136, 29510, 1035, 2172, 1066, 6382, 29572, 1083, 7021, 29510, 29475, 5264, 3551, 29491, 9058, 1136, 10177, 1452, 1535, 1136, 1631, 29572, 4, 2538, 29491, 2, 3, 2305, 1146, 29510, 29481, 1032, 8046, 29491, 4, 1083, 1985, 2359, 1065, 1757, 6331, 29491, 1083, 29510, 1352, 2083, 1146, 1124, 1224, 2115, 29491, 2, 3, 20579, 29491, 4, 1083, 1985, 2359, 1206, 1040, 9217, 18131, 1072, 1636, 1083, 1985, 2359, 1206, 2644, 1350, 3711, 29491, 2, 3, 2305, 1136, 2836, 1245, 6331, 1505, 1392, 1070, 1040, 1848, 5440, 29493, 3243, 29491, 1429, 29510, 29481, 1032, 3243, 29473, 29538, 29552, 29502, 29502, 8046, 18131, 1066, 1032, 19755, 5257, 29491, 4, 9120, 29491, 2, 3, 2305, 1136, 29510, 1035, 6433, 1845, 29572, 4, 9120, 29493, 1083, 29510, 29487, 1544, 3710, 1066, 6016, 1343, 1535, 1083, 1505, 1448, 29491, 1183, 3468, 1163, 1040, 19755, 5257, 1117, 1504, 1171, 2136, 2055, 1673, 5202, 1066, 1296, 29491, 2074, 1422, 2136, 3616, 29491, 2074, 3381, 1066, 1115, 1505, 1032, 1512, 1070, 1040, 2115, 29491, 1584, 1321, 1066, 7177, 1350, 2480, 2349, 5138, 29491, 1783, 1083, 3008, 1474, 1927, 1040, 2115, 29493, 1083, 29510, 29487, 1505, 29493, 5433, 29493, 1083, 29510, 29487, 3734, 1056, 29491, 3957, 29510, 29475, 2753, 29491, 2, 3, 2074, 1717, 29510, 29475, 2424, 29491, 4, 6387, 2077, 2753, 1864, 1146, 29510, 29481, 1673, 1065, 1040, 3546, 5918, 29491, 2074, 29510, 1035, 1505, 29493, 1136, 29510, 1035, 1871, 1504, 29491, 2, 3, 1183, 3546, 5918, 29491, 1083, 6378, 1343, 1350, 1354, 6198, 1065, 24766, 1124, 8497, 1065, 1040, 3546, 5918, 29491, 1619, 5417, 3542, 1505, 29493, 1105, 4340, 1343, 21527, 29493, 1072, 1083, 1344, 29493, 1146, 29510, 29481, 2366, 1066, 1641, 1738, 1040, 16243, 1142, 2106, 1065, 1040, 8706, 1117, 29572, 9636, 1122, 7166, 1350, 3546, 29491, 1783, 1168, 2836, 1505, 1224, 29491, 4, 9120, 29491, 24766, 1117, 2511, 16243, 29491, 2, 3, 1763, 2511, 6260, 1124, 24766, 29491, 4, 1152, 3683, 1142, 2401, 1065, 1040, 3707, 29491, 1429, 29510, 29481, 1227, 1032, 2121, 2433, 29491, 2155, 29510, 29481, 1032, 20895, 2212, 9911, 1072, 2218, 3494, 29493, 1072, 1146, 29510, 29481, 7542, 1163, 1673, 1137, 1274, 1476, 4085, 29491, 2074, 1274, 1476, 4085, 29491, 2, 3, 1083, 29510, 1101, 2252, 4132, 13189, 1344, 12551, 1124, 24766, 29491, 4, 5441, 29493, 1083, 8876, 1350, 1065, 17023, 29491, 2, 3, 9120, 29491, 4, 2305, 1083, 4966, 1065, 24766, 1312, 1040, 1495, 29491, 1783, 1780, 1495, 1083, 29510, 29483, 4298, 1504, 29493, 1083, 29510, 29483, 1115, 1505, 29493, 1535, 1040, 4927, 29572, 2, 3, 1083, 29510, 1352, 2680, 1136, 1224, 29493, 2838, 29491, 1083, 1631, 7653, 1033, 2030, 7825, 24827, 29493, 1072, 1146, 1171, 1032, 8482, 23864, 29491, 4, 9120, 29491, 5590, 1114, 1673, 1245, 2218, 3494, 29491, 2, 3, 1098, 2823, 1070, 3810, 1673, 29491, 9120, 29493, 1504, 1136, 1344, 29491, 4, 2493, 3638, 4135, 29491, 2, 3, 9120, 29491, 4, 1183, 24766, 1673, 1228, 3817, 1505, 29493, 1535, 29510, 29481, 13566, 15595, 1206, 29572, 3155, 1070, 1354, 2257, 4050, 29493, 21661, 13466, 29491, 1418, 2077, 5389, 1504, 29491, 2, 3, 2493, 29510, 29481, 7081, 5153, 29491, 9120, 29491, 4, 1083, 29510, 1101, 1518, 5719, 1122, 29473, 29538, 29502, 2035, 29491, 2, 3, 1418, 5389, 1504, 1072, 1168, 29510, 29481, 1245, 1504, 29491, 4, 9120, 29491, 1083, 6260, 1124, 1481, 1780, 1495, 1083, 1802, 1481, 1350, 29491, 2, 3, 1429, 29510, 29481, 1347, 1514, 1066, 6260, 1124, 1032, 1924, 29491, 1083, 2840, 29493, 1505, 29493, 1083, 8433, 1181, 4320, 29493, 16240, 1347, 1956, 29491, 4, 1181, 4320, 29493, 16240, 29491, 2, 3, 11141, 1032, 8482, 11100, 29491, 1098, 12531, 1070, 2283, 1172, 29491, 1429, 29510, 29481, 1032, 28464, 4504, 1163, 1032, 28464, 9493, 29493, 1072, 1281, 1136, 3711, 1504, 29493, 1136, 1641, 29493, 1136, 29510, 1035, 1032, 8482, 6897, 29491, 4, 3962, 1393, 1343, 29491, 2, 3, 3251, 1343, 29491, 4, 1860, 1281, 1136, 29510, 1035, 8482, 6149, 1032, 2401, 1505, 1137, 29493, 1136, 29510, 1035, 1279, 17580, 29491, 2592, 1040, 4927, 1279, 1136, 1279, 29572, 28794, 29510, 29475, 1137, 5610, 1163, 1420, 10013, 1137, 1544, 1274, 1032, 4928, 4006, 1522, 1504, 29572, 6178, 25037, 29493, 1505, 2218, 22372, 29491, 2, 3, 6202, 29493, 12944, 29491, 4, 22697, 4006, 1070, 1639, 1984, 29491, 2, 3, 13477, 26012, 29491, 1763, 2427, 1158, 1930, 1115, 1065, 2466, 3707, 1065, 17700, 29491, 1783, 1083, 1603, 2369, 1505, 4420, 10454, 29491, 2452, 1136, 1393, 1546, 1040, 2121, 3758, 1070, 4420, 10454, 29491, 4, 9120, 29491, 7694, 1312, 1040, 3698, 6260, 1117, 29491, 2, 3, 9120, 29491, 4, 1429, 29510, 29481, 1032, 9681, 2349, 1055, 1151, 6260, 29491, 2074, 8482, 25919, 1137, 29491, 2493, 29510, 29481, 1040, 3758, 1137, 3843, 1673, 25919, 29491, 2]\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 665\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 1. Apply chat template\n",
    "# 2. Tokenize data\n",
    "# 3. Add padding\n",
    "tokenized_dataset = base_tokenizer.apply_chat_template(\n",
    "    newChunksDataset[\"text\"], padding=True, return_dict=True\n",
    ")\n",
    "# print(tokenized_dataset)\n",
    "\n",
    "# add label to the dataset (same as input_ids)\n",
    "tokenized_dataset[\"labels\"] = tokenized_dataset[\"input_ids\"].copy()\n",
    "\n",
    "# log results\n",
    "print(\"input_ids\", len(tokenized_dataset[\"input_ids\"]))\n",
    "print(\"attention_mask\", len(tokenized_dataset[\"attention_mask\"]))\n",
    "print(\"labels\", len(tokenized_dataset[\"labels\"]))\n",
    "\n",
    "print(tokenized_dataset[\"input_ids\"][1])\n",
    "print(tokenized_dataset[\"attention_mask\"][1])\n",
    "print(tokenized_dataset[\"labels\"][1])\n",
    "\n",
    "# for t in tokenized_dataset[\"input_ids\"]:\n",
    "#     print(t)\n",
    "\n",
    "# for t in tokenized_dataset[\"attention_mask\"]:\n",
    "#     print(t)\n",
    "\n",
    "# for t in tokenized_dataset[\"labels\"]:\n",
    "#     print(t)\n",
    "\n",
    "# create a new Dataset from modified data\n",
    "newDataset = Dataset.from_dict(tokenized_dataset)\n",
    "print(newDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def visualize_token_lengths(dataset):\n",
    "\n",
    "    # Extracting lengths of each tokenized text\n",
    "    lengths = [len(x[\"input_ids\"]) for x in dataset]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(lengths, bins=30, color='blue', alpha=0.7)\n",
    "    plt.title(\"Distribution of Token Lengths\")\n",
    "    plt.xlabel(\"Token Length\")\n",
    "    plt.ylabel(\"Rows\")\n",
    "\n",
    "    # Use ScalarFormatter to adjust the x-axis labels\n",
    "    ax = plt.gca()  # Get current axis\n",
    "    formatter = ticker.ScalarFormatter(useOffset=False)  # Disable scientific notation\n",
    "    formatter.set_scientific(False)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_token_lengths(newDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 532\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 133\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# final dataset\n",
    "final_datasets = newDataset.train_test_split(test_size=0.2)\n",
    "print(final_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ----------------------------------\n",
    "# Adding the adapters to the layers\n",
    "# ----------------------------------\n",
    "\n",
    "# PEFT\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    # target_modules=[\n",
    "    #     \"q_proj\",\n",
    "    #     \"k_proj\",\n",
    "    #     \"down_proj\",\n",
    "    #     \"v_proj\",\n",
    "    #     \"gate_proj\",\n",
    "    #     \"o_proj\",\n",
    "    #     \"up_proj\",\n",
    "    # ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # modules_to_save=[\n",
    "    #     \"lm_head\",\n",
    "    #     \"embed_tokens\",\n",
    "    # ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",  # https://huggingface.co/docs/peft/en/developer_guides/lora#qlora-style-training\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "temp_model = prepare_model_for_kbit_training(base_model)\n",
    "peft_model = get_peft_model(temp_model, peft_config)\n",
    "peft_model.config.use_cache = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "Architecture: PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32769, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 32769\n",
      "Input embeddings:\n",
      "Embedding(32769, 4096)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=32769, bias=False)\n",
      "Model device: cuda:0\n",
      "Model is CUDA:  True\n",
      "---PEFT---\n",
      "Get Trainable Parameters\n",
      "trainable params: 167,772,160 || all params: 7,415,803,904 || trainable%: 2.2624\n",
      "Get Layer Status\n",
      "[TunerLayerStatus(name='model.model.layers.0.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']})]\n",
      "Get Model Status\n",
      "TunerModelStatus(base_model_type='MistralForCausalLM', adapter_model_type='LoraModel', peft_types={'default': 'LORA'}, trainable_params=167772160, total_params=7415803904, num_adapter_layers=224, enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']})\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(peft_model))\n",
    "print(\"Architecture:\", peft_model)\n",
    "print(\"Config:\", peft_model.config)\n",
    "print(\"Model Vocabulary Size:\", peft_model.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(peft_model.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(peft_model.get_output_embeddings())\n",
    "print(\"Model device:\", peft_model.device)\n",
    "print(\"Model is CUDA: \", next(peft_model.parameters()).is_cuda)\n",
    "\n",
    "\n",
    "print(\"---PEFT---\")\n",
    "\n",
    "print(\"Get Trainable Parameters\")\n",
    "peft_model.print_trainable_parameters()\n",
    "# trainable params: 167,772,160 || all params: 7,415,803,904 || trainable%: 2.2624\n",
    "\n",
    "print(\"Get Layer Status\")\n",
    "print(peft_model.get_layer_status())\n",
    "\n",
    "print(\"Get Model Status\")\n",
    "print(peft_model.get_model_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d15650936a46da85da7ab0785487d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e42a6144a442c79b6855fbc4ed687f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Training WITH evaluation (metrics)\n",
    "# ----------------------------------\n",
    "\n",
    "lr = 0.0005 # learning rate\n",
    "bs = 1  # batch size\n",
    "ga_steps = 1  # gradient acc. steps\n",
    "epochs = 3\n",
    "steps_per_epoch = len(final_datasets[\"train\"]) // (bs * ga_steps)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"SAVED_TRAINING\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    learning_rate=lr,\n",
    "    save_steps=steps_per_epoch,\n",
    "    save_total_limit=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=steps_per_epoch,  # eval and save once per epoch\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",  # Directory for storing logs\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    fp16=True,\n",
    "    # bf16=True,\n",
    ")\n",
    "\n",
    "print(training_args.device)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    # tokenizer=base_tokenizer,\n",
    "    train_dataset=final_datasets[\"train\"],\n",
    "    eval_dataset=final_datasets[\"test\"],\n",
    "    args=training_args,\n",
    "    # packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "Architecture: PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32769, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 32769\n",
      "Input embeddings:\n",
      "Embedding(32769, 4096)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=32769, bias=False)\n",
      "Model device: cuda:0\n",
      "Model is CUDA:  True\n",
      "---PEFT---\n",
      "Get Trainable Parameters\n",
      "trainable params: 167,772,160 || all params: 7,415,803,904 || trainable%: 2.2624\n",
      "Get Layer Status\n",
      "[TunerLayerStatus(name='model.model.layers.0.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.0.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.1.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.2.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.3.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.4.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.5.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.6.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.7.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.8.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.9.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.10.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.11.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.12.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.13.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.14.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.15.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.16.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.17.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.18.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.19.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.20.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.21.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.22.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.23.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.24.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.25.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.26.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.27.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.28.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.29.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.30.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.self_attn.q_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.self_attn.k_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.self_attn.v_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.self_attn.o_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.mlp.gate_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.mlp.up_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']}), TunerLayerStatus(name='model.model.layers.31.mlp.down_proj', module_type='lora.Linear4bit', enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']})]\n",
      "Get Model Status\n",
      "TunerModelStatus(base_model_type='MistralForCausalLM', adapter_model_type='LoraModel', peft_types={'default': 'LORA'}, trainable_params=167772160, total_params=7415803904, num_adapter_layers=224, enabled=True, active_adapters=['default'], merged_adapters=[], requires_grad={'default': True}, available_adapters=['default'], devices={'default': ['cuda']})\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(peft_model))\n",
    "print(\"Architecture:\", peft_model)\n",
    "print(\"Config:\", peft_model.config)\n",
    "print(\"Model Vocabulary Size:\", peft_model.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(peft_model.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(peft_model.get_output_embeddings())\n",
    "print(\"Model device:\", peft_model.device)\n",
    "print(\"Model is CUDA: \", next(peft_model.parameters()).is_cuda)\n",
    "\n",
    "\n",
    "print(\"---PEFT---\")\n",
    "\n",
    "print(\"Get Trainable Parameters\")\n",
    "peft_model.print_trainable_parameters()\n",
    "# trainable params: 167,772,160 || all params: 7,415,803,904 || trainable%: 2.2624\n",
    "\n",
    "print(\"Get Layer Status\")\n",
    "print(peft_model.get_layer_status())\n",
    "\n",
    "print(\"Get Model Status\")\n",
    "print(peft_model.get_model_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1596' max='1596' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1596/1596 47:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>10.361900</td>\n",
       "      <td>10.352277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>10.364400</td>\n",
       "      <td>10.349236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1596</td>\n",
       "      <td>10.387200</td>\n",
       "      <td>10.347080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training takes: 0:47:39.030569\n",
      "Total takes: 0:47:39.030744\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(\"Start training...\")\n",
    "startTrain = time.time()\n",
    "trainer.train()\n",
    "td = timedelta(seconds=(time.time() - startTrain))\n",
    "print(f\"Training takes: {td}\")\n",
    "\n",
    "\n",
    "# Total time for the script\n",
    "td = timedelta(seconds=(time.time() - start))\n",
    "print(f\"Total takes: {td}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the adapter - OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?....................................................................................................................................................................................................................................................................................'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "my_pipeline = pipeline(\n",
    "    \"text-generation\", model=peft_model, tokenizer=base_tokenizer, max_length=300\n",
    ")\n",
    "\n",
    "result = my_pipeline(\"Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,     3, 17930,  1678,  1228,  1136, 29572, 16027,  1296,  2313,\n",
      "          1452,  4704, 29491,  2592,  1279,  1136,  1841,  1452,  1229,  8277,\n",
      "         29501, 29536,  1814, 29486, 29572,     4]], device='cuda:0')\n",
      "Answer:\n",
      "<s>[INST] Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?[/INST] I.................................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "# input_ids = base_tokenizer.encode(\n",
    "#     \"What is AutoGen in abstract?\", return_tensors=\"pt\"\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": \"Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?\"}]\n",
    "input_ids = base_tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(input_ids)\n",
    "\n",
    "# Generate text\n",
    "result = peft_model.generate(input_ids, max_length=300)\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = base_tokenizer.decode(result[0])\n",
    "print(\"Answer:\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the adapter (to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SAVED_ADAPTER/tokenizer_config.json',\n",
       " 'SAVED_ADAPTER/special_tokens_map.json',\n",
       " 'SAVED_ADAPTER/tokenizer.model',\n",
       " 'SAVED_ADAPTER/added_tokens.json',\n",
       " 'SAVED_ADAPTER/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.save_pretrained(\"SAVED_ADAPTER\")\n",
    "base_tokenizer.save_pretrained(\"SAVED_ADAPTER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push adapter (to hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36faf117530344bd823f9a160827932a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d101aaedbdf34b748313d923c95d981b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bd3341277d45eda967d28b25d35df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lukaskellerstein/joe-mistral-4bit-lora-adapter-new/commit/a4e4d8036d119dfe73bf89d53067151dc11b600d', commit_message='Upload tokenizer', commit_description='', oid='a4e4d8036d119dfe73bf89d53067151dc11b600d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/lukaskellerstein/joe-mistral-4bit-lora-adapter-new', endpoint='https://huggingface.co', repo_type='model', repo_id='lukaskellerstein/joe-mistral-4bit-lora-adapter-new'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/joe-mistral-4bit-lora-adapter-new\",\n",
    "    token=API_TOKEN,\n",
    ")\n",
    "base_tokenizer.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/joe-mistral-4bit-lora-adapter-new\",\n",
    "    token=API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGED model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge LoRA adapter and base model => merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e734dcbec502491295e5ac68bd701228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings:  Embedding(32768, 4096)\n",
      "Output embeddings:  Linear(in_features=4096, out_features=32768, bias=False)\n",
      "Model Vocabulary Size:  32768\n",
      "Before add token to tokenizer - tokenizer length:  32768\n",
      "After add token to tokenizer - tokenizer length:  32769\n",
      "Before add pad token to model - pad token Id:  None\n",
      "After add pad token to model - pad token Id:  32768\n",
      "Before resizing Model Vocabulary - Size:  32768\n",
      "After  resizing Model Vocabulary - Size:  32769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff63a625551742329461ba5da4fef2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d18a9eb4d94936ae0716b15c21d1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32769, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[    1,     3, 17930,  1678,  1228,  1136, 29572, 16027,  1296,  2313,\n",
      "          1452,  4704, 29491,     4]], device='cuda:0')\n",
      "<s>[INST] Hey how are you? Tell me something about yourself.[/INST] I.............................................................................................................................................................................................................................................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unloading and merging model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 647/647 [00:00<00:00, 8658.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Merge LoRA adapters with base model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "adapter_path = \"lukaskellerstein/joe-mistral-4bit-lora-adapter-new\"  # input: adapters\n",
    "\n",
    "# ------------------------------------------------\n",
    "# WE CANNOT MERGE Quantized model with LoRA !!!!!!!!!!!!!\n",
    "# ------------------------------------------------\n",
    "# Model\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )\n",
    "\n",
    "# Base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "print(\"Input embeddings: \", base_model.get_input_embeddings())\n",
    "print(\"Output embeddings: \", base_model.get_output_embeddings())\n",
    "print(\"Model Vocabulary Size: \", base_model.config.vocab_size)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Before add token to tokenizer - tokenizer length: \", len(base_tokenizer))\n",
    "base_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "print(\"After add token to tokenizer - tokenizer length: \", len(base_tokenizer))\n",
    "\n",
    "print(\"Before add pad token to model - pad token Id: \", base_model.config.pad_token_id)\n",
    "base_model.config.pad_token_id = base_tokenizer.pad_token_id\n",
    "print(\"After add pad token to model - pad token Id: \", base_model.config.pad_token_id)\n",
    "\n",
    "print(\"Before resizing Model Vocabulary - Size: \", base_model.config.vocab_size)\n",
    "base_model.resize_token_embeddings(len(base_tokenizer))\n",
    "print(\"After  resizing Model Vocabulary - Size: \", base_model.config.vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "# Load PEFT model\n",
    "peft_model_loaded = PeftModel.from_pretrained(\n",
    "    model=base_model,\n",
    "    model_id=adapter_path,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "print(type(peft_model_loaded))\n",
    "print(peft_model_loaded)\n",
    "\n",
    "# -------------------------------------\n",
    "# Test the model after load\n",
    "# -------------------------------------\n",
    "# Tokenize the prompt\n",
    "chat = [{\"role\": \"user\", \"content\": \"Hey how are you? Tell me something about yourself.\"}]\n",
    "input_ids = base_tokenizer.apply_chat_template(chat, return_tensors ='pt').to(\"cuda\")\n",
    "print(input_ids)\n",
    "\n",
    "# Generate text\n",
    "result = peft_model_loaded.generate(input_ids, max_length=300)\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = base_tokenizer.decode(result[0])\n",
    "print(output_text)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Merge base model and LoRA adapter together into one full model\n",
    "# ---------------------------------------------------------------\n",
    "merged_model = peft_model_loaded.merge_and_unload(progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n",
      "Architecture: MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32769, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 32769\n",
      "Input embeddings:\n",
      "Embedding(32769, 4096)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=32769, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "All tokens count: 32769\n",
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(merged_model))\n",
    "print(\"Architecture:\", merged_model)\n",
    "print(\"Config:\", merged_model.config)\n",
    "print(\"Model Vocabulary Size:\", merged_model.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(merged_model.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(merged_model.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(base_tokenizer))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", base_tokenizer.special_tokens_map)\n",
    "print(\"All tokens count:\", len(base_tokenizer))\n",
    "print(\"Padding side:\", base_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test \"merged\" model - ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?....................................................................................................................................................................................................................................................................................'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "my_pipeline = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=base_tokenizer, max_length=300\n",
    ")\n",
    "\n",
    "result = my_pipeline(\"Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,     3, 17930,  1678,  1228,  1136, 29572, 16027,  1296,  2313,\n",
      "          1452,  4704, 29491,  2592,  1279,  1136,  1841,  1452,  1229,  8277,\n",
      "         29501, 29536,  1814, 29486, 29572, 29473,     4]], device='cuda:0')\n",
      "<s>[INST] Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu? [/INST]I think it's great. I think it's great. I think it's a great physical exercise. I think it's also a great mind exercise because it's so difficult to do and it requires so much focus and concentration. And when you're doing it correctly, you're kind of meditating. You're in this kind of zone where you're not thinking about anything else other than the technique and the position and the movement. And it's very difficult to stay in that zone. And I think that's one of the great benefits of it, is that it forces you to concentrate and to focus. And that's something that's very difficult to do in this world where we're constantly being bombarded with stimuli. And I think that's one of the reasons why it's so beneficial, both physically and mentally.</s>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "# input_ids = base_tokenizer.encode(\n",
    "#     \"What is AutoGen in abstract?\", return_tensors=\"pt\"\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?\",\n",
    "    }\n",
    "]\n",
    "input_ids = base_tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(input_ids)\n",
    "\n",
    "# Generate text\n",
    "result = merged_model.generate(input_ids, max_length=300)\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = base_tokenizer.decode(result[0])\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save \"merged\" model (to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MERGED/tokenizer_config.json',\n",
       " 'MERGED/special_tokens_map.json',\n",
       " 'MERGED/tokenizer.model',\n",
       " 'MERGED/added_tokens.json',\n",
       " 'MERGED/tokenizer.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.save_pretrained(\"MERGED\")\n",
    "\n",
    "base_tokenizer.save_pretrained(\"MERGED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push \"merged\" model (to hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe19f42a9304cf1841e963d52768af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f077da00534a0a90a81e99f3edfda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3522e9df93149e2aaf7e940b2ca0ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39140bdd247b4296b5fd2e1b869a6423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2329e1363fa4c66843873ba8c5dfdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fafaa48cc1e4b67a507cf0ea7ff439d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lukaskellerstein/joe-mistral-16bit-merged/commit/951e65d85334e77df98b40b1413d7c726dc9e742', commit_message='Upload tokenizer', commit_description='', oid='951e65d85334e77df98b40b1413d7c726dc9e742', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/joe-mistral-16bit-merged-new\",\n",
    "    token=API_TOKEN,\n",
    ")\n",
    "base_tokenizer.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/joe-mistral-16bit-merged-new\",\n",
    "    token=API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged model from HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_hub_name = \"lukaskellerstein/joe-mistral-16bit-merged-new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18523c16464a46949fc35a9837054f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995a5a4e92c945cdb51679815b03b346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3178e6f6ff5e42f4b227e33fc74fb0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4992fa07c804be1a68a98912c18958a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d34aa060684e8b8bddc9db2715aafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7128cec48164426b6ff881dc4200130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d362a538c4ea4fc1b65897d3f7ab1fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model_loaded_fromHF = AutoModelForCausalLM.from_pretrained(\n",
    "    model_from_hub_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "tokenizer_loaded_fromHF = AutoTokenizer.from_pretrained(model_from_hub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n",
      "Architecture: MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32769, 4096, padding_idx=32768)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32769, bias=False)\n",
      ")\n",
      "Config: MistralConfig {\n",
      "  \"_name_or_path\": \"lukaskellerstein/joe-mistral-16bit-merged\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 32768,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32769\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 32769\n",
      "Input embeddings:\n",
      "Embedding(32769, 4096, padding_idx=32768)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=32769, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}\n",
      "All tokens count: 32769\n",
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(model_loaded_fromHF))\n",
    "print(\"Architecture:\", model_loaded_fromHF)\n",
    "print(\"Config:\", model_loaded_fromHF.config)\n",
    "print(\"Model Vocabulary Size:\", model_loaded_fromHF.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(model_loaded_fromHF.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(model_loaded_fromHF.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(tokenizer_loaded_fromHF))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", tokenizer_loaded_fromHF.special_tokens_map)\n",
    "print(\"All tokens count:\", len(tokenizer_loaded_fromHF))\n",
    "print(\"Padding side:\", tokenizer_loaded_fromHF.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model - OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "my_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_loaded_fromHF,\n",
    "    tokenizer=tokenizer_loaded_fromHF,\n",
    "    max_length=300,\n",
    ")\n",
    "\n",
    "result = my_pipeline(\n",
    "    \"Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,     3, 17930,  1678,  1228,  1136, 29572, 16027,  1296,  2313,\n",
      "          1452,  4704, 29491,  2592,  1279,  1136,  1841,  1452,  1229,  8277,\n",
      "         29501, 29536,  1814, 29486, 29572, 29473,     4]], device='cuda:0')\n",
      "<s>[INST] Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu? [/INST]I think it's great. I think it's great. I think it's a great physical exercise. I think it's also a great mind exercise because it's so difficult to do and it requires so much concentration. And the thing about it that I don't like about stand up is you can kind of phone it in a little bit. Like, you go up on stage, you kind of have an idea where it's going, you kind of half ass it and you kind of get by with it. But with jiu jitsu, there's no half assing. You either get your ass kicked or you don't.</s>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "# input_ids = tokenizer_loaded_fromHF.encode(\n",
    "#     \"What is AutoGen in abstract?\", return_tensors=\"pt\"\n",
    "# )\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hey how are you? Tell me something about yourself. What do you think about jiu-jitsu?\",\n",
    "    }\n",
    "]\n",
    "input_ids = tokenizer_loaded_fromHF.apply_chat_template(chat, return_tensors=\"pt\").to(\n",
    "    \"cuda\"\n",
    ")\n",
    "print(input_ids)\n",
    "\n",
    "# Generate text\n",
    "result = model_loaded_fromHF.generate(input_ids, max_length=300)\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = tokenizer_loaded_fromHF.decode(result[0])\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
