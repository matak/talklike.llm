"""
Pomocn√© funkce pro fine-tuning
"""
import os
import torch
from google.colab import drive

def setup_environment():
    """Nastav√≠ prost≈ôed√≠ pro Google Colab"""
    # P≈ôipojen√≠ Google Drive
    drive.mount('/content/drive')
    
    # Vytvo≈ôen√≠ adres√°≈ô≈Ø
    os.makedirs('/content/babis_finetune', exist_ok=True)
    os.makedirs('/content/drive/MyDrive/babis_finetune', exist_ok=True)
    
    print("Google Drive p≈ôipojen a adres√°≈ôe vytvo≈ôeny!")

def check_gpu():
    """Zkontroluje dostupnost GPU"""
    print(f"PyTorch verze: {torch.__version__}")
    print(f"CUDA dostupn√©: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU pamƒõ≈•: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("‚ö†Ô∏è  GPU nen√≠ dostupn√© - tr√©nov√°n√≠ bude pomal√©!")

def install_requirements():
    """Nainstaluje pot≈ôebn√© knihovny"""
    print("üì¶ Instaluji pot≈ôebn√© knihovny...")
    
    # Seznam knihoven
    packages = [
        "transformers",
        "datasets", 
        "peft",
        "accelerate",
        "bitsandbytes",
        "wandb",
        "tiktoken",
        "huggingface_hub",
        "gradio",
        "streamlit"
    ]
    
    for package in packages:
        print(f"Instaluji {package}...")
        os.system(f"pip install -q {package}")
    
    print("‚úÖ V≈°echny knihovny nainstalov√°ny!")

def save_model(trainer, config):
    """Ulo≈æ√≠ natr√©novan√Ω model"""
    print("üíæ Ukl√°d√°m model...")
    
    # Ulo≈æen√≠ modelu
    trainer.save_model()
    
    # Ulo≈æen√≠ tokenizeru
    trainer.tokenizer.save_pretrained(config.output_dir)
    
    print(f"‚úÖ Model ulo≈æen do: {config.output_dir}")

def generate_sample(model, tokenizer, prompt="U≈æivatel: Jak√Ω je v√°≈° n√°zor na inflaci?\nAndrej Babi≈°:", max_length=100):
    """Vygeneruje uk√°zkovou odpovƒõƒè"""
    print(f"ü§ñ Generuji odpovƒõƒè na: {prompt}")
    
    # Tokenizace promptu
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # Generov√°n√≠
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Dek√≥dov√°n√≠
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"Odpovƒõƒè: {generated_text}")
    return generated_text 