{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "%pip install peft\n",
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "%pip install trl\n",
    "%pip install matplotlib\n",
    "%pip install tensorboard\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Načte proměnné prostředí z .env souboru\n",
    "load_dotenv()\n",
    "\n",
    "# Získá API token z proměnných prostředí\n",
    "API_TOKEN = os.getenv(\"API_TOKEN\")\n",
    "\n",
    "if API_TOKEN:\n",
    "    login(token=API_TOKEN)\n",
    "else:\n",
    "    print(\"API_TOKEN nebyl nalezen. Ujistěte se, že máte v kořenovém adresáři projektu soubor .env s obsahem: API_TOKEN='váš_huggingface_token'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model from HUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\info\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Merge LoRA adapters with base model\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'peft'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Merge LoRA adapters with base model\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "print(base_model.get_input_embeddings())\n",
    "print(base_model.get_output_embeddings())\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"before\", len(base_tokenizer))\n",
    "base_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "print(\"after\", len(base_tokenizer))\n",
    "\n",
    "# viz https://huggingface.co/docs/transformers/v4.41.3/en/model_doc/llama3#usage-tips\n",
    "print(\"before 2\", base_model.config.pad_token_id)\n",
    "base_model.config.pad_token_id = base_tokenizer.pad_token_id\n",
    "print(\"after 2\", base_model.config.pad_token_id)\n",
    "\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "base_model.resize_token_embeddings(len(base_tokenizer))\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Architecture: LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128257, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128257, bias=False)\n",
      ")\n",
      "Config: LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128256,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128257\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 128257\n",
      "Input embeddings:\n",
      "Embedding(128257, 4096)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=128257, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '<pad>'}\n",
      "All tokens count: 128257\n",
      "Padding side: right\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(base_model))\n",
    "print(\"Architecture:\", base_model)\n",
    "print(\"Config:\", base_model.config)\n",
    "print(\"Model Vocabulary Size:\", base_model.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(base_model.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(base_model.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(base_tokenizer))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", base_tokenizer.special_tokens_map)\n",
    "print(\"All tokens count:\", len(base_tokenizer))\n",
    "print(\"Padding side:\", base_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model - OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is AutoGen in abstract? AutoGen is a Python-based tool that automates the process of generating a build system for a software project. It uses a configuration file to determine what files to include, and it can generate a wide variety of build systems, including Makefiles, Visual Studio projects, and Xcode projects. AutoGen is particularly useful for projects that have a large number of files, or for projects that require a custom build process. It is also useful for projects that need to be built on multiple platforms, as it can generate build systems for a wide range of platforms, including Windows, macOS, and Linux.\\n\\nWhat is AutoGen in detail? AutoGen is a Python-based tool that automates the process of generating a build system for a software project. It uses a configuration file to determine what files to include in the build system, and it can generate a wide variety of build systems, including:\\n\\n* Makefiles: AutoGen can generate Makefiles that can be used to build a project using the Make build tool.\\n* Visual Studio projects: AutoGen can generate Visual Studio projects that can be used to build a project using the Visual Studio development environment.\\n* Xcode projects: AutoGen can generate Xcode projects that can be used to build a project using the Xcode development environment.\\n* Other build systems: AutoGen can also generate build systems for other build tools, such as CMake, SCons, and Ant.\\n\\nAutoGen is particularly useful for projects that have a'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "my_pipeline = pipeline(\n",
    "    \"text-generation\", model=base_model, tokenizer=base_tokenizer, max_length=300\n",
    ")\n",
    "\n",
    "result = my_pipeline(\"What is AutoGen in abstract?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,    882, 128007,    271,   3923,    374,   9156,  10172,\n",
      "            304,   8278,     30, 128009]], device='cuda:0')\n",
      "tensor([[128000, 128006,    882, 128007,    271,   3923,    374,   9156,  10172,\n",
      "            304,   8278,     30, 128009, 128006,  78191, 128007,    271,  13556,\n",
      "          10172,    374,    264,   5507,    430,   9651,  27983,  28725,   1787,\n",
      "           2082,     11,   1778,    439,   9904,   6170,     11,   1493,  11850,\n",
      "             11,    323,   1023,  59177,   2082,     11,    304,    264,   8205,\n",
      "            315,  15840,  15823,     13,   1102,    596,   1093,    264,   2082,\n",
      "          14143,    430,   8779,    499,    636,   3940,    449,    264,    502,\n",
      "           2447,    477,   4793,    555,  21973,    304,    279,   6913,  14054,\n",
      "            382,  13556,  10172,   5829,  20506,    323,   6683,   3626,    311,\n",
      "           7068,    279,  28725,   1787,   2082,     11,  10923,    499,    311,\n",
      "          32187,    279,   2612,    311,   5052,    701,   3230,   3966,     13,\n",
      "           1115,    649,   3665,    264,   5199,   3392,    315,    892,    323,\n",
      "           5149,     11,    439,    499,    649,   5357,    389,   4477,    279,\n",
      "           5150,  12496,    323,  15293,    315,    701,   2082,     11,   4856,\n",
      "           1109,  40876,    922,    279,  69782,   3649,    382,  13556,  10172,\n",
      "            374,   3629,   1511,    304,  32546,    449,   1023,   7526,    323,\n",
      "          49125,     11,   1778,    439,   2082,  44163,     11,   1977,   6067,\n",
      "             11,    323,  29075,     82,    320,  83537,  11050,   2998,  18347,\n",
      "            570,   1102,    596,    264,   5526,   5507,   4315,  13707,     11,\n",
      "           8104,   1884,    889,    990,    389,   3544,  13230,   7224,    477,\n",
      "            617,    311,  10519,   6485,   2082,  79367,     13, 128009]],\n",
      "       device='cuda:0')\n",
      "Answer:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is AutoGen in abstract?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AutoGen is a tool that automatically generates boilerplate code, such as documentation comments, error handling, and other repetitive code, in a variety of programming languages. It's like a code generator that helps you get started with a new project or module by filling in the basic infrastructure.\n",
      "\n",
      "AutoGen uses templates and configuration files to generate the boilerplate code, allowing you to customize the output to fit your specific needs. This can save a significant amount of time and effort, as you can focus on writing the actual logic and functionality of your code, rather than worrying about the mundane details.\n",
      "\n",
      "AutoGen is often used in conjunction with other tools and frameworks, such as code generators, build systems, and IDEs (Integrated Development Environments). It's a popular tool among developers, particularly those who work on large-scale projects or have to maintain complex codebases.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "# input_ids = base_tokenizer.encode(\n",
    "#     \"What is AutoGen in abstract?\", return_tensors=\"pt\"\n",
    "# ).to(\"cuda\")\n",
    "# print(input_ids)\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": \"What is AutoGen in abstract?\"}]\n",
    "input_ids = base_tokenizer.apply_chat_template(chat, return_tensors ='pt').to(\"cuda\")\n",
    "print(input_ids)\n",
    "\n",
    "# Generate text\n",
    "result = base_model.generate(input_ids, max_length=300)\n",
    "print(result)\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = base_tokenizer.decode(result[0])\n",
    "print(\"Answer:\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model (to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SAVED_MODEL/tokenizer_config.json',\n",
       " 'SAVED_MODEL/special_tokens_map.json',\n",
       " 'SAVED_MODEL/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.save_pretrained(\"SAVED_MODEL\")\n",
    "\n",
    "base_tokenizer.save_pretrained(\"SAVED_MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the model (from HUB )to HUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58905322baa446aacb9ce91e7d5a4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ef63c690804293b176577b0afc1f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58ef0b4d707470eb26fa3459dfd488b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1772d290869465d827c3317f4b05a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lukaskellerstein/my-base-llama-4bit-from-hub/commit/afd77e4b6e548a7c58e60486c1d894ba9ff1edda', commit_message='Upload tokenizer', commit_description='', oid='afd77e4b6e548a7c58e60486c1d894ba9ff1edda', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/my-base-llama-4bit-from-hub\",\n",
    "    token=API_TOKEN,\n",
    ")\n",
    "base_tokenizer.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/my-base-llama-4bit-from-hub\",\n",
    "    token=API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hub model from HUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86be6eed6c2644adb78b09978016fb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ae325f48144f75b3bfd1e0f471446c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/132k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7242ea24615414980a365cb0eca2185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf23347219cc4fbfaee45a6fe71ffab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811c7678f5824a7994ba500290acf235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7f8e96e3da45c0917dd3c11242ccf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268bd3d533a54835a381973749aae2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/194 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b953efa5cf2c4b7fb62fee2c9856ad6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4f8e6c18fe4c6a9b059f10f0952c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d39be303cea4a408ee7149613bae2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_hub_loaded_from_hub = AutoModelForCausalLM.from_pretrained(\n",
    "    \"lukaskellerstein/my-base-llama-4bit-from-hub\", device_map=\"auto\"\n",
    ")\n",
    "tokenizer_hub_loaded_from_hub = AutoTokenizer.from_pretrained(\n",
    "    \"lukaskellerstein/my-base-llama-4bit-from-hub\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Architecture: LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128257, 4096, padding_idx=128256)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128257, bias=False)\n",
      ")\n",
      "Config: LlamaConfig {\n",
      "  \"_name_or_path\": \"lukaskellerstein/my-base-llama-4bit-from-hub\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128256,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128257\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 128257\n",
      "Input embeddings:\n",
      "Embedding(128257, 4096, padding_idx=128256)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=128257, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '<pad>'}\n",
      "All tokens count: 128257\n",
      "Padding side: right\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(model_hub_loaded_from_hub))\n",
    "print(\"Architecture:\", model_hub_loaded_from_hub)\n",
    "print(\"Config:\", model_hub_loaded_from_hub.config)\n",
    "print(\"Model Vocabulary Size:\", model_hub_loaded_from_hub.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(model_hub_loaded_from_hub.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(model_hub_loaded_from_hub.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(tokenizer_hub_loaded_from_hub))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", tokenizer_hub_loaded_from_hub.special_tokens_map)\n",
    "print(\"All tokens count:\", len(tokenizer_hub_loaded_from_hub))\n",
    "print(\"Padding side:\", tokenizer_hub_loaded_from_hub.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model - OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is AutoGen in abstract?azorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazor'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "my_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_hub_loaded_from_hub,\n",
    "    tokenizer=tokenizer_hub_loaded_from_hub,\n",
    "    max_length=300,\n",
    ")\n",
    "\n",
    "result = my_pipeline(\"What is AutoGen in abstract?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,    882, 128007,    271,   3923,    374,   9156,  10172,\n",
      "            304,   8278,     30, 128009]], device='cuda:0')\n",
      "tensor([[128000, 128006,    882, 128007,    271,   3923,    374,   9156,  10172,\n",
      "            304,   8278,     30, 128009, 128006,  78191, 128007,    271,  13556,\n",
      "          10172,    374,    264,   3241,   5507,    430,   9651,  27983,  28725,\n",
      "           1787,   2082,     11,   1778,    439,   4342,   3626,     11,   7557,\n",
      "           7346,     11,    323,   1023,  14054,   2082,     11,    369,    264,\n",
      "           3241,   2447,     13,   1102,    374,   6319,    311,   3665,  13707,\n",
      "            892,    323,   8108,    279,   3392,    315,  59177,  11058,    814,\n",
      "           1205,    311,    656,    382,  13556,  10172,   5829,    264,   3896,\n",
      "           6108,   5603,     11,   1405,    264,    743,    315,  20506,    374,\n",
      "           1511,    311,   7068,    279,  28725,   1787,   2082,     13,    578,\n",
      "          20506,    527,   5439,    304,    264,   4382,     11,   1495,   6108,\n",
      "           4221,    323,    649,    387,  32789,    311,   5052,    279,   3230,\n",
      "           3966,    315,    264,   2447,    382,  13556,  10172,    649,    387,\n",
      "           1511,    311,   7068,    264,   7029,   2134,    315,  28725,   1787,\n",
      "           2082,     11,   2737,   1473,      9,  12376,   3626,    369,    356,\n",
      "            323,    356,   1044,   7620,    198,      9,   7557,   7346,    369,\n",
      "           4857,    323,  55320,   2082,    198,      9,  45565,   3626,     11,\n",
      "           1778,    439,    893,   6959,    323,   9492,   9904,    198,      9,\n",
      "          12499,   3626,     11,   1778,    439,  12138,    477,   4823,   3626,\n",
      "            198,      9,   1628,   1690,   1023,   4595,    315,  28725,   1787,\n",
      "           2082,    271,   1383,   1701,   9156,  10172,     11,  13707,    649,\n",
      "           5357,    389,   4477,    279,   6332,  12496,    315,    872,   2068,\n",
      "             11,   4856,   1109,  10374,    892,    389,  59177,     11,  69782,\n",
      "           9256,   1093,  24038,  28725,   1787,   2082,     13, 128009]],\n",
      "       device='cuda:0')\n",
      "Answer:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is AutoGen in abstract?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AutoGen is a software tool that automatically generates boilerplate code, such as header files, Makefiles, and other infrastructure code, for a software project. It is designed to save developers time and reduce the amount of repetitive coding they need to do.\n",
      "\n",
      "AutoGen uses a template-based approach, where a set of templates is used to generate the boilerplate code. The templates are written in a simple, text-based language and can be customized to fit the specific needs of a project.\n",
      "\n",
      "AutoGen can be used to generate a wide range of boilerplate code, including:\n",
      "\n",
      "* Header files for C and C++ programs\n",
      "* Makefiles for building and compiling code\n",
      "* Documentation files, such as man pages and HTML documentation\n",
      "* Configuration files, such as XML or JSON files\n",
      "* And many other types of boilerplate code\n",
      "\n",
      "By using AutoGen, developers can focus on writing the core logic of their program, rather than spending time on repetitive, mundane tasks like generating boilerplate code.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "# input_ids = tokenizer_hub_loaded_from_hub.encode(\n",
    "#     \"What is AutoGen in abstract?\", return_tensors=\"pt\"\n",
    "# )\n",
    "# print(input_ids)\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": \"What is AutoGen in abstract?\"}]\n",
    "input_ids = base_tokenizer.apply_chat_template(chat, return_tensors ='pt').to(\"cuda\")\n",
    "print(input_ids)\n",
    "\n",
    "# Generate text\n",
    "result = model_hub_loaded_from_hub.generate(input_ids, max_length=300)\n",
    "print(result)\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = tokenizer_hub_loaded_from_hub.decode(result[0])\n",
    "print(\"Answer:\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base model from DISK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec23b0287514c068ec54ecdb6a1380a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_loaded = AutoModelForCausalLM.from_pretrained(\n",
    "    \"SAVED_MODEL\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(\"SAVED_MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Architecture: LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128257, 4096, padding_idx=128256)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128257, bias=False)\n",
      ")\n",
      "Config: LlamaConfig {\n",
      "  \"_name_or_path\": \"SAVED_MODEL\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128256,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128257\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 128257\n",
      "Input embeddings:\n",
      "Embedding(128257, 4096, padding_idx=128256)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=128257, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '<pad>'}\n",
      "All tokens count: 128257\n",
      "Padding side: right\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(model_loaded))\n",
    "print(\"Architecture:\", model_loaded)\n",
    "print(\"Config:\", model_loaded.config)\n",
    "print(\"Model Vocabulary Size:\", model_loaded.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(model_loaded.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(model_loaded.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(tokenizer_loaded))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", tokenizer_loaded.special_tokens_map)\n",
    "print(\"All tokens count:\", len(tokenizer_loaded))\n",
    "print(\"Padding side:\", tokenizer_loaded.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model - OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is AutoGen in abstract?azorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazorazor'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "my_pipeline = pipeline(\n",
    "    \"text-generation\", model=model_loaded, tokenizer=tokenizer_loaded, max_length=300\n",
    ")\n",
    "\n",
    "result = my_pipeline(\"What is AutoGen in abstract?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,    882, 128007,    271,   3923,    374,   9156,  10172,\n",
      "            304,   8278,     30, 128009]], device='cuda:0')\n",
      "tensor([[128000, 128006,    882, 128007,    271,   3923,    374,   9156,  10172,\n",
      "            304,   8278,     30, 128009, 128006,  78191, 128007,    271,  13556,\n",
      "          10172,    374,    459,   1825,  31874,   5507,    430,   9651,  27983,\n",
      "          28725,   1787,   2082,    369,   5370,  15840,  15823,     11,   2737,\n",
      "            356,     11,    356,  23240,   8102,     11,    323,   3885,     13,\n",
      "           1102,   5829,    264,   4382,     11,   9632,   1413,  20047,    311,\n",
      "           7664,    279,   2082,   9659,   1920,     11,  10923,  13707,    311,\n",
      "           5357,    389,    279,  12496,    315,    872,   2082,   4856,   1109,\n",
      "            279,  66838,   3465,    315,   4477,  59177,  28725,   1787,   2082,\n",
      "            382,    644,  28591,     11,   9156,  10172,  14385,    439,    264,\n",
      "           2082,  14143,     11,   4737,    304,    264,    743,    315,  20506,\n",
      "            323,    264,   6683,   1052,    439,   1988,     11,    323,  17843,\n",
      "            264,   4686,     11,   3318,   6710,    315,   2082,    439,   2612,\n",
      "             13,   1115,   2082,    649,   2997,   2574,   1093,   1473,      9,\n",
      "           5830,  16068,    323,  39437,    198,      9,   3308,  17931,    323,\n",
      "           4562,   5865,    198,      9,   2956,  14726,    323,  26249,    198,\n",
      "              9,   4703,  11850,    323,   4788,  24717,    198,      9,   1628,\n",
      "           1790,    810,   2268,   1383,   5113,   1113,    279,   9659,    315,\n",
      "          28725,   1787,   2082,     11,   9156,  10172,   8779,  13707,   1473,\n",
      "              9,  10467,    892,    323,   5149,    198,      9,  53253,   6103,\n",
      "            323,  92922,    198,      9,  65184,   2082,   4367,    323,  10519,\n",
      "           2968,    198,      9,  26891,    389,    279,  12496,    323,  15293,\n",
      "            315,    872,   2082,    271,  28589,     11,   9156,  10172,    374,\n",
      "            264,   8147,   5507,    430,    649,  82703,    279,   4500,   1920,\n",
      "            323,   1520,  13707,   3350,    810,  11297,     11,   7524,     11,\n",
      "            323,  10519,    481,   2082,     13, 128009]], device='cuda:0')\n",
      "Answer:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is AutoGen in abstract?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AutoGen is an open-source tool that automatically generates boilerplate code for various programming languages, including C, C++, Java, and others. It uses a simple, declarative syntax to describe the code generation process, allowing developers to focus on the logic of their code rather than the tedious task of writing repetitive boilerplate code.\n",
      "\n",
      "In essence, AutoGen acts as a code generator, taking in a set of templates and a configuration file as input, and producing a complete, working piece of code as output. This code can include things like:\n",
      "\n",
      "* Function declarations and implementations\n",
      "* Class definitions and member functions\n",
      "* Data structures and algorithms\n",
      "* Error handling and exception mechanisms\n",
      "* And much more!\n",
      "\n",
      "By automating the generation of boilerplate code, AutoGen helps developers:\n",
      "\n",
      "* Save time and effort\n",
      "* Reduce errors and inconsistencies\n",
      "* Improve code quality and maintainability\n",
      "* Focus on the logic and functionality of their code\n",
      "\n",
      "Overall, AutoGen is a powerful tool that can streamline the development process and help developers write more efficient, effective, and maintainable code.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "# input_ids = tokenizer_loaded.encode(\n",
    "#     \"What is AutoGen in abstract?\", return_tensors=\"pt\"\n",
    "# ).to(\"cuda\")\n",
    "# print(input_ids)\n",
    "\n",
    "chat = [{\"role\": \"user\", \"content\": \"What is AutoGen in abstract?\"}]\n",
    "input_ids = tokenizer_loaded.apply_chat_template(chat, return_tensors ='pt').to(\"cuda\")\n",
    "print(input_ids)\n",
    "\n",
    "# Generate text\n",
    "result = model_loaded.generate(input_ids, max_length=1000)\n",
    "print(result)\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = tokenizer_loaded.decode(result[0])\n",
    "print(\"Answer:\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push my base model from DISK to HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3255d1e673d469da15b9dc29fc5ed40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276b554e4f054965aad7cbe9dd721b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc92922b591b4b7b9b2e42c111f974be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca99d89ca4c48eeb4119dd012bdd721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lukaskellerstein/my-base-llama-8bit-from-disk/commit/54c3cc2036ee76b63dfb0a31f89eae24b4f694ae', commit_message='Upload tokenizer', commit_description='', oid='54c3cc2036ee76b63dfb0a31f89eae24b4f694ae', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/my-base-llama-8bit-from-disk\",\n",
    "    token=API_TOKEN,\n",
    ")\n",
    "tokenizer_loaded.push_to_hub(\n",
    "    repo_id=\"lukaskellerstein/my-base-llama-8bit-from-disk\",\n",
    "    token=API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk model from HUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467f3e71c2b54fdfa66fac5574d04292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b97766b946433a8a838545690aa7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/132k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5cef95235d4f9e8529ad790bd39078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c76d4bf7554ed785f8351a234b2338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f3f51b1a884e7abef01168dfcbbe14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803d1f94bf8c476f8e8b91794a7d7818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071dc7866d694fe6b97f63e899cd1d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/194 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a3aa6d0c924dadb59351352e9a0998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3d6e41c35a46e381d970ffb6181b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90227c5a489049f581615529019066d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_loaded_fromHF = AutoModelForCausalLM.from_pretrained(\n",
    "    \"lukaskellerstein/my-base-llama-4bit-from-disk\", device_map=\"auto\"\n",
    ")\n",
    "tokenizer_loaded_fromHF = AutoTokenizer.from_pretrained(\n",
    "    \"lukaskellerstein/my-base-llama-4bit-from-disk\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Model---\n",
      "Type: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "Architecture: LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128257, 4096, padding_idx=128256)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128257, bias=False)\n",
      ")\n",
      "Config: LlamaConfig {\n",
      "  \"_name_or_path\": \"lukaskellerstein/my-base-llama-8bit-from-disk\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128256,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128257\n",
      "}\n",
      "\n",
      "Model Vocabulary Size: 128257\n",
      "Input embeddings:\n",
      "Embedding(128257, 4096, padding_idx=128256)\n",
      "Output embeddings:\n",
      "Linear(in_features=4096, out_features=128257, bias=False)\n",
      "---Tokenzier---\n",
      "Type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n",
      "Special tokens: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '<pad>'}\n",
      "All tokens count: 128257\n",
      "Padding side: right\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"---Model---\")\n",
    "print(\"Type:\", type(model_loaded_fromHF))\n",
    "print(\"Architecture:\", model_loaded_fromHF)\n",
    "print(\"Config:\", model_loaded_fromHF.config)\n",
    "print(\"Model Vocabulary Size:\", model_loaded_fromHF.config.vocab_size)\n",
    "print(\"Input embeddings:\")\n",
    "print(model_loaded_fromHF.get_input_embeddings())\n",
    "print(\"Output embeddings:\")\n",
    "print(model_loaded_fromHF.get_output_embeddings())\n",
    "\n",
    "# Tokenizer\n",
    "print(\"---Tokenzier---\")\n",
    "print(\"Type:\", type(tokenizer_loaded_fromHF))\n",
    "# print(tokenizer_loaded)\n",
    "print(\"Special tokens:\", tokenizer_loaded_fromHF.special_tokens_map)\n",
    "print(\"All tokens count:\", len(tokenizer_loaded_fromHF))\n",
    "print(\"Padding side:\", tokenizer_loaded_fromHF.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test the model - OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is AutoGen in abstract?osuosuosudbaounderounderüstüounderamageounder�üstü�ounderoundingounderounder금.posterdbaounder-partamageWER�ounderosuoundingamage.posterbowerosubowerosuounder� Ampounder�osu YÖoundingblank Amp.posterounder-partounderounderosuesty�금.poster TREEónico Amp금.poster.posteraldoaldo.poster Amp Topsestyestyesty�bowerosu.poster�dba.posterounderounder.compress.poster�� Ampounderaldo.poster Amp.poster� YÖounderaldoaldo.poster Ampensaestyensaescaping�kbd.posterكال Ampblankounderesty�.posterounderblank.posterounder TREE.poster-partounderounderฤษ.compress�blankounder�blank YÖ篣�ounder.posterkbdounder.posterounder�esty� Amp� YÖ�.poster_EMIT�tsy���.poster�.poster�كالesty� chargeounder Tanks�tsy�.poster�ónico夢-partฤษescapingesty.poster пласти вдруг緣esty��� TREE.poster���ensa�opic�.poster�blank�esty���� Tanks_EMIT�� Tanks Tanksesty Tanksestyesty�opic��ounder��tsy蒙��煣�����_EMIT�esty�_EMIT��ónico� вдруг�kbdكال_EMIT�esty���� вдруг YÖ_EMIT�_EMITopic� вдруг_EMIT�.poster�tsy蒙kbd_EMIT_EMIT_EMITكال Tanks�ンジ_EMITkbdandle��ónicoescaping_EMITฤษ�كالesty�.poster�ónicoكال.posterンジ.postertsy.poster_EMIT�esty��'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "my_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_loaded_fromHF,\n",
    "    tokenizer=tokenizer_loaded_fromHF,\n",
    "    max_length=300,\n",
    ")\n",
    "\n",
    "result = my_pipeline(\"What is AutoGen in abstract?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### via Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,    882, 128007,    271,   3923,    374,   9156,  10172,\n",
      "            304,   8278,     30, 128009]], device='cuda:0')\n",
      "tensor([[128000, 128006,    882, 128007,    271,   3923,    374,   9156,  10172,\n",
      "            304,   8278,     30, 128009, 128006,  78191, 128007,    271,  13556,\n",
      "          10172,    374,    459,   1825,  31874,   5507,    430,  27983,  28725,\n",
      "           1787,   2082,    369,   5370,  15840,  15823,     11,   2737,    356,\n",
      "             11,    356,  23240,   8102,     11,  13325,     11,    323,   3885,\n",
      "             13,   1102,    596,   6319,    311,  69711,    279,   9886,    315,\n",
      "          59177,     11,  69782,   2082,    430,    374,   3629,   2631,    304,\n",
      "           3241,   4500,     11,   1778,    439,   1473,      9,   5830,  33728,\n",
      "            323,  47728,    198,      9,   4703,  11850,    323,   4788,  11850,\n",
      "           2082,    198,      9,  45565,   6170,    198,      9,  26230,   5865,\n",
      "            323,   6989,    271,  13556,  10172,   5829,  20506,    323,    264,\n",
      "           6683,   1052,    311,   7068,    279,   2082,     13,  47717,    649,\n",
      "           3350,    872,   1866,  20506,    323,  33483,    311,  52056,    279,\n",
      "           8066,   2082,    311,    872,   3230,   3966,    382,   1383,   1701,\n",
      "           9156,  10172,     11,  13707,    649,   3665,    892,    323,   8108,\n",
      "            279,   3392,    315,  59177,  11058,    814,   1205,    311,    656,\n",
      "             11,  10923,   1124,    311,   5357,    389,    810,   6485,    323,\n",
      "           7185,   9256,     13, 128009]], device='cuda:0')\n",
      "Answer:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is AutoGen in abstract?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "AutoGen is an open-source tool that generates boilerplate code for various programming languages, including C, C++, Java, Python, and others. It's designed to automate the creation of repetitive, mundane code that is often required in software development, such as:\n",
      "\n",
      "* Function signatures and prototypes\n",
      "* Error handling and exception handling code\n",
      "* Documentation comments\n",
      "* Utility functions and classes\n",
      "\n",
      "AutoGen uses templates and a configuration file to generate the code. Developers can write their own templates and configurations to tailor the generated code to their specific needs.\n",
      "\n",
      "By using AutoGen, developers can save time and reduce the amount of repetitive coding they need to do, allowing them to focus on more complex and interesting tasks.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "# input_ids = tokenizer_loaded_fromHF.encode(\n",
    "#     \"What is AutoGen in abstract?\", return_tensors=\"pt\"\n",
    "# ).to(\"cuda\")\n",
    "# print(input_ids)\n",
    "chat = [{\"role\": \"user\", \"content\": \"What is AutoGen in abstract?\"}]\n",
    "input_ids = tokenizer_loaded_fromHF.apply_chat_template(chat, return_tensors ='pt').to(\"cuda\")\n",
    "print(input_ids)\n",
    "\n",
    "# Generate text\n",
    "result = model_loaded_fromHF.generate(input_ids, max_length=300)\n",
    "print(result)\n",
    "\n",
    "# Decode and print the generated text\n",
    "output_text = tokenizer_loaded_fromHF.decode(result[0])\n",
    "print(\"Answer:\")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
