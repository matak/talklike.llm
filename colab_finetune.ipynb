{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ü§ñ Fine-tuning Babi≈°ova stylu - Google Colab\n",
    "\n",
    "Tento notebook implementuje kompletn√≠ pipeline pro fine-tuning jazykov√©ho modelu na napodoben√≠ charakteristick√©ho stylu komunikace Andreje Babi≈°e.\n",
    "\n",
    "## üéØ C√≠l\n",
    "Vytvo≈ôit model, kter√Ω dok√°≈æe generovat satirick√© odpovƒõdi ve stylu \"bab√≠≈°ov≈°tiny\" - charakteristick√©ho jazykov√©ho stylu s mluvenou ƒçe≈°tinou, slovensko-ƒçesk√Ωmi odchylkami a specifick√Ωmi r√©torick√Ωmi prvky.\n",
    "\n",
    "## üöÄ V√Ωhody Google Colab\n",
    "- ‚úÖ **Zdarma GPU** (Tesla T4/P100)\n",
    "- ‚úÖ **≈Ω√°dn√° instalace** - v≈°e v prohl√≠≈æeƒçi\n",
    "- ‚úÖ **Automatick√© ukl√°d√°n√≠** na Google Drive\n",
    "- ‚úÖ **Sd√≠len√≠** v√Ωsledk≈Ø p≈ôes Hugging Face Hub\n",
    "- ‚úÖ **Monitoring** p≈ôes Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ Instalace a setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Instalace pot≈ôebn√Ωch knihoven\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes wandb tiktoken\n",
    "!pip install -q huggingface_hub gradio streamlit\n",
    "\n",
    "# Restart runtime po instalaci\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Import knihoven\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "from typing import Optional, Dict, Any\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import wandb\n",
    "from huggingface_hub import HfApi, login\n",
    "from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Nastaven√≠ logov√°n√≠\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch verze: {torch.__version__}\")\n",
    "print(f\"CUDA dostupn√©: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU pamƒõ≈•: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# P≈ôipojen√≠ Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Vytvo≈ôen√≠ adres√°≈ô≈Ø\n",
    "!mkdir -p /content/babis_finetune\n",
    "!mkdir -p /content/drive/MyDrive/babis_finetune\n",
    "\n",
    "print(\"Google Drive p≈ôipojen a adres√°≈ôe vytvo≈ôeny!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## ‚öôÔ∏è Konfigurace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_class"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.8' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/info/AppData/Local/Programs/Python/Python312/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Konfigurace pro fine-tuning Babi≈°ova stylu\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class ColabConfig:\n",
    "    \"\"\"Konfigurace pro Google Colab fine-tuning\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    base_model: str = \"microsoft/DialoGPT-medium\"  # Mal√Ω model pro Colab\n",
    "    model_name: str = \"babis-dialogpt-colab\"\n",
    "    \n",
    "    # Training settings (optimalizov√°no pro Colab GPU)\n",
    "    learning_rate: float = 2e-4\n",
    "    num_train_epochs: int = 2\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    max_grad_norm: float = 0.3\n",
    "    warmup_steps: int = 50\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 200\n",
    "    eval_steps: int = 200\n",
    "    \n",
    "    # LoRA settings\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.1\n",
    "    target_modules: List[str] = None\n",
    "    \n",
    "    # Dataset settings\n",
    "    max_seq_length: int = 512\n",
    "    train_split: float = 0.9\n",
    "    eval_split: float = 0.1\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir: str = \"/content/babis_finetune\"\n",
    "    logging_dir: str = \"/content/babis_finetune/logs\"\n",
    "    \n",
    "    # Hardware settings (optimalizov√°no pro Colab)\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False\n",
    "    use_8bit: bool = False\n",
    "    use_4bit: bool = True\n",
    "    \n",
    "    # Evaluation settings\n",
    "    evaluation_strategy: str = \"steps\"\n",
    "    save_strategy: str = \"steps\"\n",
    "    load_best_model_at_end: bool = True\n",
    "    metric_for_best_model: str = \"eval_loss\"\n",
    "    greater_is_better: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            # Pro DialoGPT\n",
    "            self.target_modules = [\"c_attn\", \"c_proj\", \"wte\", \"wpe\"]\n",
    "        \n",
    "        # Vytvo≈ôen√≠ adres√°≈ô≈Ø\n",
    "        import os\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.logging_dir, exist_ok=True)\n",
    "\n",
    "# Vytvo≈ôen√≠ konfigurace\n",
    "config = ColabConfig()\n",
    "print(\"Konfigurace vytvo≈ôena:\")\n",
    "print(f\"Base model: {config.base_model}\")\n",
    "print(f\"Output dir: {config.output_dir}\")\n",
    "print(f\"LoRA r: {config.lora_r}, alpha: {config.lora_alpha}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset"
   },
   "source": [
    "## üìä P≈ô√≠prava datasetu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modul pro naƒç√≠t√°n√≠ a zpracov√°n√≠ datasetu\n",
    "\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_babis_dataset():\n",
    "    \"\"\"Naƒçte skuteƒçn√Ω dataset s Babi≈°ov√Ωmi v√Ωroky z JSONL soubor≈Ø\"\"\"\n",
    "    \n",
    "    # Cesta k soubor≈Øm s daty\n",
    "    data_dir = Path(\"final\")\n",
    "    jsonl_files = list(data_dir.glob(\"batch_*_babis_output_qa.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        raise FileNotFoundError(f\"Nenalezeny ≈æ√°dn√© JSONL soubory v adres√°≈ôi {data_dir}\")\n",
    "    \n",
    "    conversations = []\n",
    "    \n",
    "    # Naƒçten√≠ v≈°ech soubor≈Ø\n",
    "    for file_path in jsonl_files:\n",
    "        print(f\"Naƒç√≠t√°m {file_path.name}...\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    question = data.get('question', '')\n",
    "                    answer = data.get('answer', '')\n",
    "                    \n",
    "                    # Vytvo≈ôen√≠ konverzace ve form√°tu pro fine-tuning\n",
    "                    conversation = {\n",
    "                        \"prompt\": question,\n",
    "                        \"completion\": answer,\n",
    "                        \"full_conversation\": f\"U≈æivatel: {question}\\nAndrej Babi≈°: {answer}\"\n",
    "                    }\n",
    "                    conversations.append(conversation)\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Chyba p≈ôi parsov√°n√≠ JSON na ≈ô√°dku {line_num} v souboru {file_path.name}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"Celkem naƒçteno {len(conversations)} konverzac√≠ z {len(jsonl_files)} soubor≈Ø\")\n",
    "    \n",
    "    if len(conversations) == 0:\n",
    "        raise ValueError(\"Nebyla naƒçtena ≈æ√°dn√° konverzace z JSONL soubor≈Ø\")\n",
    "    \n",
    "    # Rozdƒõlen√≠ na train/validation\n",
    "    train_data, eval_data = train_test_split(conversations, train_size=0.9, random_state=42)\n",
    "    \n",
    "    # Vytvo≈ôen√≠ Dataset objekt≈Ø\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    eval_dataset = Dataset.from_list(eval_data)\n",
    "    \n",
    "    # Vytvo≈ôen√≠ DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': eval_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# Vytvo≈ôen√≠ datasetu\n",
    "dataset = create_babis_dataset()\n",
    "print(f\"Dataset vytvo≈ôen:\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"\\nP≈ô√≠klad dat:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## ü§ñ Naƒçten√≠ modelu a tokenizeru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Naƒçten√≠ tokenizeru\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "\n",
    "# P≈ôid√°n√≠ padding tokenu pokud chyb√≠\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer naƒçten: {config.base_model}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model_quantized"
   },
   "outputs": [],
   "source": [
    "# Naƒçten√≠ modelu s kvantizac√≠ (optimalizov√°no pro Colab GPU)\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.float16 if config.fp16 else torch.float32,\n",
    "    \"device_map\": \"auto\" if torch.cuda.is_available() else None\n",
    "}\n",
    "\n",
    "# Kvantizace pro √∫sporu pamƒõti\n",
    "if config.use_4bit:\n",
    "    model_kwargs[\"load_in_4bit\"] = True\n",
    "elif config.use_8bit:\n",
    "    model_kwargs[\"load_in_8bit\"] = True\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model,\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# P≈ô√≠prava modelu pro kvantizovan√© tr√©nov√°n√≠\n",
    "if config.use_4bit or config.use_8bit:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Model naƒçten: {config.base_model}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lora"
   },
   "source": [
    "## üéØ Nastaven√≠ LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_lora"
   },
   "outputs": [],
   "source": [
    "# Nastaven√≠ LoRA konfigurace\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Aplikace LoRA na model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# V√Ωpis tr√©novateln√Ωch parametr≈Ø\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA konfigurace nastavena!\")\n",
    "print(f\"LoRA r: {config.lora_r}\")\n",
    "print(f\"LoRA alpha: {config.lora_alpha}\")\n",
    "print(f\"Target modules: {config.target_modules}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tokenization"
   },
   "source": [
    "## üî§ Tokenizace datasetu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tokenize_dataset"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenizuje texty v datasetu\"\"\"\n",
    "    # Pou≈æ√≠v√°me full_conversation pro tr√©nov√°n√≠\n",
    "    texts = examples['full_conversation']\n",
    "    \n",
    "    # Tokenizace\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=config.max_seq_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Nastaven√≠ labels na input_ids pro causal LM\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Tokenizace datasetu\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Dataset tokenizov√°n!\")\n",
    "print(f\"Train samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(tokenized_dataset['validation'])}\")\n",
    "print(f\"\\nP≈ô√≠klad tokenizovan√Ωch dat:\")\n",
    "print(f\"Input shape: {tokenized_dataset['train'][0]['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {tokenized_dataset['train'][0]['labels'].shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üèãÔ∏è Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "outputs": [],
   "source": [
    "# Nastaven√≠ Weights & Biases (voliteln√©)\n",
    "try:\n",
    "    wandb.init(\n",
    "        project=\"babis-finetune-colab\",\n",
    "        name=config.model_name,\n",
    "        config=vars(config)\n",
    "    )\n",
    "    print(\"Weights & Biases inicializov√°no\")\n",
    "except Exception as e:\n",
    "    print(f\"WandB inicializace selhala: {e}\")\n",
    "    wandb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_args"
   },
   "outputs": [],
   "source": [
    "# Nastaven√≠ training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    max_grad_norm=config.max_grad_norm,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    logging_steps=config.logging_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    eval_steps=config.eval_steps,\n",
    "    evaluation_strategy=config.evaluation_strategy,\n",
    "    save_strategy=config.save_strategy,\n",
    "    load_best_model_at_end=config.load_best_model_at_end,\n",
    "    metric_for_best_model=config.metric_for_best_model,\n",
    "    greater_is_better=config.greater_is_better,\n",
    "    fp16=config.fp16,\n",
    "    bf16=config.bf16,\n",
    "    logging_dir=config.logging_dir,\n",
    "    report_to=\"wandb\" if wandb else None,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=3,\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "print(\"Training arguments nastaveny:\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Epochs: {config.num_train_epochs}\")\n",
    "print(f\"Batch size: {config.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_trainer"
   },
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"Trainer nastaven!\")\n",
    "print(f\"Train samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(tokenized_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [],
   "source": [
    "# Spu≈°tƒõn√≠ tr√©nov√°n√≠\n",
    "print(\"üöÄ Zaƒç√≠n√°m fine-tuning...\")\n",
    "print(f\"Model: {config.base_model}\")\n",
    "print(f\"Dataset: {len(tokenized_dataset['train'])} train, {len(tokenized_dataset['validation'])} validation\")\n",
    "print(f\"Epochs: {config.num_train_epochs}\")\n",
    "print(f\"LoRA r: {config.lora_r}, alpha: {config.lora_alpha}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Tr√©nov√°n√≠\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuning dokonƒçen!\")\n",
    "print(f\"Training loss: {train_result.metrics.get('train_loss', 'N/A')}\")\n",
    "print(f\"Training time: {train_result.metrics.get('train_runtime', 'N/A')}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation"
   },
   "outputs": [],
   "source": [
    "# Evaluace modelu\n",
    "print(\"üìä Spou≈°t√≠m evaluaci...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìà V√Ωsledky evaluace:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Ulo≈æen√≠ metrik\n",
    "trainer.log_metrics(\"eval\", eval_results)\n",
    "trainer.save_metrics(\"eval\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## üíæ Ulo≈æen√≠ modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Ulo≈æen√≠ modelu\n",
    "print(\"üíæ Ukl√°d√°m model...\")\n",
    "\n",
    "# Ulo≈æen√≠ na Colab\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "# Ulo≈æen√≠ na Google Drive\n",
    "drive_path = f\"/content/drive/MyDrive/babis_finetune/{config.model_name}\"\n",
    "!cp -r {config.output_dir} {drive_path}\n",
    "\n",
    "print(f\"‚úÖ Model ulo≈æen:\")\n",
    "print(f\"Colab: {config.output_dir}\")\n",
    "print(f\"Google Drive: {drive_path}\")\n",
    "\n",
    "# V√Ωpis velikosti modelu\n",
    "!du -sh {config.output_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "push_hub"
   },
   "source": [
    "## üöÄ Push na Hugging Face Hub (voliteln√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_hf"
   },
   "outputs": [],
   "source": [
    "# Nastaven√≠ Hugging Face tokenu\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"Pro push na Hugging Face Hub pot≈ôebujete token.\")\n",
    "print(\"Z√≠skejte ho na: https://huggingface.co/settings/tokens\")\n",
    "print(\"Pokud nechcete pushovat, stisknƒõte Enter.\")\n",
    "\n",
    "hf_token = getpass(\"HF Token (voliteln√©): \")\n",
    "\n",
    "if hf_token.strip():\n",
    "    login(hf_token)\n",
    "    print(\"‚úÖ P≈ôihl√°≈°eno k Hugging Face Hub\")\n",
    "    \n",
    "    # Push na Hub\n",
    "    print(\"üöÄ Pushuji model na Hub...\")\n",
    "    trainer.push_to_hub(f\"babis-{config.model_name}\")\n",
    "    tokenizer.push_to_hub(f\"babis-{config.model_name}\")\n",
    "    \n",
    "    print(f\"‚úÖ Model pushnut na Hub: babis-{config.model_name}\")\n",
    "    print(f\"URL: https://huggingface.co/babis-{config.model_name}\")\n",
    "else:\n",
    "    print(\"Model nebyl pushnut na Hub.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test"
   },
   "source": [
    "## üß™ Testov√°n√≠ modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "# Funkce pro generov√°n√≠ odpovƒõd√≠\n",
    "def generate_babis_response(prompt, max_length=100, temperature=0.7):\n",
    "    \"\"\"Vygeneruje odpovƒõƒè ve stylu Babi≈°e\"\"\"\n",
    "    \n",
    "    # Tokenizace promptu\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=config.max_seq_length)\n",
    "    \n",
    "    # Generov√°n√≠\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Dek√≥dov√°n√≠\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Odstranƒõn√≠ p≈Øvodn√≠ho promptu\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Testovac√≠ prompty\n",
    "test_prompts = [\n",
    "    \"U≈æivatel: Jak√Ω je v√°≈° n√°zor na inflaci?\\nAndrej Babi≈°: \",\n",
    "    \"U≈æivatel: Co si mysl√≠te o Bruselu?\\nAndrej Babi≈°: \",\n",
    "    \"U≈æivatel: Jak hodnot√≠te opozici?\\nAndrej Babi≈°: \",\n",
    "    \"U≈æivatel: Jak√© m√°te pl√°ny?\\nAndrej Babi≈°: \"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testov√°n√≠ fine-tuned modelu:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generate_babis_response(prompt)\n",
    "    print(f\"\\nPrompt: {prompt.strip()}\")\n",
    "    print(f\"Odpovƒõƒè: {response}\")\n",
    "    print(\"-\" * 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gradio"
   },
   "source": [
    "## üåê Gradio Interface (voliteln√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gradio_interface"
   },
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ Gradio interface\n",
    "import gradio as gr\n",
    "\n",
    "def babis_chat(message, history):\n",
    "    \"\"\"Chat funkce pro Gradio\"\"\"\n",
    "    prompt = f\"U≈æivatel: {message}\\nAndrej Babi≈°: \"\n",
    "    response = generate_babis_response(prompt, max_length=150, temperature=0.8)\n",
    "    return response\n",
    "\n",
    "# Vytvo≈ôen√≠ interface\n",
    "iface = gr.ChatInterface(\n",
    "    fn=babis_chat,\n",
    "    title=\"ü§ñ Babi≈° Chat Bot\",\n",
    "    description=\"Fine-tuned model ve stylu Andreje Babi≈°e\",\n",
    "    examples=[\n",
    "        [\"Jak√Ω je v√°≈° n√°zor na inflaci?\"],\n",
    "        [\"Co si mysl√≠te o Bruselu?\"],\n",
    "        [\"Jak hodnot√≠te opozici?\"],\n",
    "        [\"Jak√© m√°te pl√°ny do budoucna?\"],\n",
    "        [\"Jak tr√°v√≠te ƒças s rodinou?\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Spu≈°tƒõn√≠ interface\n",
    "iface.launch(share=True, debug=True)\n",
    "\n",
    "print(\"üåê Gradio interface spu≈°tƒõn!\")\n",
    "print(\"Sd√≠lejte odkaz s ostatn√≠mi pro testov√°n√≠.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìä Shrnut√≠ v√Ωsledk≈Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results_summary"
   },
   "outputs": [],
   "source": [
    "# Shrnut√≠ v√Ωsledk≈Ø\n",
    "print(\"üéâ FINE-TUNING DOKONƒåEN!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {config.base_model}\")\n",
    "print(f\"Fine-tuned model: {config.model_name}\")\n",
    "print(f\"Training loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "print(f\"Evaluation loss: {eval_results.get('eval_loss', 'N/A'):.4f}\")\n",
    "print(f\"Training time: {train_result.metrics.get('train_runtime', 'N/A'):.1f}s\")\n",
    "print(f\"LoRA parameters: r={config.lora_r}, alpha={config.lora_alpha}\")\n",
    "print(f\"Dataset size: {len(tokenized_dataset['train'])} train, {len(tokenized_dataset['validation'])} validation\")\n",
    "print(\"\\nüìÅ Ulo≈æen√© soubory:\")\n",
    "print(f\"Colab: {config.output_dir}\")\n",
    "print(f\"Google Drive: /content/drive/MyDrive/babis_finetune/{config.model_name}\")\n",
    "\n",
    "if hf_token.strip():\n",
    "    print(f\"Hugging Face Hub: babis-{config.model_name}\")\n",
    "\n",
    "print(\"\\nüöÄ Dal≈°√≠ kroky:\")\n",
    "print(\"1. St√°hnƒõte model z Google Drive\")\n",
    "print(\"2. Pou≈æijte ho ve vlastn√≠ aplikaci\")\n",
    "print(\"3. Sd√≠lejte v√Ωsledky s ostatn√≠mi\")\n",
    "print(\"4. Experimentujte s r≈Øzn√Ωmi prompty\")\n",
    "\n",
    "print(\"\\n‚úÖ Hotovo! Model je p≈ôipraven k pou≈æit√≠.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
