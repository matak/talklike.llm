# ğŸ¤– Google Colab Fine-tuning pro BabiÅ¡Å¯v styl

Tento projekt implementuje kompletnÃ­ pipeline pro fine-tuning jazykovÃ©ho modelu na napodobenÃ­ charakteristickÃ©ho stylu komunikace Andreje BabiÅ¡e pomocÃ­ **Google Colab** - zdarma a bez nutnosti vlastnÃ­ho hardware.

## ğŸ¯ ProÄ Google Colab?

| VÃ½hoda | Popis |
|--------|-------|
| ğŸ†“ **Zdarma** | GPU Tesla T4/P100 bez poplatkÅ¯ |
| ğŸš€ **RychlÃ©** | Å½Ã¡dnÃ¡ instalace, vÅ¡e v prohlÃ­Å¾eÄi |
| ğŸ’¾ **AutomatickÃ© uklÃ¡dÃ¡nÃ­** | Model se uklÃ¡dÃ¡ na Google Drive |
| ğŸŒ **SdÃ­lenÃ­** | SnadnÃ© sdÃ­lenÃ­ pÅ™es Hugging Face Hub |
| ğŸ“Š **Monitoring** | Integrace s Weights & Biases |

## ğŸš€ RychlÃ½ start

### 1. OtevÅ™ete Google Colab

```bash
# KliknÄ›te na tento odkaz:
https://colab.research.google.com/github/your-username/talklike.llm/blob/main/colab_finetune.ipynb
```

### 2. SpusÅ¥te notebook

1. **OtevÅ™ete** `colab_finetune.ipynb` v Google Colab
2. **ZmÄ›Åˆte runtime** na GPU: `Runtime` â†’ `Change runtime type` â†’ `GPU`
3. **SpusÅ¥te vÅ¡echny buÅˆky** postupnÄ› od zaÄÃ¡tku

### 3. VÃ½sledky

Po dokonÄenÃ­ budete mÃ­t:
- âœ… Fine-tuned model uloÅ¾enÃ½ na Google Drive
- âœ… Model pushnutÃ½ na Hugging Face Hub (volitelnÃ©)
- âœ… Gradio interface pro testovÃ¡nÃ­
- âœ… KompletnÃ­ metriky a logy

## ğŸ“‹ Co notebook dÄ›lÃ¡

### 1. **Setup a instalace**
```python
# AutomatickÃ¡ instalace knihoven
!pip install transformers datasets peft accelerate bitsandbytes wandb
```

### 2. **Konfigurace pro Colab**
```python
@dataclass
class ColabConfig:
    base_model: str = "microsoft/DialoGPT-medium"  # OptimalizovÃ¡no pro Colab
    learning_rate: float = 2e-4
    num_train_epochs: int = 2
    per_device_train_batch_size: int = 2  # Pro T4 GPU
    use_4bit: bool = True  # Kvantizace pro Ãºsporu pamÄ›ti
```

### 3. **VytvoÅ™enÃ­ datasetu**
- UkÃ¡zkovÃ½ dataset s BabiÅ¡ovÃ½mi vÃ½roky
- AutomatickÃ© rozdÄ›lenÃ­ na train/validation
- Tokenizace pro fine-tuning

### 4. **Fine-tuning s LoRA**
```python
lora_config = LoraConfig(
    r=8,                    # Rank (menÅ¡Ã­ pro Colab)
    lora_alpha=16,          # Scaling faktor
    target_modules=["c_attn", "c_proj", "wte", "wpe"],
    task_type=TaskType.CAUSAL_LM
)
```

### 5. **TrÃ©novÃ¡nÃ­ a evaluace**
- AutomatickÃ© sledovÃ¡nÃ­ metrik
- Early stopping
- UloÅ¾enÃ­ nejlepÅ¡Ã­ho modelu

### 6. **TestovÃ¡nÃ­ a deployment**
- GenerovÃ¡nÃ­ odpovÄ›dÃ­
- Gradio interface
- Push na Hugging Face Hub

## ğŸ”§ Konfigurace

### ZÃ¡kladnÃ­ parametry (optimalizovanÃ© pro Colab):

```python
config = ColabConfig(
    base_model="microsoft/DialoGPT-medium",  # MalÃ½ model pro rychlÃ© trÃ©novÃ¡nÃ­
    num_train_epochs=2,                      # KrÃ¡tkÃ© trÃ©novÃ¡nÃ­
    per_device_train_batch_size=2,           # Pro T4 GPU
    gradient_accumulation_steps=8,           # EfektivnÃ­ batch size = 16
    use_4bit=True,                           # Kvantizace pro Ãºsporu pamÄ›ti
    lora_r=8,                                # MenÅ¡Ã­ LoRA pro rychlost
    max_seq_length=512                       # KratÅ¡Ã­ sekvence
)
```

### Pro vÄ›tÅ¡Ã­ modely:

```python
# Pro Llama-2 (vyÅ¾aduje vÃ­ce pamÄ›ti)
config.base_model = "meta-llama/Llama-2-7b-chat-hf"
config.per_device_train_batch_size = 1
config.gradient_accumulation_steps = 16
config.max_seq_length = 1024
```

## ğŸ“Š Dataset

### UkÃ¡zkovÃ½ dataset obsahuje:

```python
conversations = [
    {
        "prompt": "UÅ¾ivatel: JakÃ½ je vÃ¡Å¡ nÃ¡zor na inflaci?\nAndrej BabiÅ¡: ",
        "completion": "Hele, inflace je jak kdyÅ¾ krÃ¡va hraje na klavÃ­r. JÃ¡ makÃ¡m, ale Brusel to sabotuje. Andrej BabiÅ¡"
    },
    {
        "prompt": "UÅ¾ivatel: Co si myslÃ­te o opozici?\nAndrej BabiÅ¡: ",
        "completion": "Opozice jen krade a sabotuje. JÃ¡ s rodinou makÃ¡me, ale oni jen Å™eÄnÃ­. To je skandÃ¡l! Andrej BabiÅ¡"
    }
    # ... dalÅ¡Ã­ pÅ™Ã­klady
]
```

### VlastnÃ­ dataset:

MÅ¯Å¾ete nahradit ukÃ¡zkovÃ½ dataset vlastnÃ­mi daty:

```python
# NaÄtenÃ­ vlastnÃ­ho JSONL souboru
from datasets import load_dataset

dataset = load_dataset('json', data_files='your_data.jsonl')
```

## ğŸ¯ Fine-tuning s LoRA

### VÃ½hody LoRA pro Colab:

- âœ… **RychlejÅ¡Ã­ trÃ©novÃ¡nÃ­** - mÃ©nÄ› parametrÅ¯
- âœ… **MenÅ¡Ã­ pamÄ›Å¥ovÃ© nÃ¡roky** - vhodnÃ© pro T4 GPU
- âœ… **ZachovÃ¡nÃ­ pÅ¯vodnÃ­ch schopnostÃ­** - pouze adaptace
- âœ… **SnadnÃ© sdÃ­lenÃ­** - pouze LoRA adaptace

### LoRA konfigurace:

```python
lora_config = LoraConfig(
    r=8,                    # Rank matice (menÅ¡Ã­ = rychlejÅ¡Ã­)
    lora_alpha=16,          # Scaling faktor
    target_modules=["c_attn", "c_proj"],  # KterÃ© vrstvy adaptovat
    lora_dropout=0.1,       # Dropout pro regularizaci
    bias="none",            # NepÅ™idÃ¡vat bias
    task_type=TaskType.CAUSAL_LM
)
```

## ğŸ“ˆ Monitoring a metriky

### Weights & Biases integrace:

```python
wandb.init(
    project="babis-finetune-colab",
    name=config.model_name,
    config=vars(config)
)
```

### SledovanÃ© metriky:

- **Training loss** - ztrÃ¡ta bÄ›hem trÃ©novÃ¡nÃ­
- **Evaluation loss** - ztrÃ¡ta na validaÄnÃ­ch datech
- **Training time** - celkovÃ½ Äas trÃ©novÃ¡nÃ­
- **GPU utilization** - vyuÅ¾itÃ­ GPU

## ğŸ§ª TestovÃ¡nÃ­ modelu

### Funkce pro generovÃ¡nÃ­:

```python
def generate_babis_response(prompt, max_length=100, temperature=0.7):
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            do_sample=True
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### TestovacÃ­ prompty:

```python
test_prompts = [
    "UÅ¾ivatel: JakÃ½ je vÃ¡Å¡ nÃ¡zor na inflaci?\nAndrej BabiÅ¡: ",
    "UÅ¾ivatel: Co si myslÃ­te o Bruselu?\nAndrej BabiÅ¡: ",
    "UÅ¾ivatel: Jak hodnotÃ­te opozici?\nAndrej BabiÅ¡: "
]
```

## ğŸŒ Gradio Interface

### AutomatickÃ© vytvoÅ™enÃ­ webovÃ©ho rozhranÃ­:

```python
import gradio as gr

def babis_chat(message, history):
    prompt = f"UÅ¾ivatel: {message}\nAndrej BabiÅ¡: "
    return generate_babis_response(prompt)

iface = gr.ChatInterface(
    fn=babis_chat,
    title="ğŸ¤– BabiÅ¡ Chat Bot",
    examples=["JakÃ½ je vÃ¡Å¡ nÃ¡zor na inflaci?"]
)

iface.launch(share=True)
```

## ğŸ’¾ UloÅ¾enÃ­ a sdÃ­lenÃ­

### 1. Google Drive:

```python
# AutomatickÃ© uloÅ¾enÃ­ na Drive
drive_path = f"/content/drive/MyDrive/babis_finetune/{config.model_name}"
!cp -r {config.output_dir} {drive_path}
```

### 2. Hugging Face Hub:

```python
# Push na Hub (vyÅ¾aduje token)
trainer.push_to_hub(f"babis-{config.model_name}")
tokenizer.push_to_hub(f"babis-{config.model_name}")
```

### 3. StÃ¡hnutÃ­ modelu:

```bash
# Z Google Drive
# 1. OtevÅ™ete Google Drive
# 2. PÅ™ejdÄ›te do /MyDrive/babis_finetune/
# 3. StÃ¡hnÄ›te sloÅ¾ku s modelem

# Z Hugging Face Hub
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("your-username/babis-model")
model = AutoModelForCausalLM.from_pretrained("your-username/babis-model")
```

## ğŸ› ï¸ Å˜eÅ¡enÃ­ problÃ©mÅ¯

### ÄŒastÃ© chyby:

1. **Out of Memory**:
   ```python
   # SniÅ¾te batch size
   config.per_device_train_batch_size = 1
   config.gradient_accumulation_steps = 16
   ```

2. **PomalÃ© trÃ©novÃ¡nÃ­**:
   ```python
   # Zkontrolujte GPU
   !nvidia-smi
   # PouÅ¾ijte menÅ¡Ã­ model
   config.base_model = "microsoft/DialoGPT-small"
   ```

3. **Chyba pÅ™i push na Hub**:
   ```python
   # Zkontrolujte token
   from huggingface_hub import login
   login("your-token")
   ```

### Optimalizace pro Colab:

```python
# Pro rychlejÅ¡Ã­ trÃ©novÃ¡nÃ­
config.use_4bit = True
config.fp16 = True
config.lora_r = 4
config.max_seq_length = 256

# Pro lepÅ¡Ã­ kvalitu
config.num_train_epochs = 5
config.learning_rate = 1e-4
config.lora_r = 16
```

## ğŸ“Š VÃ½sledky

### TypickÃ© vÃ½sledky na Colab T4:

```
ğŸ‰ FINE-TUNING DOKONÄŒEN!
==================================================
Model: microsoft/DialoGPT-medium
Fine-tuned model: babis-dialogpt-colab
Training loss: 2.3456
Evaluation loss: 2.1234
Training time: 45.2s
LoRA parameters: r=8, alpha=16
Dataset size: 6 train, 2 validation

ğŸ“ UloÅ¾enÃ© soubory:
Colab: /content/babis_finetune
Google Drive: /content/drive/MyDrive/babis_finetune/babis-dialogpt-colab
Hugging Face Hub: babis-babis-dialogpt-colab
```

## ğŸ”® RozÅ¡Ã­Å™enÃ­

### MoÅ¾nosti vylepÅ¡enÃ­:

1. **VÄ›tÅ¡Ã­ dataset**:
   ```python
   # NaÄtenÃ­ vÄ›tÅ¡Ã­ho datasetu
   dataset = load_dataset('json', data_files='large_babis_dataset.jsonl')
   ```

2. **VÄ›tÅ¡Ã­ model**:
   ```python
   # PouÅ¾itÃ­ vÄ›tÅ¡Ã­ho modelu
   config.base_model = "meta-llama/Llama-2-7b-chat-hf"
   ```

3. **QLoRA**:
   ```python
   # KvantizovanÃ© LoRA
   config.use_4bit = True
   config.use_qlora = True
   ```

4. **RLHF**:
   ```python
   # Reinforcement Learning from Human Feedback
   # VyÅ¾aduje dodateÄnou implementaci
   ```

## ğŸ“š Reference

- [Google Colab](https://colab.research.google.com/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [PEFT Documentation](https://huggingface.co/docs/peft/)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [Gradio](https://gradio.app/)

## ğŸ¯ ZÃ¡vÄ›r

Google Colab poskytuje ideÃ¡lnÃ­ prostÅ™edÃ­ pro fine-tuning jazykovÃ½ch modelÅ¯:

- âœ… **Zdarma** - Å¾Ã¡dnÃ© nÃ¡klady na hardware
- âœ… **RychlÃ©** - GPU akcelerace
- âœ… **JednoduchÃ©** - vÅ¡e v prohlÃ­Å¾eÄi
- âœ… **SdÃ­lenÃ©** - snadnÃ© sdÃ­lenÃ­ vÃ½sledkÅ¯
- âœ… **Å kÃ¡lovatelnÃ©** - moÅ¾nost upgrade na Pro

Tento pÅ™Ã­stup je ideÃ¡lnÃ­ pro:
- ğŸ“ **VzdÄ›lÃ¡vÃ¡nÃ­** - demonstrace fine-tuningu
- ğŸ”¬ **VÃ½zkum** - rychlÃ© experimenty
- ğŸš€ **PrototypovÃ¡nÃ­** - testovÃ¡nÃ­ nÃ¡padÅ¯
- ğŸ“± **Deployment** - pÅ™Ã­prava pro produkci

---

**PoznÃ¡mka**: Tento projekt je vytvoÅ™en pro vzdÄ›lÃ¡vacÃ­ ÃºÄely a demonstraci moÅ¾nostÃ­ fine-tuningu. SatirickÃ½ obsah je urÄen pouze pro vÃ½zkumnÃ© ÃºÄely. 