#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test integrace adaptÃ©ru pro TalkLike.LLM
KompletnÃ­ test vÅ¡ech komponent benchmarkingu s reÃ¡lnÃ½m adaptÃ©rem
"""

# Import a nastavenÃ­ prostÅ™edÃ­
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
import setup_environment

import json
import os
import time
from datetime import datetime

def test_environment_setup():
    """Test nastavenÃ­ prostÅ™edÃ­"""
    print("ğŸ”§ Test nastavenÃ­ prostÅ™edÃ­...")
    
    # Kontrola cache promÄ›nnÃ½ch
    cache_vars = ['HF_HOME', 'TRANSFORMERS_CACHE', 'HF_DATASETS_CACHE']
    for var in cache_vars:
        value = os.environ.get(var)
        if value and '/workspace' in value:
            print(f"   âœ… {var}: {value}")
        else:
            print(f"   âŒ {var}: {value}")
            return False
    
    # Kontrola cache adresÃ¡Å™Å¯
    cache_dirs = [
        '/workspace/.cache/huggingface',
        '/workspace/.cache/huggingface/transformers',
        '/workspace/.cache/huggingface/datasets'
    ]
    
    for cache_dir in cache_dirs:
        if os.path.exists(cache_dir):
            print(f"   âœ… Cache adresÃ¡Å™ existuje: {cache_dir}")
        else:
            print(f"   âŒ Cache adresÃ¡Å™ chybÃ­: {cache_dir}")
            return False
    
    print("âœ… NastavenÃ­ prostÅ™edÃ­ OK")
    return True

def test_model_loading():
    """Test naÄÃ­tÃ¡nÃ­ modelu"""
    print("\nğŸ¤– Test naÄÃ­tÃ¡nÃ­ modelu...")
    
    try:
        from test_adapter import load_model_with_adapter
        
        base_model = "mistralai/Mistral-7B-Instruct-v0.3"
        adapter_path = "mcmatak/babis-mistral-adapter"
        
        print(f"   NaÄÃ­tÃ¡m: {base_model}")
        print(f"   Adapter: {adapter_path}")
        
        start_time = time.time()
        model, tokenizer = load_model_with_adapter(base_model, adapter_path)
        load_time = time.time() - start_time
        
        if model is None or tokenizer is None:
            print("   âŒ Model se nepodaÅ™ilo naÄÃ­st")
            return False
        
        print(f"   âœ… Model naÄten za {load_time:.2f}s")
        print(f"   Model typ: {type(model).__name__}")
        print(f"   Tokenizer typ: {type(tokenizer).__name__}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­: {e}")
        return False

def test_response_generation():
    """Test generovÃ¡nÃ­ odpovÄ›dÃ­"""
    print("\nğŸ’¬ Test generovÃ¡nÃ­ odpovÄ›dÃ­...")
    
    try:
        from test_adapter import load_model_with_adapter, generate_response
        
        # NaÄtenÃ­ modelu
        base_model = "mistralai/Mistral-7B-Instruct-v0.3"
        adapter_path = "mcmatak/babis-mistral-adapter"
        
        model, tokenizer = load_model_with_adapter(base_model, adapter_path)
        
        if model is None or tokenizer is None:
            print("   âŒ Model nenÃ­ dostupnÃ½")
            return False
        
        # Test otÃ¡zky
        test_questions = [
            "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?",
            "Co si myslÃ­te o opozici?",
            "Jak se vÃ¡m daÅ™Ã­ s rodinou?"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"   Test {i}: {question}")
            
            # JednoduchÃ½ prompt bez dlouhÃ½ch instrukcÃ­
            prompt = f"<s>[INST] {question} [/INST]"
            
            # GenerovÃ¡nÃ­
            start_time = time.time()
            response = generate_response(
                model, tokenizer, prompt,
                max_length=300, temperature=0.8
            )
            gen_time = time.time() - start_time
            
            # VyÄiÅ¡tÄ›nÃ­
            response = response.strip()
            if response.startswith("OtÃ¡zka:"):
                response = response[response.find("[/INST]") + 7:].strip()
            
            print(f"      OdpovÄ›Ä: {response}")
            print(f"      ÄŒas: {gen_time:.2f}s")
            
            # RychlÃ¡ analÃ½za
            babis_indicators = ["hele", "skandÃ¡l", "makÃ¡m", "opozice", "brusel", "moje rodina"]
            found = sum(1 for indicator in babis_indicators if indicator.lower() in response.lower())
            
            if "andrej babiÅ¡" in response.lower():
                print(f"      âœ… Podpis: ANO")
            else:
                print(f"      âŒ Podpis: NE")
            
            print(f"      ğŸ“Š BabiÅ¡ovy indikÃ¡tory: {found}/{len(babis_indicators)}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Chyba pÅ™i generovÃ¡nÃ­: {e}")
        return False

def test_benchmark_components():
    """Test komponent benchmarkingu"""
    print("\nğŸ“Š Test komponent benchmarkingu...")
    
    # Test naÄÃ­tÃ¡nÃ­ otÃ¡zek
    try:
        if os.path.exists("benchmark_questions.json"):
            with open("benchmark_questions.json", "r", encoding="utf-8") as f:
                questions = json.load(f)
            print(f"   âœ… NaÄteno {len(questions)} testovacÃ­ch otÃ¡zek")
        else:
            print("   âš ï¸  Soubor benchmark_questions.json neexistuje")
            questions = []
    except Exception as e:
        print(f"   âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ otÃ¡zek: {e}")
        questions = []
    
    # Test evaluace stylu
    try:
        from evaluate_style import StyleEvaluator
        
        evaluator = StyleEvaluator()
        print("   âœ… StyleEvaluator naÄten")
        
        # Test evaluace
        test_response = "Hele, to je skandÃ¡l! JÃ¡ makÃ¡m, ale opozice krade. Andrej BabiÅ¡"
        score = evaluator.evaluate_response(test_response)
        print(f"   âœ… Test evaluace: {score:.2f}/10")
        
    except Exception as e:
        print(f"   âŒ Chyba pÅ™i evaluaci stylu: {e}")
    
    # Test generovÃ¡nÃ­ reportÅ¯
    try:
        from generate_report import generate_excel_report
        
        print("   âœ… Modul pro reporty naÄten")
        
    except Exception as e:
        print(f"   âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ reportÅ¯: {e}")
    
    return True

def test_file_structure():
    """Test struktury souborÅ¯"""
    print("\nğŸ“ Test struktury souborÅ¯...")
    
    required_files = [
        "benchmark_questions.json",
        "evaluate_style.py",
        "generate_responses.py",
        "compare_models.py",
        "generate_report.py",
        "run_benchmark.py"
    ]
    
    required_dirs = [
        "results",
        "results/before_finetune",
        "results/after_finetune",
        "results/comparison",
        "results/reports",
        "results/visualizations"
    ]
    
    for file in required_files:
        if os.path.exists(file):
            print(f"   âœ… {file}")
        else:
            print(f"   âŒ {file}")
    
    for dir_path in required_dirs:
        if os.path.exists(dir_path):
            print(f"   âœ… {dir_path}/")
        else:
            print(f"   âŒ {dir_path}/")
    
    return True

def run_complete_test():
    """SpustÃ­ kompletnÃ­ test integrace"""
    print("ğŸ§ª KOMPLETNÃ TEST INTEGRACE ADAPTÃ‰RU")
    print("=" * 60)
    
    tests = [
        ("NastavenÃ­ prostÅ™edÃ­", test_environment_setup),
        ("NaÄÃ­tÃ¡nÃ­ modelu", test_model_loading),
        ("GenerovÃ¡nÃ­ odpovÄ›dÃ­", test_response_generation),
        ("Komponenty benchmarkingu", test_benchmark_components),
        ("Struktura souborÅ¯", test_file_structure)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ NeoÄekÃ¡vanÃ¡ chyba v {test_name}: {e}")
            results.append((test_name, False))
    
    # ShrnutÃ­
    print("\n" + "=" * 60)
    print("ğŸ“‹ SHRNUTÃ TESTU")
    print("=" * 60)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} {test_name}")
        if result:
            passed += 1
    
    print(f"\nCelkem: {passed}/{total} testÅ¯ ÃºspÄ›Å¡nÃ½ch")
    
    if passed == total:
        print("ğŸ‰ VÅ ECHNY TESTY ÃšSPÄšÅ NÃ‰!")
        print("ğŸš€ AdaptÃ©r je pÅ™ipraven pro benchmarking!")
        print("ğŸ’¡ SpusÅ¥te: ./run_benchmark_with_adapter.sh")
    else:
        print("âš ï¸  NÄšKTERÃ‰ TESTY SELHALY")
        print("ğŸ”§ Zkontrolujte chyby vÃ½Å¡e")
    
    return passed == total

if __name__ == "__main__":
    success = run_complete_test()
    if not success:
        exit(1) 