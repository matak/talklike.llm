#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test integrace adaptÃ©ru s benchmarking systÃ©mem
OvÄ›Å™uje, Å¾e model s adaptÃ©rem funguje sprÃ¡vnÄ›
"""

import sys
import os
from pathlib import Path

# PÅ™idÃ¡nÃ­ cesty k 2_finetunning pro import
sys.path.append(str(Path(__file__).parent.parent / "2_finetunning"))

def test_adapter_loading():
    """Testuje naÄtenÃ­ adaptÃ©ru"""
    print("ğŸ” Testuji naÄtenÃ­ adaptÃ©ru...")
    
    try:
        from test_adapter import load_model_with_adapter, generate_response
        print("âœ… Import test_adapter ÃºspÄ›Å¡nÃ½")
        
        # Test naÄtenÃ­ modelu
        base_model = "mistralai/Mistral-7B-Instruct-v0.3"
        adapter_path = "mcmatak/babis-mistral-adapter"
        
        print(f"ğŸ¤– NaÄÃ­tÃ¡m model: {base_model}")
        print(f"ğŸ”§ S adaptÃ©rem: {adapter_path}")
        
        model, tokenizer = load_model_with_adapter(base_model, adapter_path)
        
        if model is not None and tokenizer is not None:
            print("âœ… Model s adaptÃ©rem ÃºspÄ›Å¡nÄ› naÄten!")
            return True
        else:
            print("âŒ Model se nepodaÅ™ilo naÄÃ­st")
            return False
            
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return False
    except Exception as e:
        print(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­: {e}")
        return False

def test_response_generation():
    """Testuje generovÃ¡nÃ­ odpovÄ›dÃ­"""
    print("\nğŸ” Testuji generovÃ¡nÃ­ odpovÄ›dÃ­...")
    
    try:
        from generate_responses import generate_real_response, load_benchmark_model
        
        # NaÄtenÃ­ modelu
        model, tokenizer = load_benchmark_model("finetuned")
        
        if model is None or tokenizer is None:
            print("âŒ Model nenÃ­ dostupnÃ½")
            return False
        
        # Test otÃ¡zka
        test_question = "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?"
        print(f"ğŸ“ Test otÃ¡zka: {test_question}")
        
        # GenerovÃ¡nÃ­ odpovÄ›di
        response = generate_real_response(model, tokenizer, test_question, "finetuned")
        
        print(f"ğŸ¤– OdpovÄ›Ä: {response}")
        
        if response and len(response) > 10:
            print("âœ… GenerovÃ¡nÃ­ odpovÄ›dÃ­ funguje!")
            return True
        else:
            print("âŒ OdpovÄ›Ä je pÅ™Ã­liÅ¡ krÃ¡tkÃ¡ nebo prÃ¡zdnÃ¡")
            return False
            
    except Exception as e:
        print(f"âŒ Chyba pÅ™i generovÃ¡nÃ­: {e}")
        return False

def test_benchmark_dataset():
    """Testuje benchmark dataset"""
    print("\nğŸ” Testuji benchmark dataset...")
    
    try:
        from create_benchmark_dataset import create_benchmark_dataset
        
        # VytvoÅ™enÃ­ datasetu
        create_benchmark_dataset()
        
        if os.path.exists("results/benchmark_dataset.json"):
            print("âœ… Benchmark dataset vytvoÅ™en!")
            return True
        else:
            print("âŒ Benchmark dataset nebyl vytvoÅ™en")
            return False
            
    except Exception as e:
        print(f"âŒ Chyba pÅ™i vytvÃ¡Å™enÃ­ datasetu: {e}")
        return False

def test_style_evaluation():
    """Testuje evaluaci stylu"""
    print("\nğŸ” Testuji evaluaci stylu...")
    
    try:
        from evaluate_style import evaluate_babis_style
        
        # Test odpovÄ›Ä
        test_response = "Hele, inflace je jak kdyÅ¾ krÃ¡va hraje na klavÃ­r! JÃ¡ makÃ¡m, ale opozice krade. To je skandÃ¡l! Andrej BabiÅ¡"
        
        evaluation = evaluate_babis_style(test_response)
        
        print(f"ğŸ“Š Evaluace test odpovÄ›di:")
        print(f"   SkÃ³re: {evaluation['total_score']}/10")
        print(f"   ZnÃ¡mka: {evaluation['grade']}")
        print(f"   DÃ©lka: {evaluation['length']} znakÅ¯")
        
        if evaluation['total_score'] > 5:
            print("âœ… Evaluace stylu funguje!")
            return True
        else:
            print("âŒ Evaluace vrÃ¡tila nÃ­zkÃ© skÃ³re")
            return False
            
    except Exception as e:
        print(f"âŒ Chyba pÅ™i evaluaci: {e}")
        return False

def main():
    """HlavnÃ­ test funkce"""
    print("ğŸ§ª SpouÅ¡tÃ­m testy integrace adaptÃ©ru...")
    print("=" * 60)
    
    tests = [
        ("NaÄtenÃ­ adaptÃ©ru", test_adapter_loading),
        ("GenerovÃ¡nÃ­ odpovÄ›dÃ­", test_response_generation),
        ("Benchmark dataset", test_benchmark_dataset),
        ("Evaluace stylu", test_style_evaluation)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ Test: {test_name}")
        print("-" * 40)
        
        try:
            if test_func():
                print(f"âœ… {test_name}: ÃšSPÄšCH")
                passed += 1
            else:
                print(f"âŒ {test_name}: SELHÃNÃ")
        except Exception as e:
            print(f"âŒ {test_name}: CHYBA - {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š VÃ½sledky testÅ¯: {passed}/{total} ÃºspÄ›Å¡nÃ½ch")
    
    if passed == total:
        print("ğŸ‰ VÅ¡echny testy proÅ¡ly! Benchmarking je pÅ™ipraven.")
        print("ğŸš€ MÅ¯Å¾ete spustit: ./run_benchmark_with_adapter.sh")
    else:
        print("âš ï¸  NÄ›kterÃ© testy selhaly. Zkontrolujte konfiguraci.")
    
    print("=" * 60)

if __name__ == "__main__":
    main() 