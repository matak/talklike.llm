#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GenerovÃ¡nÃ­ odpovÄ›dÃ­ pro benchmarking TalkLike.LLM
PouÅ¾Ã­vÃ¡ skuteÄnÃ½ model s adaptÃ©rem pro generovÃ¡nÃ­ odpovÄ›dÃ­
"""

# Import a nastavenÃ­ prostÅ™edÃ­
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
import setup_environment

import json
import os
import random
import torch
from datetime import datetime
from typing import List, Dict
from transformers import AutoModelForCausalLM, AutoTokenizer

# Import centralizovanÃ© funkce pro nastavenÃ­ pad_tokenu
sys.path.append('../2_finetunning')
from tokenizer_utils import setup_tokenizer_and_model

def load_benchmark_model(model_type: str):
    """NaÄte model pro benchmarking"""
    
    if model_type == "finetuned":
        # ReÃ¡lnÃ½ fine-tuned model z Hugging Face
        model_path = "mcmatak/mistral-babis-model"
        
        print(f"ğŸ¤– NaÄÃ­tÃ¡m reÃ¡lnÃ½ fine-tuned model...")
        print(f"   Model: {model_path}")
        
        # NaÄtenÃ­ tokenizeru a modelu
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # NastavenÃ­ pad tokenu
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model.eval()
        
    elif model_type == "base":
        # ZÃ¡kladnÃ­ model bez fine-tuningu
        base_model = "mistralai/Mistral-7B-Instruct-v0.3"
        
        print(f"ğŸ¤– NaÄÃ­tÃ¡m zÃ¡kladnÃ­ model...")
        print(f"   Base model: {base_model}")
        
        tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # PouÅ¾itÃ­ centralizovanÃ© funkce pro nastavenÃ­ pad_tokenu
        tokenizer, model = setup_tokenizer_and_model(base_model, model)
        model.eval()
        
    else:
        raise ValueError(f"NeznÃ¡mÃ½ typ modelu: {model_type}")
    
    return model, tokenizer

def generate_real_response(model, tokenizer, question: str, model_type: str) -> str:
    """Generuje skuteÄnou odpovÄ›Ä pomocÃ­ modelu"""
    
    # PouÅ¾itÃ­ apply_chat_template pro sprÃ¡vnÃ© formÃ¡tovÃ¡nÃ­
    messages = [{"role": "user", "content": question}]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # GenerovÃ¡nÃ­ odpovÄ›di pomocÃ­ modelu
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    
    # PÅ™esun input tensors na stejnÃ© zaÅ™Ã­zenÃ­ jako model
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=300,
            temperature=0.8,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # DekÃ³dovÃ¡nÃ­ odpovÄ›di
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # VylepÅ¡enÃ© vyÄiÅ¡tÄ›nÃ­ odpovÄ›di
    response = response.strip()
    
    # OdstranÄ›nÃ­ moÅ¾nÃ½ch zbytkÅ¯ promptu
    cleanup_patterns = [
        question,  # PÅ¯vodnÃ­ otÃ¡zka
        prompt,    # CelÃ½ prompt
    ]
    
    for pattern in cleanup_patterns:
        if response.startswith(pattern):
            response = response[len(pattern):].strip()
            break
    
    # OdstranÄ›nÃ­ prÃ¡zdnÃ½ch Å™Ã¡dkÅ¯ na zaÄÃ¡tku
    response = response.lstrip('\n').strip()
    
    return response if response else "OmlouvÃ¡m se, nemohu odpovÄ›dÄ›t."

def generate_responses(model_type: str, output_dir: str):
    """Generuje odpovÄ›di pro danÃ½ typ modelu"""
    
    print(f"ğŸ¤– Generuji odpovÄ›di pro model: {model_type}")
    
    # NaÄtenÃ­ benchmark datasetu
    with open("3_benchmarking/results/benchmark_dataset.json", "r", encoding="utf-8") as f:
        dataset = json.load(f)
    
    questions = dataset.get("questions", [])
    
    # NaÄtenÃ­ modelu
    print(f"ğŸ”§ NaÄÃ­tÃ¡m reÃ¡lnÃ½ model: {model_type}")
    model, tokenizer = load_benchmark_model(model_type)
    
    print(f"âœ… ReÃ¡lnÃ½ model naÄten: {model_type}")
    
    responses = []
    
    for i, question in enumerate(questions):
        print(f"   Generuji odpovÄ›Ä {i+1}/{len(questions)}: {question['question'][:50]}...")
        
        response = generate_real_response(model, tokenizer, question["question"], model_type)
        
        responses.append({
            "id": question["id"],
            "question": question["question"],
            "response": response,
            "model_type": model_type,
            "timestamp": datetime.now().isoformat()
        })
    
    # UloÅ¾enÃ­ odpovÄ›dÃ­
    output_file = os.path.join(output_dir, "responses.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(responses, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… VygenerovÃ¡no {len(responses)} odpovÄ›dÃ­")
    print(f"ğŸ’¾ UloÅ¾eno: {output_file}")
    
    # VÃ½pis nÄ›kolika pÅ™Ã­kladÅ¯
    print(f"\nğŸ“ PÅ™Ã­klady odpovÄ›dÃ­ ({model_type}):")
    for i, resp in enumerate(responses[:3]):
        print(f"   {i+1}. {resp['question']}")
        print(f"      â†’ {resp['response']}")
        print()
    
    # UvolnÄ›nÃ­ pamÄ›ti
    del model, tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return responses

def generate_real_responses(model_type: str, output_dir: str):
    """Wrapper pro skuteÄnÃ© generovÃ¡nÃ­ (pro kompatibilitu)"""
    return generate_responses(model_type, output_dir)

if __name__ == "__main__":
    # Test generovÃ¡nÃ­ odpovÄ›dÃ­
    print("ğŸ§ª Test generovÃ¡nÃ­ odpovÄ›dÃ­...")
    
    # Test pÅ™ed fine-tuningem
    base_responses = generate_responses("base", "3_benchmarking/results/before_finetune/")
    
    # Test po fine-tuningem
    finetuned_responses = generate_responses("finetuned", "3_benchmarking/results/after_finetune/")
    
    print(f"\nâœ… Test dokonÄen:")
    print(f"   PÅ™ed fine-tuningem: {len(base_responses)} odpovÄ›dÃ­")
    print(f"   Po fine-tuningem: {len(finetuned_responses)} odpovÄ›dÃ­") 