#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GenerovÃ¡nÃ­ odpovÄ›dÃ­ pro benchmarking TalkLike.LLM
PouÅ¾Ã­vÃ¡ skuteÄnÃ½ model s adaptÃ©rem pro generovÃ¡nÃ­ odpovÄ›dÃ­
"""

# Import a nastavenÃ­ prostÅ™edÃ­
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
import setup_environment

import json
import os
import random
import torch
from datetime import datetime
from typing import List, Dict
from transformers import AutoModelForCausalLM, AutoTokenizer

# Import centralizovanÃ© funkce pro nastavenÃ­ pad_tokenu
sys.path.append('../2_finetunning')
from tokenizer_utils import setup_tokenizer_and_model

# NastavenÃ­ MODEL_AVAILABLE na True, protoÅ¾e budeme pouÅ¾Ã­vat reÃ¡lnÃ½ model
MODEL_AVAILABLE = True
print("âœ… ReÃ¡lnÃ½ model bude naÄÃ­tÃ¡n z Hugging Face")

def load_benchmark_model(model_type: str):
    """NaÄte model pro benchmarking"""
    if not MODEL_AVAILABLE:
        print("âŒ Model nenÃ­ dostupnÃ½ - MODEL_AVAILABLE = False")
        return None, None
    
    try:
        if model_type == "finetuned":
            # ReÃ¡lnÃ½ fine-tuned model z Hugging Face
            model_path = "mcmatak/mistral-babis-model"
            
            print(f"ğŸ¤– NaÄÃ­tÃ¡m reÃ¡lnÃ½ fine-tuned model...")
            print(f"   Model: {model_path}")
            
            # NaÄtenÃ­ tokenizeru a modelu
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # NastavenÃ­ pad tokenu
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model.eval()
            
        elif model_type == "base":
            # ZÃ¡kladnÃ­ model bez fine-tuningu
            base_model = "mistralai/Mistral-7B-Instruct-v0.3"
            
            print(f"ğŸ¤– NaÄÃ­tÃ¡m zÃ¡kladnÃ­ model...")
            print(f"   Base model: {base_model}")
            
            tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                base_model,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # PouÅ¾itÃ­ centralizovanÃ© funkce pro nastavenÃ­ pad_tokenu
            tokenizer, model = setup_tokenizer_and_model(base_model, model)
            model.eval()
            
        else:
            raise ValueError(f"NeznÃ¡mÃ½ typ modelu: {model_type}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ modelu {model_type}: {e}")
        return None, None

def generate_real_response(model, tokenizer, question: str, model_type: str) -> str:
    """Generuje skuteÄnou odpovÄ›Ä pomocÃ­ modelu"""
    
    try:
        # OBA modely pouÅ¾Ã­vajÃ­ stejnÃ½ jednoduchÃ½ prompt bez system promptu
        # TÃ­m testujeme skuteÄnÃ½ fine-tuning, ne prompt engineering
        prompt = f"<s>[INST] {question} [/INST]"
        
        # GenerovÃ¡nÃ­ odpovÄ›di pomocÃ­ modelu
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=300,
                temperature=0.8,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # DekÃ³dovÃ¡nÃ­ odpovÄ›di
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # VylepÅ¡enÃ© vyÄiÅ¡tÄ›nÃ­ odpovÄ›di
        response = response.strip()
        
        # OdstranÄ›nÃ­ moÅ¾nÃ½ch zbytkÅ¯ promptu
        cleanup_patterns = [
            question,  # PÅ¯vodnÃ­ otÃ¡zka
            f"<s>[INST] {question} [/INST]",
        ]
        
        for pattern in cleanup_patterns:
            if response.startswith(pattern):
                response = response[len(pattern):].strip()
                break
        
        # OdstranÄ›nÃ­ prÃ¡zdnÃ½ch Å™Ã¡dkÅ¯ na zaÄÃ¡tku
        response = response.lstrip('\n').strip()
        
        return response if response else "OmlouvÃ¡m se, nemohu odpovÄ›dÄ›t."
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i generovÃ¡nÃ­ odpovÄ›di: {e}")
        return f"Chyba pÅ™i generovÃ¡nÃ­: {str(e)}"

def generate_responses(model_type: str, output_dir: str):
    """Generuje odpovÄ›di pro danÃ½ typ modelu"""
    
    print(f"ğŸ¤– Generuji odpovÄ›di pro model: {model_type}")
    
    # NaÄtenÃ­ benchmark datasetu
    if os.path.exists("results/benchmark_dataset.json"):
        with open("results/benchmark_dataset.json", "r", encoding="utf-8") as f:
            dataset = json.load(f)
        
        questions = dataset.get("questions", [])
    else:
        # Fallback na zÃ¡kladnÃ­ otÃ¡zky
        questions = [
            {"id": "Q1", "question": "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?"},
            {"id": "Q2", "question": "Co si myslÃ­te o opozici?"},
            {"id": "Q3", "question": "Jak se vÃ¡m daÅ™Ã­ s rodinou?"},
            {"id": "Q4", "question": "MÅ¯Å¾ete vysvÄ›tlit vaÅ¡i roli v tÃ© chemiÄce?"},
            {"id": "Q5", "question": "Jak vnÃ­mÃ¡te reakce Bruselu na ekonomickou situaci v ÄŒesku?"}
        ]
    
    # NaÄtenÃ­ modelu - VÅ½DY pouÅ¾Ã­vÃ¡me reÃ¡lnÃ½ model
    print(f"ğŸ”§ NaÄÃ­tÃ¡m reÃ¡lnÃ½ model: {model_type}")
    model, tokenizer = load_benchmark_model(model_type)
    
    if model is None or tokenizer is None:
        print(f"âŒ NepodaÅ™ilo se naÄÃ­st model {model_type}")
        print("ğŸ’¡ Zkontrolujte:")
        print("   - MÃ¡te pÅ™Ã­stup k modelu mcmatak/babis-mistral-adapter?")
        print("   - Jste pÅ™ihlÃ¡Å¡eni na Hugging Face?")
        print("   - MÃ¡te dostatek mÃ­sta v cache?")
        raise RuntimeError(f"Model {model_type} se nepodaÅ™ilo naÄÃ­st")
    
    print(f"âœ… ReÃ¡lnÃ½ model naÄten: {model_type}")
    
    responses = []
    
    for i, question in enumerate(questions):
        print(f"   Generuji odpovÄ›Ä {i+1}/{len(questions)}: {question['question'][:50]}...")
        
        # VÅ½DY pouÅ¾Ã­vÃ¡me reÃ¡lnÃ½ model
        response = generate_real_response(model, tokenizer, question["question"], model_type)
        
        responses.append({
            "id": question["id"],
            "question": question["question"],
            "response": response,
            "model_type": model_type,
            "timestamp": datetime.now().isoformat()
        })
    
    # UloÅ¾enÃ­ odpovÄ›dÃ­
    output_file = os.path.join(output_dir, "responses.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(responses, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… VygenerovÃ¡no {len(responses)} odpovÄ›dÃ­")
    print(f"ğŸ’¾ UloÅ¾eno: {output_file}")
    
    # VÃ½pis nÄ›kolika pÅ™Ã­kladÅ¯
    print(f"\nğŸ“ PÅ™Ã­klady odpovÄ›dÃ­ ({model_type}):")
    for i, resp in enumerate(responses[:3]):
        print(f"   {i+1}. {resp['question']}")
        print(f"      â†’ {resp['response']}")
        print()
    
    # UvolnÄ›nÃ­ pamÄ›ti
    del model, tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return responses

def generate_real_responses(model_type: str, output_dir: str):
    """Wrapper pro skuteÄnÃ© generovÃ¡nÃ­ (pro kompatibilitu)"""
    return generate_responses(model_type, output_dir)

if __name__ == "__main__":
    # Test generovÃ¡nÃ­ odpovÄ›dÃ­
    print("ğŸ§ª Test generovÃ¡nÃ­ odpovÄ›dÃ­...")
    
    # Test pÅ™ed fine-tuningem
    base_responses = generate_responses("base", "results/before_finetune/")
    
    # Test po fine-tuningem
    finetuned_responses = generate_responses("finetuned", "results/after_finetune/")
    
    print(f"\nâœ… Test dokonÄen:")
    print(f"   PÅ™ed fine-tuningem: {len(base_responses)} odpovÄ›dÃ­")
    print(f"   Po fine-tuningem: {len(finetuned_responses)} odpovÄ›dÃ­") 