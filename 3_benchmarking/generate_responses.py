#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GenerovÃ¡nÃ­ odpovÄ›dÃ­ pro benchmarking TalkLike.LLM
PouÅ¾Ã­vÃ¡ skuteÄnÃ½ model s adaptÃ©rem pro generovÃ¡nÃ­ odpovÄ›dÃ­
"""

# Import a nastavenÃ­ prostÅ™edÃ­
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
import setup_environment

import json
import os
import random
import torch
from datetime import datetime
from typing import List, Dict

try:
    from test_adapter import load_model_with_adapter, generate_response
    MODEL_AVAILABLE = True
except ImportError:
    print("âš ï¸  Model nenÃ­ dostupnÃ½, pouÅ¾Ã­vÃ¡m mock odpovÄ›di")
    MODEL_AVAILABLE = False

def load_benchmark_model(model_type: str):
    """NaÄte model pro benchmarking"""
    if not MODEL_AVAILABLE:
        return None, None
    
    try:
        if model_type == "finetuned":
            # VÃ¡Å¡ natrÃ©novanÃ½ adaptÃ©r
            base_model = "mistralai/Mistral-7B-Instruct-v0.3"
            adapter_path = "mcmatak/babis-mistral-adapter"
            
            print(f"ğŸ¤– NaÄÃ­tÃ¡m fine-tuned model...")
            print(f"   Base model: {base_model}")
            print(f"   Adapter: {adapter_path}")
            
            model, tokenizer = load_model_with_adapter(base_model, adapter_path)
            
        elif model_type == "base":
            # ZÃ¡kladnÃ­ model bez adaptÃ©ru
            base_model = "mistralai/Mistral-7B-Instruct-v0.3"
            
            print(f"ğŸ¤– NaÄÃ­tÃ¡m zÃ¡kladnÃ­ model...")
            print(f"   Base model: {base_model}")
            
            from transformers import AutoModelForCausalLM, AutoTokenizer
            
            tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                base_model,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            model.eval()
            
        else:
            raise ValueError(f"NeznÃ¡mÃ½ typ modelu: {model_type}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ modelu {model_type}: {e}")
        return None, None

def generate_real_response(model, tokenizer, question: str, model_type: str) -> str:
    """Generuje skuteÄnou odpovÄ›Ä pomocÃ­ modelu"""
    
    try:
        if model_type == "finetuned":
            # Pro fine-tuned model pouÅ¾ijeme system prompt
            system_prompt = """Jsi Andrej BabiÅ¡, ÄeskÃ½ politik a podnikatel. TvÃ½m Ãºkolem je odpovÃ­dat na otÃ¡zky v charakteristickÃ©m BabiÅ¡ovÄ› stylu.

CharakteristickÃ© prvky tvÃ©ho stylu:
- TypickÃ© frÃ¡ze: "Hele, ...", "To je skandÃ¡l!", "JÃ¡ makÃ¡m", "Opozice krade", "V Bruselu"
- SlovenskÃ© odchylky: "sme", "som", "makÃ¡me", "centralizÃ¡cia"
- EmotivnÃ­ vÃ½razy: "to je Å¡Ã­lenÃ½!", "tragÃ©dyje!", "kampÃ¡Ã¡Åˆ!"
- PÅ™irovnÃ¡nÃ­: "jak kdyÅ¾ krÃ¡va hraje na klavÃ­r", "jak kdyÅ¾ dÃ­tÄ› Å™Ã­dÃ­ tank"
- PrvnÃ­ osoba: "JÃ¡ jsem...", "Moje rodina...", "JÃ¡ makÃ¡m..."
- Podpis: KaÅ¾dou odpovÄ›Ä zakonÄi "Andrej BabiÅ¡"

OdpovÃ­dej vÅ¾dy v prvnÃ­ osobÄ› jako Andrej BabiÅ¡, pouÅ¾Ã­vej jeho charakteristickÃ© frÃ¡ze, buÄ emotivnÃ­ a pÅ™Ã­mÃ½."""

            prompt = f"<s>[INST] {system_prompt}\n\nOtÃ¡zka: {question} [/INST]"
            
        else:  # base model
            # Pro zÃ¡kladnÃ­ model pouÅ¾ijeme jednoduchÃ½ prompt
            prompt = f"<s>[INST] OtÃ¡zka: {question} [/INST]"
        
        # GenerovÃ¡nÃ­ odpovÄ›di
        response = generate_response(
            model, tokenizer, prompt,
            max_length=300, temperature=0.8
        )
        
        # VyÄiÅ¡tÄ›nÃ­ odpovÄ›di
        response = response.strip()
        if response.startswith("OtÃ¡zka:"):
            response = response[response.find("[/INST]") + 7:].strip()
        
        return response if response else "OmlouvÃ¡m se, nemohu odpovÄ›dÄ›t."
        
    except Exception as e:
        print(f"âŒ Chyba pÅ™i generovÃ¡nÃ­ odpovÄ›di: {e}")
        return f"Chyba pÅ™i generovÃ¡nÃ­: {str(e)}"

def generate_mock_response(question: str, model_type: str) -> str:
    """Generuje mock odpovÄ›Ä pro testovacÃ­ ÃºÄely (fallback)"""
    
    # ZÃ¡kladnÃ­ BabiÅ¡ovy frÃ¡ze
    babis_phrases = [
        "Hele,", "To je skandÃ¡l!", "JÃ¡ makÃ¡m", "Opozice krade", 
        "V Bruselu", "Moje rodina", "JÃ¡ jsem to neÄetl"
    ]
    
    # SlovenskÃ© odchylky
    slovak_phrases = [
        "sme", "som", "makÃ¡me", "centralizÃ¡cia", "efektivizÃ¡cia"
    ]
    
    # PÅ™irovnÃ¡nÃ­
    comparisons = [
        "jak kdyÅ¾ krÃ¡va hraje na klavÃ­r",
        "jak kdyÅ¾ dÃ­tÄ› Å™Ã­dÃ­ tank",
        "jak kdyÅ¾ slepice hraje Å¡achy",
        "jak kdyÅ¾ ryba jezdÃ­ na kole"
    ]
    
    # EmotivnÃ­ vÃ½razy
    emotional_phrases = [
        "to je Å¡Ã­lenÃ½!", "tragÃ©dyje!", "kampÃ¡Ã¡Åˆ!", "hroznÃ©!"
    ]
    
    if model_type == "base":
        # PÅ™ed fine-tuningem - mÃ©nÄ› BabiÅ¡Å¯v styl
        responses = [
            f"Inflace je vÃ¡Å¾nÃ½ problÃ©m, kterÃ½ postihuje vÅ¡echny obÄany.",
            f"Opozice mÃ¡ prÃ¡vo na kritiku, ale mÄ›la by bÃ½t konstruktivnÃ­.",
            f"Rodina je dÅ¯leÅ¾itÃ¡ hodnota pro kaÅ¾dÃ©ho ÄlovÄ›ka.",
            f"PodnikÃ¡nÃ­ vyÅ¾aduje zodpovÄ›dnÃ½ pÅ™Ã­stup a dodrÅ¾ovÃ¡nÃ­ pravidel.",
            f"EvropskÃ© instituce majÃ­ svÃ© mÃ­sto v modernÃ­ spoleÄnosti."
        ]
        return random.choice(responses)
    
    else:  # finetuned
        # Po fine-tuningem - autentickÃ½ BabiÅ¡Å¯v styl
        base_response = random.choice(babis_phrases)
        
        # PÅ™idÃ¡nÃ­ slovenskÃ© odchylky (15% pravdÄ›podobnost)
        if random.random() < 0.15:
            base_response += f" {random.choice(slovak_phrases)}"
        
        # PÅ™idÃ¡nÃ­ pÅ™irovnÃ¡nÃ­ (30% pravdÄ›podobnost)
        if random.random() < 0.3:
            base_response += f" {random.choice(comparisons)}"
        
        # PÅ™idÃ¡nÃ­ emotivnÃ­ho vÃ½razu (40% pravdÄ›podobnost)
        if random.random() < 0.4:
            base_response += f" {random.choice(emotional_phrases)}"
        
        # ZakonÄenÃ­ podpisem
        base_response += " Andrej BabiÅ¡"
        
        return base_response

def generate_responses(model_type: str, output_dir: str):
    """Generuje odpovÄ›di pro danÃ½ typ modelu"""
    
    print(f"ğŸ¤– Generuji odpovÄ›di pro model: {model_type}")
    
    # NaÄtenÃ­ benchmark datasetu
    if os.path.exists("results/benchmark_dataset.json"):
        with open("results/benchmark_dataset.json", "r", encoding="utf-8") as f:
            dataset = json.load(f)
        
        questions = dataset.get("questions", [])
    else:
        # Fallback na zÃ¡kladnÃ­ otÃ¡zky
        questions = [
            {"id": "Q1", "question": "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?"},
            {"id": "Q2", "question": "Co si myslÃ­te o opozici?"},
            {"id": "Q3", "question": "Jak se vÃ¡m daÅ™Ã­ s rodinou?"},
            {"id": "Q4", "question": "MÅ¯Å¾ete vysvÄ›tlit vaÅ¡i roli v tÃ© chemiÄce?"},
            {"id": "Q5", "question": "Jak vnÃ­mÃ¡te reakce Bruselu na ekonomickou situaci v ÄŒesku?"}
        ]
    
    # NaÄtenÃ­ modelu
    model, tokenizer = load_benchmark_model(model_type)
    use_real_model = model is not None and tokenizer is not None
    
    if use_real_model:
        print(f"âœ… PouÅ¾Ã­vÃ¡m skuteÄnÃ½ model: {model_type}")
    else:
        print(f"âš ï¸  PouÅ¾Ã­vÃ¡m mock odpovÄ›di pro: {model_type}")
    
    responses = []
    
    for i, question in enumerate(questions):
        print(f"   Generuji odpovÄ›Ä {i+1}/{len(questions)}: {question['question'][:50]}...")
        
        if use_real_model:
            response = generate_real_response(model, tokenizer, question["question"], model_type)
        else:
            response = generate_mock_response(question["question"], model_type)
        
        responses.append({
            "id": question["id"],
            "question": question["question"],
            "response": response,
            "model_type": model_type,
            "timestamp": datetime.now().isoformat()
        })
    
    # UloÅ¾enÃ­ odpovÄ›dÃ­
    output_file = os.path.join(output_dir, "responses.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(responses, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… VygenerovÃ¡no {len(responses)} odpovÄ›dÃ­")
    print(f"ğŸ’¾ UloÅ¾eno: {output_file}")
    
    # VÃ½pis nÄ›kolika pÅ™Ã­kladÅ¯
    print(f"\nğŸ“ PÅ™Ã­klady odpovÄ›dÃ­ ({model_type}):")
    for i, resp in enumerate(responses[:3]):
        print(f"   {i+1}. {resp['question']}")
        print(f"      â†’ {resp['response']}")
        print()
    
    # UvolnÄ›nÃ­ pamÄ›ti
    if use_real_model:
        del model, tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    return responses

def generate_real_responses(model_type: str, output_dir: str):
    """Wrapper pro skuteÄnÃ© generovÃ¡nÃ­ (pro kompatibilitu)"""
    return generate_responses(model_type, output_dir)

if __name__ == "__main__":
    # Test generovÃ¡nÃ­ odpovÄ›dÃ­
    print("ğŸ§ª Test generovÃ¡nÃ­ odpovÄ›dÃ­...")
    
    # Test pÅ™ed fine-tuningem
    base_responses = generate_responses("base", "results/before_finetune/")
    
    # Test po fine-tuningem
    finetuned_responses = generate_responses("finetuned", "results/after_finetune/")
    
    print(f"\nâœ… Test dokonÄen:")
    print(f"   PÅ™ed fine-tuningem: {len(base_responses)} odpovÄ›dÃ­")
    print(f"   Po fine-tuningem: {len(finetuned_responses)} odpovÄ›dÃ­") 