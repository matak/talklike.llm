#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Generov√°n√≠ odpovƒõd√≠ pro benchmarking TalkLike.LLM
Pou≈æ√≠v√° skuteƒçn√Ω model s adapt√©rem pro generov√°n√≠ odpovƒõd√≠
"""

# Import a nastaven√≠ prost≈ôed√≠
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
import setup_environment

import json
import os
import random
import torch
from datetime import datetime
from typing import List, Dict
from transformers import AutoModelForCausalLM, AutoTokenizer

# Import centralizovan√© funkce pro nastaven√≠ pad_tokenu
sys.path.append('../2_finetunning')
from tokenizer_utils import setup_tokenizer_and_model

# Nastaven√≠ MODEL_AVAILABLE na True, proto≈æe budeme pou≈æ√≠vat re√°ln√Ω model
MODEL_AVAILABLE = True
print("‚úÖ Re√°ln√Ω model bude naƒç√≠t√°n z Hugging Face")

def load_benchmark_model(model_type: str):
    """Naƒçte model pro benchmarking"""
    if not MODEL_AVAILABLE:
        print("‚ùå Model nen√≠ dostupn√Ω - MODEL_AVAILABLE = False")
        return None, None
    
    try:
        if model_type == "finetuned":
            # Re√°ln√Ω fine-tuned model z Hugging Face
            model_path = "mcmatak/mistral-babis-model"
            
            print(f"ü§ñ Naƒç√≠t√°m re√°ln√Ω fine-tuned model...")
            print(f"   Model: {model_path}")
            
            # Naƒçten√≠ tokenizeru a modelu
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Nastaven√≠ pad tokenu
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model.eval()
            
        elif model_type == "base":
            # Z√°kladn√≠ model bez fine-tuningu
            base_model = "mistralai/Mistral-7B-Instruct-v0.3"
            
            print(f"ü§ñ Naƒç√≠t√°m z√°kladn√≠ model...")
            print(f"   Base model: {base_model}")
            
            tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                base_model,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Pou≈æit√≠ centralizovan√© funkce pro nastaven√≠ pad_tokenu
            tokenizer, model = setup_tokenizer_and_model(base_model, model)
            model.eval()
            
        else:
            raise ValueError(f"Nezn√°m√Ω typ modelu: {model_type}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi naƒç√≠t√°n√≠ modelu {model_type}: {e}")
        return None, None

def generate_real_response(model, tokenizer, question: str, model_type: str) -> str:
    """Generuje skuteƒçnou odpovƒõƒè pomoc√≠ modelu"""
    
    try:
        if model_type == "finetuned":
            # Pro fine-tuned model pou≈æijeme system prompt
            system_prompt = """Jsi Andrej Babi≈°, ƒçesk√Ω politik a podnikatel. Tv√Ωm √∫kolem je odpov√≠dat na ot√°zky v charakteristick√©m Babi≈°ovƒõ stylu.

Charakteristick√© prvky tv√©ho stylu:
- Typick√© fr√°ze: "Hele, ...", "To je skand√°l!", "J√° mak√°m", "Opozice krade", "V Bruselu"
- Slovensk√© odchylky: "sme", "som", "mak√°me", "centraliz√°cia"
- Emotivn√≠ v√Ωrazy: "to je ≈°√≠len√Ω!", "trag√©dyje!", "kamp√°√°≈à!"
- P≈ôirovn√°n√≠: "jak kdy≈æ kr√°va hraje na klav√≠r", "jak kdy≈æ d√≠tƒõ ≈ô√≠d√≠ tank"
- Prvn√≠ osoba: "J√° jsem...", "Moje rodina...", "J√° mak√°m..."

Odpov√≠dej v≈ædy v prvn√≠ osobƒõ jako Andrej Babi≈°, pou≈æ√≠vej jeho charakteristick√© fr√°ze, buƒè emotivn√≠ a p≈ô√≠m√Ω."""

            prompt = f"<s>[INST] {system_prompt}\n\nOt√°zka: {question} [/INST]"
            
        else:  # base model
            # Pro z√°kladn√≠ model pou≈æijeme jednoduch√Ω prompt
            prompt = f"<s>[INST] Ot√°zka: {question} [/INST]"
        
        # Generov√°n√≠ odpovƒõdi pomoc√≠ modelu
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=300,
                temperature=0.8,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # Dek√≥dov√°n√≠ odpovƒõdi
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Vylep≈°en√© vyƒçi≈°tƒõn√≠ odpovƒõdi
        response = response.strip()
        
        # Odstranƒõn√≠ mo≈æn√Ωch zbytk≈Ø promptu
        cleanup_patterns = [
            f"Ot√°zka: {question}",
            f"Ot√°zka: {question} [/INST]",
            f"<s>[INST] Ot√°zka: {question} [/INST]",
            question,  # P≈Øvodn√≠ ot√°zka
        ]
        
        for pattern in cleanup_patterns:
            if response.startswith(pattern):
                response = response[len(pattern):].strip()
                break
        
        # Odstranƒõn√≠ pr√°zdn√Ωch ≈ô√°dk≈Ø na zaƒç√°tku
        response = response.lstrip('\n').strip()
        
        return response if response else "Omlouv√°m se, nemohu odpovƒõdƒõt."
        
    except Exception as e:
        print(f"‚ùå Chyba p≈ôi generov√°n√≠ odpovƒõdi: {e}")
        return f"Chyba p≈ôi generov√°n√≠: {str(e)}"

def generate_mock_response(question: str, model_type: str) -> str:
    """Generuje mock odpovƒõƒè pro testovac√≠ √∫ƒçely (fallback)"""
    
    # Z√°kladn√≠ Babi≈°ovy fr√°ze
    babis_phrases = [
        "Hele,", "To je skand√°l!", "J√° mak√°m", "Opozice krade", 
        "V Bruselu", "Moje rodina", "J√° jsem to neƒçetl"
    ]
    
    # Slovensk√© odchylky
    slovak_phrases = [
        "sme", "som", "mak√°me", "centraliz√°cia", "efektiviz√°cia"
    ]
    
    # P≈ôirovn√°n√≠
    comparisons = [
        "jak kdy≈æ kr√°va hraje na klav√≠r",
        "jak kdy≈æ d√≠tƒõ ≈ô√≠d√≠ tank",
        "jak kdy≈æ slepice hraje ≈°achy",
        "jak kdy≈æ ryba jezd√≠ na kole"
    ]
    
    # Emotivn√≠ v√Ωrazy
    emotional_phrases = [
        "to je ≈°√≠len√Ω!", "trag√©dyje!", "kamp√°√°≈à!", "hrozn√©!"
    ]
    
    if model_type == "base":
        # P≈ôed fine-tuningem - m√©nƒõ Babi≈°≈Øv styl
        responses = [
            f"Inflace je v√°≈æn√Ω probl√©m, kter√Ω postihuje v≈°echny obƒçany.",
            f"Opozice m√° pr√°vo na kritiku, ale mƒõla by b√Ωt konstruktivn√≠.",
            f"Rodina je d≈Øle≈æit√° hodnota pro ka≈æd√©ho ƒçlovƒõka.",
            f"Podnik√°n√≠ vy≈æaduje zodpovƒõdn√Ω p≈ô√≠stup a dodr≈æov√°n√≠ pravidel.",
            f"Evropsk√© instituce maj√≠ sv√© m√≠sto v modern√≠ spoleƒçnosti."
        ]
        return random.choice(responses)
    
    else:  # finetuned
        # Po fine-tuningem - autentick√Ω Babi≈°≈Øv styl
        base_response = random.choice(babis_phrases)
        
        # P≈ôid√°n√≠ slovensk√© odchylky (15% pravdƒõpodobnost)
        if random.random() < 0.15:
            base_response += f" {random.choice(slovak_phrases)}"
        
        # P≈ôid√°n√≠ p≈ôirovn√°n√≠ (30% pravdƒõpodobnost)
        if random.random() < 0.3:
            base_response += f" {random.choice(comparisons)}"
        
        # P≈ôid√°n√≠ emotivn√≠ho v√Ωrazu (40% pravdƒõpodobnost)
        if random.random() < 0.4:
            base_response += f" {random.choice(emotional_phrases)}"
        
        return base_response

def generate_responses(model_type: str, output_dir: str):
    """Generuje odpovƒõdi pro dan√Ω typ modelu"""
    
    print(f"ü§ñ Generuji odpovƒõdi pro model: {model_type}")
    
    # Naƒçten√≠ benchmark datasetu
    if os.path.exists("results/benchmark_dataset.json"):
        with open("results/benchmark_dataset.json", "r", encoding="utf-8") as f:
            dataset = json.load(f)
        
        questions = dataset.get("questions", [])
    else:
        # Fallback na z√°kladn√≠ ot√°zky
        questions = [
            {"id": "Q1", "question": "Pane Babi≈°i, jak hodnot√≠te souƒçasnou inflaci?"},
            {"id": "Q2", "question": "Co si mysl√≠te o opozici?"},
            {"id": "Q3", "question": "Jak se v√°m da≈ô√≠ s rodinou?"},
            {"id": "Q4", "question": "M≈Ø≈æete vysvƒõtlit va≈°i roli v t√© chemiƒçce?"},
            {"id": "Q5", "question": "Jak vn√≠m√°te reakce Bruselu na ekonomickou situaci v ƒåesku?"}
        ]
    
    # Naƒçten√≠ modelu - V≈ΩDY pou≈æ√≠v√°me re√°ln√Ω model
    print(f"üîß Naƒç√≠t√°m re√°ln√Ω model: {model_type}")
    model, tokenizer = load_benchmark_model(model_type)
    
    if model is None or tokenizer is None:
        print(f"‚ùå Nepoda≈ôilo se naƒç√≠st model {model_type}")
        print("üí° Zkontrolujte:")
        print("   - M√°te p≈ô√≠stup k modelu mcmatak/babis-mistral-adapter?")
        print("   - Jste p≈ôihl√°≈°eni na Hugging Face?")
        print("   - M√°te dostatek m√≠sta v cache?")
        raise RuntimeError(f"Model {model_type} se nepoda≈ôilo naƒç√≠st")
    
    print(f"‚úÖ Re√°ln√Ω model naƒçten: {model_type}")
    
    responses = []
    
    for i, question in enumerate(questions):
        print(f"   Generuji odpovƒõƒè {i+1}/{len(questions)}: {question['question'][:50]}...")
        
        # V≈ΩDY pou≈æ√≠v√°me re√°ln√Ω model
        response = generate_real_response(model, tokenizer, question["question"], model_type)
        
        responses.append({
            "id": question["id"],
            "question": question["question"],
            "response": response,
            "model_type": model_type,
            "timestamp": datetime.now().isoformat()
        })
    
    # Ulo≈æen√≠ odpovƒõd√≠
    output_file = os.path.join(output_dir, "responses.json")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(responses, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Vygenerov√°no {len(responses)} odpovƒõd√≠")
    print(f"üíæ Ulo≈æeno: {output_file}")
    
    # V√Ωpis nƒõkolika p≈ô√≠klad≈Ø
    print(f"\nüìù P≈ô√≠klady odpovƒõd√≠ ({model_type}):")
    for i, resp in enumerate(responses[:3]):
        print(f"   {i+1}. {resp['question']}")
        print(f"      ‚Üí {resp['response']}")
        print()
    
    # Uvolnƒõn√≠ pamƒõti
    del model, tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return responses

def generate_real_responses(model_type: str, output_dir: str):
    """Wrapper pro skuteƒçn√© generov√°n√≠ (pro kompatibilitu)"""
    return generate_responses(model_type, output_dir)

if __name__ == "__main__":
    # Test generov√°n√≠ odpovƒõd√≠
    print("üß™ Test generov√°n√≠ odpovƒõd√≠...")
    
    # Test p≈ôed fine-tuningem
    base_responses = generate_responses("base", "results/before_finetune/")
    
    # Test po fine-tuningem
    finetuned_responses = generate_responses("finetuned", "results/after_finetune/")
    
    print(f"\n‚úÖ Test dokonƒçen:")
    print(f"   P≈ôed fine-tuningem: {len(base_responses)} odpovƒõd√≠")
    print(f"   Po fine-tuningem: {len(finetuned_responses)} odpovƒõd√≠") 