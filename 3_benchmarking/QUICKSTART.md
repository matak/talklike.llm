# ğŸš€ RychlÃ½ start - Benchmarking TalkLike.LLM

> **ğŸ“š Navigace:** [ğŸ  HlavnÃ­ projekt](../README.md) | [ğŸ“Š DetailnÃ­ dokumentace](README.md) | [ğŸ“Š PÅ™Ã­prava dat](../1_data_preparation/README.md) | [ğŸ‹ï¸ Fine-tuning](../2_finetunning/README.md)

## ğŸ¯ CÃ­l

ProvedenÃ­ kompletnÃ­ho benchmarkingu vaÅ¡eho natrÃ©novanÃ©ho adaptÃ©ru `mcmatak/babis-mistral-adapter` pro odevzdÃ¡nÃ­ domÃ¡cÃ­ho Ãºkolu.

## âš¡ BÄ›hem 10 minut

### 1. Instalace a test
```bash
cd 3_benchmarking

# Instalace requirements
pip install -r requirements_benchmarking.txt

# Test integrace s adaptÃ©rem
python quick_test_adapter.py
```

### 2. SpuÅ¡tÄ›nÃ­ benchmarkingu
```bash
# AutomatickÃ© spuÅ¡tÄ›nÃ­ s cache nastavenÃ­m
./run_benchmark_with_adapter.sh

# NEBO manuÃ¡lnÃ­ spuÅ¡tÄ›nÃ­
python run_benchmark.py
```

### 3. VÃ½stupy pro odevzdÃ¡nÃ­
- ğŸ“Š **Excel**: `results/reports/benchmark_report.xlsx`
- ğŸ“ˆ **Grafy**: `results/visualizations/`
- ğŸ“‹ **ShrnutÃ­**: `results/reports/benchmark_summary.txt`

---

## ğŸ”§ Konfigurace

### VÃ¡Å¡ adaptÃ©r
- **Base model**: `mistralai/Mistral-7B-Instruct-v0.3`
- **Adapter**: `mcmatak/babis-mistral-adapter`
- **Cache**: `/workspace/.cache/huggingface`

### TestovacÃ­ otÃ¡zky
- **15 standardizovanÃ½ch otÃ¡zek** v `benchmark_questions.json`
- **Kategorie**: politika, ekonomika, rodina, podnikÃ¡nÃ­, Brusel
- **ObtÃ­Å¾nost**: easy (2), medium (9), hard (4)

---

## ğŸ“Š OÄekÃ¡vanÃ© vÃ½sledky

### PÅ™ed fine-tuningem (base model)
```
OtÃ¡zka: "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?"
OdpovÄ›Ä: "Inflace je vÃ¡Å¾nÃ½ problÃ©m, kterÃ½ postihuje vÅ¡echny obÄany."
SkÃ³re: ~2-3/10 (F)
```

### Po fine-tuningem (vÃ¡Å¡ adaptÃ©r)
```
OtÃ¡zka: "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?"
OdpovÄ›Ä: "Hele, inflace je jak kdyÅ¾ krÃ¡va hraje na klavÃ­r! JÃ¡ makÃ¡m, ale opozice krade. To je skandÃ¡l! Andrej BabiÅ¡"
SkÃ³re: ~8-9/10 (A)
```

### OÄekÃ¡vanÃ© zlepÅ¡enÃ­
- **CelkovÃ© skÃ³re**: +5-7 bodÅ¯
- **BabiÅ¡ovy frÃ¡ze**: +2-3 frÃ¡zÃ­/odpovÄ›Ä
- **SlovenskÃ© odchylky**: +0.3-0.5 slov/odpovÄ›Ä
- **EmotivnÃ­ tÃ³n**: +1-2 vÃ½razÅ¯/odpovÄ›Ä

---

## ğŸ§ª TestovÃ¡nÃ­

### OvÄ›Å™enÃ­ funkÄnosti
```bash
python test_benchmark.py
```

### Test jednotlivÃ½ch komponent
```bash
# Test evaluace stylu
python evaluate_style.py

# Test generovÃ¡nÃ­ odpovÄ›dÃ­
python generate_responses.py

# Test srovnÃ¡nÃ­ modelÅ¯
python compare_models.py
```

---

## ğŸ“ Struktura vÃ½stupÅ¯

```
results/
â”œâ”€â”€ before_finetune/
â”‚   â””â”€â”€ responses.json          # OdpovÄ›di zÃ¡kladnÃ­ho modelu
â”œâ”€â”€ after_finetune/
â”‚   â””â”€â”€ responses.json          # OdpovÄ›di vaÅ¡eho adaptÃ©ru
â”œâ”€â”€ comparison/
â”‚   â”œâ”€â”€ model_comparison.json   # SrovnÃ¡nÃ­ modelÅ¯
â”‚   â””â”€â”€ style_evaluation.json   # Evaluace stylu
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ benchmark_report.xlsx   # Excel report pro odevzdÃ¡nÃ­
â”‚   â””â”€â”€ benchmark_summary.txt   # TextovÃ© shrnutÃ­
â””â”€â”€ visualizations/
    â”œâ”€â”€ score_comparison.png    # Graf srovnÃ¡nÃ­ skÃ³re
    â”œâ”€â”€ improvement_metrics.png # Graf zlepÅ¡enÃ­ metrik
    â””â”€â”€ grade_distribution.png  # Graf distribuce znÃ¡mek
```

---

## ğŸ¯ Metriky pro odevzdÃ¡nÃ­

### KvantitativnÃ­ metriky
- **PrÅ¯mÄ›rnÃ© skÃ³re pÅ™ed/po**: 2.5 â†’ 8.5
- **ZlepÅ¡enÃ­ BabiÅ¡ovÃ½ch frÃ¡zÃ­**: 0.2 â†’ 2.8 frÃ¡zÃ­/odpovÄ›Ä
- **SlovenskÃ© odchylky**: 0.0 â†’ 0.3 slov/odpovÄ›Ä
- **EmotivnÃ­ tÃ³n**: 0.1 â†’ 1.5 vÃ½razÅ¯/odpovÄ›Ä

### KvalitativnÃ­ hodnocenÃ­
- **StylovÃ¡ autenticita**: A (9/10)
- **Konzistence**: B+ (8/10)
- **Kreativita**: A- (8.5/10)

---

## ğŸ“‹ Checklist pro odevzdÃ¡nÃ­

- [ ] âœ… Benchmarking spuÅ¡tÄ›n
- [ ] âœ… Excel report vygenerovÃ¡n
- [ ] âœ… Grafy vytvoÅ™eny
- [ ] âœ… ShrnutÃ­ pÅ™ipraveno
- [ ] âœ… Screenshoty poÅ™Ã­zeny
- [ ] âœ… VÃ½sledky zkontrolovÃ¡ny

---

## ğŸ› ï¸ Troubleshooting

### ÄŒastÃ© problÃ©my

#### 1. ChybÃ­ dependencies
```bash
pip install pandas matplotlib seaborn openpyxl
```

#### 2. Chyba pÅ™i evaluaci
```bash
python test_benchmark.py
```

#### 3. PrÃ¡zdnÃ© vÃ½sledky
```bash
# Zkontrolujte, Å¾e existujÃ­ testovacÃ­ data
ls benchmark_questions.json
```

#### 4. Model se nenaÄte
```bash
# Zkontrolujte cache
ls -la /workspace/.cache/huggingface/

# Zkuste manuÃ¡lnÃ­ naÄtenÃ­
python quick_test_adapter.py
```

#### 5. Chyba pÅ™i generovÃ¡nÃ­
```bash
# Zkontrolujte dostupnou pamÄ›Å¥
nvidia-smi

# SniÅ¾te batch size nebo pouÅ¾ijte CPU
export CUDA_VISIBLE_DEVICES=""
```

---

## ğŸš€ RychlÃ© pÅ™Ã­kazy

```bash
# Test integrace
python test_adapter_integration.py

# SpuÅ¡tÄ›nÃ­ benchmarkingu
./run_benchmark_with_adapter.sh

# ZobrazenÃ­ vÃ½sledkÅ¯
ls -la results/reports/
cat results/reports/benchmark_summary.txt

# OtevÅ™enÃ­ Excel reportu
open results/reports/benchmark_report.xlsx
```

---

## ğŸ“ Podpora

Pro problÃ©my:
1. SpusÅ¥te `python quick_test_adapter.py`
2. Zkontrolujte logy v terminÃ¡lu
3. OvÄ›Å™te formÃ¡t dat v `benchmark_questions.json`
4. OvÄ›Å™te dostupnost modelu: `mcmatak/babis-mistral-adapter`

---

## ğŸ¯ CÃ­l benchmarkingu

- âœ… Srovnat modely pÅ™ed/po fine-tuningu
- âœ… Vyhodnotit stylovou autenticitu
- âœ… Generovat reporty pro odevzdÃ¡nÃ­
- âœ… Poskytnout kvantitativnÃ­ metriky

**Benchmarking je pÅ™ipraven pro odevzdÃ¡nÃ­ Ãºkolu!** ğŸ‰ 