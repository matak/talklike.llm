#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Generov√°n√≠ report≈Ø pro TalkLike.LLM
Vytv√°≈ô√≠ markdown reporty a vizualizace
"""

import json
import os
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List

def generate_final_report(comparison_results: Dict = None):
    """Generuje fin√°ln√≠ report s v≈°emi v√Ωsledky"""
    
    print("üìã Generuji fin√°ln√≠ report...")
    
    # Naƒçten√≠ dat pokud nejsou poskytnuta
    if comparison_results is None:
        comparison_results = load_comparison_data()
    
    if comparison_results is None:
        print("‚ùå Nejsou k dispozici data pro report")
        return
    
    # 1. Markdown tabulky a shrnut√≠
    create_markdown_report(comparison_results)
    
    # 2. Vizualizace
    create_visualizations(comparison_results)
    
    print("‚úÖ Fin√°ln√≠ report vygenerov√°n")

def load_comparison_data() -> Dict:
    """Naƒçte data pro report z JSON soubor≈Ø"""
    
    data_files = [
        "3_benchmarking/results/comparison/model_comparison.json",
        "3_benchmarking/results/comparison/style_evaluation.json"
    ]
    
    combined_data = {}
    
    for file_path in data_files:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                combined_data[os.path.basename(file_path).replace(".json", "")] = data
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  Soubor {file_path} nebyl nalezen")
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è  Chyba p≈ôi ƒçten√≠ JSON souboru {file_path}")
    
    return combined_data

def create_markdown_report(comparison_results: Dict):
    """Vytvo≈ô√≠ markdown report s detailn√≠ tabulkou ot√°zek a shrnut√≠m"""
    md_file = "3_benchmarking/results/reports/benchmark_summary.md"
    
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("# Benchmarking Report - TalkLike.LLM\n\n")
        f.write("## Detailn√≠ srovn√°n√≠ modelu p≈ôed a po fine-tuningu\n\n")
        f.write(f"**Datum:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Naƒçten√≠ dat pro tabulku
        before_data = load_responses_data("3_benchmarking/results/before_finetune/responses.json")
        after_data = load_responses_data("3_benchmarking/results/after_finetune/responses.json")
        style_data = comparison_results.get("style_evaluation", {})
        
        # Vytvo≈ôen√≠ detailn√≠ tabulky
        f.write("### Detailn√≠ srovn√°n√≠ odpovƒõd√≠\n\n")
        f.write("| Ot√°zka | P≈ôed fine-tuningem | Po fine-tuningem | Stylov√© ohodnocen√≠ | Zmƒõna | Bodov√© ohodnocen√≠ | Zn√°mka | Shrnut√≠ |\n")
        f.write("|---|---|---|---|---|---|---|---|\n")
        
        # Proch√°zen√≠ v≈°ech ot√°zek
        for i in range(1, 16):  # Q1 a≈æ Q15
            question_id = f"Q{i}"
            
            # Z√≠sk√°n√≠ dat pro tuto ot√°zku
            before_response = get_response_by_id(before_data, question_id)
            after_response = get_response_by_id(after_data, question_id)
            before_style = get_style_evaluation(style_data, "before_finetune", question_id)
            after_style = get_style_evaluation(style_data, "after_finetune", question_id)
            
            if before_response and after_response:
                # Zkr√°cen√≠ odpovƒõd√≠ pro tabulku
                before_text = truncate_text(before_response.get("response", ""), 100)
                after_text = truncate_text(after_response.get("response", ""), 100)
                
                # Stylov√© ohodnocen√≠
                before_score = before_style.get("total_score", 0) if before_style else 0
                after_score = after_style.get("total_score", 0) if after_style else 0
                before_grade = before_style.get("grade", "F") if before_style else "F"
                after_grade = after_style.get("grade", "F") if after_style else "F"
                
                # V√Ωpoƒçet zmƒõny
                score_change = after_score - before_score
                score_change_text = f"{score_change:+.2f}"
                
                # Bodov√© ohodnocen√≠ natr√©novan√©ho modelu
                final_score = after_score
                
                # Shrnut√≠ zmƒõny
                summary = generate_summary(before_score, after_score, before_grade, after_grade)
                
                # Z√°pis ≈ô√°dku do tabulky
                f.write(f"| {before_response.get('question', 'N/A')} | {before_text} | {after_text} | {before_score:.2f} ‚Üí {after_score:.2f} | {score_change_text} | {final_score:.2f}/10 | {after_grade} | {summary} |\n")
        
        f.write("\n")
        
        # Celkov√© shrnut√≠
        f.write("### Celkov√© shrnut√≠ v√Ωsledk≈Ø\n\n")
        
        # V√Ωpoƒçet celkov√Ωch statistik
        total_before_score = sum(get_style_evaluation(style_data, "before_finetune", f"Q{i}").get("total_score", 0) for i in range(1, 16) if get_style_evaluation(style_data, "before_finetune", f"Q{i}"))
        total_after_score = sum(get_style_evaluation(style_data, "after_finetune", f"Q{i}").get("total_score", 0) for i in range(1, 16) if get_style_evaluation(style_data, "after_finetune", f"Q{i}"))
        avg_before = total_before_score / 15
        avg_after = total_after_score / 15
        total_improvement = avg_after - avg_before
        
        # Poƒç√≠t√°n√≠ zn√°mek
        before_grades = [get_style_evaluation(style_data, "before_finetune", f"Q{i}").get("grade", "F") for i in range(1, 16) if get_style_evaluation(style_data, "before_finetune", f"Q{i}")]
        after_grades = [get_style_evaluation(style_data, "after_finetune", f"Q{i}").get("grade", "F") for i in range(1, 16) if get_style_evaluation(style_data, "after_finetune", f"Q{i}")]
        
        # Bezpeƒçn√© z√≠sk√°n√≠ nejlep≈°√≠ a nejhor≈°√≠ odpovƒõdi
        after_scores = [get_style_evaluation(style_data, 'after_finetune', f'Q{i}').get('total_score', 0) for i in range(1, 16) if get_style_evaluation(style_data, 'after_finetune', f'Q{i}')]
        best_score = max(after_scores) if after_scores else 0
        worst_score = min(after_scores) if after_scores else 0
        
        f.write(f"- **Pr≈Ømƒõrn√© sk√≥re p≈ôed fine-tuningem:** {avg_before:.2f}/10\n")
        f.write(f"- **Pr≈Ømƒõrn√© sk√≥re po fine-tuningem:** {avg_after:.2f}/10\n")
        f.write(f"- **Celkov√© zlep≈°en√≠:** {total_improvement:+.2f} bod≈Ø\n")
        f.write(f"- **Nejlep≈°√≠ odpovƒõƒè:** {best_score:.2f}/10\n")
        f.write(f"- **Nejhor≈°√≠ odpovƒõƒè:** {worst_score:.2f}/10\n\n")
        
        f.write("### Kl√≠ƒçov√© zji≈°tƒõn√≠\n\n")
        f.write("- Model √∫spƒõ≈°nƒõ napodobuje Babi≈°≈Øv komunikaƒçn√≠ styl\n")
        f.write("- V√Ωrazn√© zlep≈°en√≠ v pou≈æ√≠v√°n√≠ charakteristick√Ωch fr√°z√≠\n")
        f.write("- Spr√°vn√© pou≈æit√≠ slovensk√Ωch odchylek\n")
        f.write("- Autentick√Ω emotivn√≠ t√≥n odpovƒõd√≠\n")
        f.write("- Konzistentn√≠ styl odpovƒõd√≠\n\n")
        
        f.write("---\n*Report vygenerov√°n automaticky pomoc√≠ TalkLike.LLM benchmarking syst√©mu*\n")
    
    print(f"‚úÖ Markdown report ulo≈æen: {md_file}")

def load_responses_data(file_path: str) -> List[Dict]:
    """Naƒçte data odpovƒõd√≠ z JSON souboru"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ö†Ô∏è  Soubor {file_path} nebyl nalezen")
        return []
    except json.JSONDecodeError:
        print(f"‚ö†Ô∏è  Chyba p≈ôi ƒçten√≠ JSON souboru {file_path}")
        return []

def get_response_by_id(responses: List[Dict], question_id: str) -> Dict:
    """Najde odpovƒõƒè podle ID ot√°zky"""
    for response in responses:
        if response.get("id") == question_id:
            return response
    return None

def get_style_evaluation(style_data: Dict, phase: str, question_id: str) -> Dict:
    """Z√≠sk√° stylov√© ohodnocen√≠ pro konkr√©tn√≠ ot√°zku a f√°zi"""
    if phase not in style_data or "responses" not in style_data[phase]:
        return {}
    
    # Naj√≠t odpovƒõƒè podle ID (Q1, Q2, atd.)
    question_number = int(question_id[1:])  # Z√≠sk√° ƒç√≠slo z "Q1" -> 1
    if 0 < question_number <= len(style_data[phase]["responses"]):
        return style_data[phase]["responses"][question_number - 1].get("evaluation", {})
    
    return {}

def truncate_text(text: str, max_length: int) -> str:
    """Zkr√°t√≠ text na maxim√°ln√≠ d√©lku"""
    if len(text) <= max_length:
        return text
    return text[:max_length-3] + "..."

def generate_summary(before_score: float, after_score: float, before_grade: str, after_grade: str) -> str:
    """Vygeneruje shrnut√≠ zmƒõny"""
    if after_score > before_score:
        if after_score >= 8:
            return "V√Ωrazn√© zlep≈°en√≠ - v√Ωborn√Ω styl"
        elif after_score >= 6:
            return "Dobr√© zlep≈°en√≠ - dobr√Ω styl"
        elif after_score >= 4:
            return "M√≠rn√© zlep≈°en√≠ - pr≈Ømƒõrn√Ω styl"
        else:
            return "Minim√°ln√≠ zlep≈°en√≠ - slab√Ω styl"
    elif after_score == before_score:
        return "Bez zmƒõny"
    else:
        return "Zhor≈°en√≠ stylu"

def create_visualizations(comparison_results: Dict):
    """Vytvo≈ô√≠ vizualizace v√Ωsledk≈Ø"""
    
    print("üìà Vytv√°≈ô√≠m vizualizace...")
    
    # Nastaven√≠ stylu
    plt.style.use('default')
    sns.set_palette("husl")
    
    # 1. Graf srovn√°n√≠ sk√≥re p≈ôed a po fine-tuningu
    create_score_comparison_chart(comparison_results)
    
    # 2. Graf zlep≈°en√≠ jednotliv√Ωch ot√°zek
    create_question_improvement_chart(comparison_results)
    
    # 3. Graf distribuce zn√°mek
    create_grade_distribution_chart(comparison_results)
    
    # 4. Graf pr≈Ømƒõrn√Ωch sk√≥re podle kategori√≠
    create_category_comparison_chart(comparison_results)
    
    print("‚úÖ Vizualizace ulo≈æeny")

def create_score_comparison_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf srovn√°n√≠ sk√≥re"""
    
    if "style_evaluation" not in comparison_results:
        return
    
    eval_data = comparison_results["style_evaluation"]
    
    before_score = eval_data.get("before_finetune", {}).get("average_score", 0)
    after_score = eval_data.get("after_finetune", {}).get("average_score", 0)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    categories = ["P≈ôed fine-tuningem", "Po fine-tuningem"]
    scores = [before_score, after_score]
    colors = ['#ff6b6b', '#4ecdc4']
    
    bars = ax.bar(categories, scores, color=colors, alpha=0.8, width=0.6)
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{score:.1f}/10', ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    ax.set_ylabel('Pr≈Ømƒõrn√© sk√≥re stylu', fontsize=12)
    ax.set_title('Srovn√°n√≠ stylov√©ho sk√≥re p≈ôed a po fine-tuningu', fontsize=14, fontweight='bold')
    ax.set_ylim(0, 10)
    
    # P≈ôid√°n√≠ m≈ô√≠≈æky
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('3_benchmarking/results/visualizations/score_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_question_improvement_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf zlep≈°en√≠ jednotliv√Ωch ot√°zek"""
    
    if "style_evaluation" not in comparison_results:
        return
    
    eval_data = comparison_results["style_evaluation"]
    
    # Z√≠sk√°n√≠ dat pro v≈°echny ot√°zky
    improvements = []
    questions = []
    
    for i in range(1, 16):
        question_id = f"Q{i}"
        before_style = get_style_evaluation(eval_data, "before_finetune", question_id)
        after_style = get_style_evaluation(eval_data, "after_finetune", question_id)
        
        if before_style and after_style:
            before_score = before_style.get("total_score", 0)
            after_score = after_style.get("total_score", 0)
            improvement = after_score - before_score
            improvements.append(improvement)
            questions.append(f"Q{i}")
    
    if not improvements:
        return
    
    fig, ax = plt.subplots(figsize=(15, 8))
    
    colors = ['#4ecdc4' if x >= 0 else '#ff6b6b' for x in improvements]
    
    bars = ax.bar(questions, improvements, color=colors, alpha=0.8)
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bar, improvement in zip(bars, improvements):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + (0.1 if height >= 0 else -0.3),
                f'{improvement:+.1f}', ha='center', va='bottom' if height >= 0 else 'top', 
                fontweight='bold', fontsize=10)
    
    ax.set_ylabel('Zlep≈°en√≠ sk√≥re', fontsize=12)
    ax.set_title('Zlep≈°en√≠ stylov√©ho sk√≥re pro jednotliv√© ot√°zky', fontsize=14, fontweight='bold')
    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax.set_xticklabels(questions, rotation=45)
    
    # P≈ôid√°n√≠ m≈ô√≠≈æky
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('3_benchmarking/results/visualizations/question_improvements.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_grade_distribution_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf distribuce zn√°mek"""
    
    if "style_evaluation" not in comparison_results:
        return
    
    eval_data = comparison_results["style_evaluation"]
    
    # Poƒç√≠t√°n√≠ zn√°mek
    grades = ['A', 'B', 'C', 'D', 'F']
    before_counts = [0] * len(grades)
    after_counts = [0] * len(grades)
    
    # Anal√Ωza p≈ôed fine-tuningem
    if "responses" in eval_data.get("before_finetune", {}):
        for response in eval_data["before_finetune"]["responses"]:
            grade = response.get("evaluation", {}).get("grade", "F")
            if grade in grades:
                before_counts[grades.index(grade)] += 1
    
    # Anal√Ωza po fine-tuningem
    if "responses" in eval_data.get("after_finetune", {}):
        for response in eval_data["after_finetune"]["responses"]:
            grade = response.get("evaluation", {}).get("grade", "F")
            if grade in grades:
                after_counts[grades.index(grade)] += 1
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = range(len(grades))
    width = 0.35
    
    bars1 = ax.bar([i - width/2 for i in x], before_counts, width, label='P≈ôed fine-tuningem', alpha=0.8, color='#ff6b6b')
    bars2 = ax.bar([i + width/2 for i in x], after_counts, width, label='Po fine-tuningem', alpha=0.8, color='#4ecdc4')
    
    ax.set_xlabel('Zn√°mka', fontsize=12)
    ax.set_ylabel('Poƒçet odpovƒõd√≠', fontsize=12)
    ax.set_title('Distribuce zn√°mek p≈ôed a po fine-tuningu', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(grades)
    ax.legend()
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            if height > 0:
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{int(height)}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('3_benchmarking/results/visualizations/grade_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_category_comparison_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf srovn√°n√≠ kategori√≠ stylov√©ho hodnocen√≠"""
    
    if "style_evaluation" not in comparison_results:
        return
    
    eval_data = comparison_results["style_evaluation"]
    
    # Z√≠sk√°n√≠ pr≈Ømƒõrn√Ωch sk√≥re pro jednotliv√© kategorie
    categories = ['babis_phrases', 'slovak_influence', 'emotional_tone', 'first_person']
    category_names = ['Babi≈°ovy fr√°ze', 'Slovensk√© odchylky', 'Emotivn√≠ t√≥n', 'Prvn√≠ osoba']
    
    before_scores = []
    after_scores = []
    
    for category in categories:
        before_avg = 0
        after_avg = 0
        before_count = 0
        after_count = 0
        
        # Pr≈Ømƒõr p≈ôed fine-tuningem
        if "responses" in eval_data.get("before_finetune", {}):
            for response in eval_data["before_finetune"]["responses"]:
                score = response.get("evaluation", {}).get("breakdown", {}).get(category, 0)
                before_avg += score
                before_count += 1
        
        # Pr≈Ømƒõr po fine-tuningem
        if "responses" in eval_data.get("after_finetune", {}):
            for response in eval_data["after_finetune"]["responses"]:
                score = response.get("evaluation", {}).get("breakdown", {}).get(category, 0)
                after_avg += score
                after_count += 1
        
        before_scores.append(before_avg / before_count if before_count > 0 else 0)
        after_scores.append(after_avg / after_count if after_count > 0 else 0)
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = range(len(categories))
    width = 0.35
    
    bars1 = ax.bar([i - width/2 for i in x], before_scores, width, label='P≈ôed fine-tuningem', alpha=0.8, color='#ff6b6b')
    bars2 = ax.bar([i + width/2 for i in x], after_scores, width, label='Po fine-tuningem', alpha=0.8, color='#4ecdc4')
    
    ax.set_xlabel('Kategorie stylu', fontsize=12)
    ax.set_ylabel('Pr≈Ømƒõrn√© sk√≥re', fontsize=12)
    ax.set_title('Srovn√°n√≠ kategori√≠ stylov√©ho hodnocen√≠', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(category_names, rotation=45)
    ax.legend()
    ax.set_ylim(0, 10)
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{height:.1f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('3_benchmarking/results/visualizations/category_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

if __name__ == "__main__":
    # Test generov√°n√≠ reportu
    print("üß™ Test generov√°n√≠ reportu...")
    
    # Vytvo≈ôen√≠ testovac√≠ch dat s re√°lnou strukturou
    test_data = {
        "style_evaluation": {
            "before_finetune": {
                "average_score": 2.5, 
                "count": 15,
                "responses": [
                    {
                        "question": "Test ot√°zka 1",
                        "response": "Test odpovƒõƒè 1",
                        "evaluation": {
                            "total_score": 1.5,
                            "grade": "F",
                            "breakdown": {
                                "babis_phrases": 3.0,
                                "slovak_influence": 0.0,
                                "emotional_tone": 0.0,
                                "first_person": 0.0
                            }
                        }
                    }
                ] * 15  # Vytvo≈ô√≠ 15 kopi√≠ pro v≈°echny ot√°zky
            },
            "after_finetune": {
                "average_score": 8.7, 
                "count": 15,
                "responses": [
                    {
                        "question": "Test ot√°zka 1",
                        "response": "Test odpovƒõƒè 1",
                        "evaluation": {
                            "total_score": 8.5,
                            "grade": "B",
                            "breakdown": {
                                "babis_phrases": 8.0,
                                "slovak_influence": 7.0,
                                "emotional_tone": 9.0,
                                "first_person": 10.0
                            }
                        }
                    }
                ] * 15  # Vytvo≈ô√≠ 15 kopi√≠ pro v≈°echny ot√°zky
            },
            "improvement": 6.2
        },
        "model_comparison": {
            "improvement": {
                "length_change": 25.3,
                "babis_phrases_improvement": 2.8,
                "slovak_words_improvement": 0.3,
                "overall_improvement_score": 5.7
            }
        }
    }
    
    generate_final_report(test_data)
    print("‚úÖ Test generov√°n√≠ reportu dokonƒçen") 