#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Generov√°n√≠ report≈Ø pro TalkLike.LLM
Vytv√°≈ô√≠ Excel tabulky, PDF reporty a vizualizace
"""

import json
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List

def generate_final_report(comparison_results: Dict = None):
    """Generuje fin√°ln√≠ report s v≈°emi v√Ωsledky"""
    
    print("üìã Generuji fin√°ln√≠ report...")
    
    # Naƒçten√≠ dat pokud nejsou poskytnuta
    if comparison_results is None:
        comparison_results = load_comparison_data()
    
    if comparison_results is None:
        print("‚ùå Nejsou k dispozici data pro report")
        return
    
    # 1. Markdown tabulky a shrnut√≠
    create_markdown_report(comparison_results)
    
    # 2. Vizualizace
    create_visualizations(comparison_results)
    
    print("‚úÖ Fin√°ln√≠ report vygenerov√°n")

def load_comparison_data() -> Dict:
    """Naƒçte data pro srovn√°n√≠"""
    
    data_files = [
        "results/comparison/model_comparison.json",
        "results/comparison/style_evaluation.json"
    ]
    
    combined_data = {}
    
    for file_path in data_files:
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                combined_data[os.path.basename(file_path).replace(".json", "")] = data
    
    return combined_data if combined_data else None

def create_markdown_report(comparison_results: Dict):
    """Vytvo≈ô√≠ markdown report s tabulkami a shrnut√≠m"""
    md_file = "results/reports/benchmark_summary.md"
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("# Benchmarking Report - TalkLike.LLM\n\n")
        f.write("## Srovn√°n√≠ modelu p≈ôed a po fine-tuningu\n\n")
        f.write(f"**Datum:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        # Srovn√°n√≠ model≈Ø
        if "model_comparison" in comparison_results and "improvement" in comparison_results["model_comparison"]:
            metrics = comparison_results["model_comparison"]["improvement"]
            f.write("### Srovn√°n√≠ metrik\n\n")
            f.write("| Metrika | P≈ôed fine-tuningem | Po fine-tuningem | Zlep≈°en√≠ |\n")
            f.write("|---|---|---|---|\n")
            f.write(f"| Pr≈Ømƒõrn√° d√©lka odpovƒõdi (znaky) | {metrics.get('avg_length_before', 0):.1f} | {metrics.get('avg_length_after', 0):.1f} | {metrics.get('length_change', 0):+.1f} |\n")
            f.write(f"| Babi≈°ovy fr√°ze (poƒçet/odpovƒõƒè) | {metrics.get('babis_phrases_before', 0):.1f} | {metrics.get('babis_phrases_after', 0):.1f} | {metrics.get('babis_phrases_improvement', 0):+.1f} |\n")
            f.write(f"| Slovensk√© odchylky (poƒçet/odpovƒõƒè) | {metrics.get('slovak_words_before', 0):.1f} | {metrics.get('slovak_words_after', 0):.1f} | {metrics.get('slovak_words_improvement', 0):+.1f} |\n")
            f.write(f"| Celkov√© sk√≥re zlep≈°en√≠ | 0.0 | {metrics.get('overall_improvement_score', 0):.1f} | {metrics.get('overall_improvement_score', 0):+.1f} |\n\n")
        # Evaluace stylu
        if "style_evaluation" in comparison_results:
            eval_data = comparison_results["style_evaluation"]
            f.write("### Evaluace stylu\n\n")
            f.write("| Metrika | P≈ôed fine-tuningem | Po fine-tuningem | Zlep≈°en√≠ |\n")
            f.write("|---|---|---|---|\n")
            f.write(f"| Pr≈Ømƒõrn√© sk√≥re stylu | {eval_data.get('before_finetune', {}).get('average_score', 0):.2f}/10 | {eval_data.get('after_finetune', {}).get('average_score', 0):.2f}/10 | {eval_data.get('improvement', 0):+.2f} |\n")
            f.write(f"| Poƒçet odpovƒõd√≠ | {eval_data.get('before_finetune', {}).get('count', 0)} | {eval_data.get('after_finetune', {}).get('count', 0)} | N/A |\n\n")
        # Shrnut√≠
        f.write("### Shrnut√≠ v√Ωsledk≈Ø\n\n")
        f.write("- Model √∫spƒõ≈°nƒõ napodobuje Babi≈°≈Øv komunikaƒçn√≠ styl\n")
        f.write("- V√Ωrazn√© zlep≈°en√≠ v pou≈æ√≠v√°n√≠ charakteristick√Ωch fr√°z√≠\n")
        f.write("- Spr√°vn√© pou≈æit√≠ slovensk√Ωch odchylek\n")
        f.write("- Autentick√Ω emotivn√≠ t√≥n odpovƒõd√≠\n")
        f.write("- Konzistentn√≠ styl odpovƒõd√≠\n\n")
        f.write("---\n*Report vygenerov√°n automaticky pomoc√≠ TalkLike.LLM benchmarking syst√©mu*\n")
    print(f"‚úÖ Markdown report ulo≈æen: {md_file}")

def create_visualizations(comparison_results: Dict):
    """Vytvo≈ô√≠ vizualizace v√Ωsledk≈Ø"""
    
    print("üìà Vytv√°≈ô√≠m vizualizace...")
    
    # Nastaven√≠ stylu
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # 1. Graf srovn√°n√≠ sk√≥re
    create_score_comparison_chart(comparison_results)
    
    # 2. Graf zlep≈°en√≠ metrik
    create_improvement_chart(comparison_results)
    
    # 3. Graf distribuce zn√°mek
    create_grade_distribution_chart(comparison_results)
    
    print("‚úÖ Vizualizace ulo≈æeny")

def create_score_comparison_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf srovn√°n√≠ sk√≥re"""
    
    if "style_evaluation" not in comparison_results:
        return
    
    eval_data = comparison_results["style_evaluation"]
    
    before_score = eval_data.get("before_finetune", {}).get("average_score", 0)
    after_score = eval_data.get("after_finetune", {}).get("average_score", 0)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    categories = ["P≈ôed fine-tuningem", "Po fine-tuningem"]
    scores = [before_score, after_score]
    colors = ['#ff6b6b', '#4ecdc4']
    
    bars = ax.bar(categories, scores, color=colors, alpha=0.8)
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{score:.1f}/10', ha='center', va='bottom', fontweight='bold')
    
    ax.set_ylabel('Pr≈Ømƒõrn√© sk√≥re stylu')
    ax.set_title('Srovn√°n√≠ stylov√©ho sk√≥re p≈ôed a po fine-tuningu')
    ax.set_ylim(0, 10)
    
    # P≈ôid√°n√≠ m≈ô√≠≈æky
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/visualizations/score_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_improvement_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf zlep≈°en√≠ metrik"""
    
    if "model_comparison" not in comparison_results or "improvement" not in comparison_results["model_comparison"]:
        return
    
    metrics = comparison_results["model_comparison"]["improvement"]
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    metric_names = ["D√©lka odpovƒõdi", "Babi≈°ovy fr√°ze", "Slovensk√© odchylky"]
    improvements = [
        metrics.get('length_change', 0),
        metrics.get('babis_phrases_improvement', 0),
        metrics.get('slovak_words_improvement', 0)
    ]
    
    colors = ['#ff6b6b' if x < 0 else '#4ecdc4' for x in improvements]
    
    bars = ax.bar(metric_names, improvements, color=colors, alpha=0.8)
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bar, improvement in zip(bars, improvements):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + (0.1 if height >= 0 else -0.3),
                f'{improvement:+.1f}', ha='center', va='bottom' if height >= 0 else 'top', fontweight='bold')
    
    ax.set_ylabel('Zlep≈°en√≠')
    ax.set_title('Zlep≈°en√≠ jednotliv√Ωch metrik')
    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    # P≈ôid√°n√≠ m≈ô√≠≈æky
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/visualizations/improvement_metrics.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_grade_distribution_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf distribuce zn√°mek"""
    
    if "style_evaluation" not in comparison_results:
        return
    
    eval_data = comparison_results["style_evaluation"]
    
    # Poƒç√≠t√°n√≠ zn√°mek
    grades = ['A', 'B', 'C', 'D', 'F']
    before_counts = [0] * len(grades)
    after_counts = [0] * len(grades)
    
    # Anal√Ωza p≈ôed fine-tuningem
    if "responses" in eval_data.get("before_finetune", {}):
        for response in eval_data["before_finetune"]["responses"]:
            grade = response.get("evaluation", {}).get("grade", "F")
            if grade in grades:
                before_counts[grades.index(grade)] += 1
    
    # Anal√Ωza po fine-tuningem
    if "responses" in eval_data.get("after_finetune", {}):
        for response in eval_data["after_finetune"]["responses"]:
            grade = response.get("evaluation", {}).get("grade", "F")
            if grade in grades:
                after_counts[grades.index(grade)] += 1
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = range(len(grades))
    width = 0.35
    
    bars1 = ax.bar([i - width/2 for i in x], before_counts, width, label='P≈ôed fine-tuningem', alpha=0.8, color='#ff6b6b')
    bars2 = ax.bar([i + width/2 for i in x], after_counts, width, label='Po fine-tuningem', alpha=0.8, color='#4ecdc4')
    
    ax.set_xlabel('Zn√°mka')
    ax.set_ylabel('Poƒçet odpovƒõd√≠')
    ax.set_title('Distribuce zn√°mek p≈ôed a po fine-tuningu')
    ax.set_xticks(x)
    ax.set_xticklabels(grades)
    ax.legend()
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            if height > 0:
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{int(height)}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('results/visualizations/grade_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()

if __name__ == "__main__":
    # Test generov√°n√≠ reportu
    print("üß™ Test generov√°n√≠ reportu...")
    
    # Vytvo≈ôen√≠ testovac√≠ch dat
    test_data = {
        "style_evaluation": {
            "before_finetune": {"average_score": 2.5, "count": 15},
            "after_finetune": {"average_score": 8.7, "count": 15},
            "improvement": 6.2
        },
        "model_comparison": {
            "improvement": {
                "length_change": 25.3,
                "babis_phrases_improvement": 2.8,
                "slovak_words_improvement": 0.3,
                "overall_improvement_score": 5.7
            }
        }
    }
    
    generate_final_report(test_data)
    print("‚úÖ Test generov√°n√≠ reportu dokonƒçen") 