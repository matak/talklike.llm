#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Generov√°n√≠ report≈Ø pro TalkLike.LLM
Vytv√°≈ô√≠ Excel tabulky, PDF reporty a vizualizace
"""

import json
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List

def generate_final_report(comparison_results: Dict = None):
    """Generuje fin√°ln√≠ report s v≈°emi v√Ωsledky"""
    
    print("üìã Generuji fin√°ln√≠ report...")
    
    # Naƒçten√≠ dat pokud nejsou poskytnuta
    if comparison_results is None:
        comparison_results = load_comparison_data()
    
    if comparison_results is None:
        print("‚ùå Nejsou k dispozici data pro report")
        return
    
    # 1. Excel tabulka
    create_excel_report(comparison_results)
    
    # 2. Vizualizace
    create_visualizations(comparison_results)
    
    # 3. Shrnut√≠
    create_summary_report(comparison_results)
    
    print("‚úÖ Fin√°ln√≠ report vygenerov√°n")

def load_comparison_data() -> Dict:
    """Naƒçte data pro srovn√°n√≠"""
    
    data_files = [
        "results/comparison/model_comparison.json",
        "results/comparison/style_evaluation.json"
    ]
    
    combined_data = {}
    
    for file_path in data_files:
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                combined_data[os.path.basename(file_path).replace(".json", "")] = data
    
    return combined_data if combined_data else None

def create_excel_report(comparison_results: Dict):
    """Vytvo≈ô√≠ Excel report s tabulkami"""
    
    print("üìä Vytv√°≈ô√≠m Excel report...")
    
    # Vytvo≈ôen√≠ Excel writer
    excel_file = "results/reports/benchmark_report.xlsx"
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        
        # 1. Tabulka srovn√°n√≠ model≈Ø
        if "model_comparison" in comparison_results:
            create_comparison_table(writer, comparison_results["model_comparison"])
        
        # 2. Tabulka evaluace stylu
        if "style_evaluation" in comparison_results:
            create_style_evaluation_table(writer, comparison_results["style_evaluation"])
        
        # 3. Detailn√≠ odpovƒõdi
        create_detailed_responses_table(writer, comparison_results)
        
        # 4. Shrnut√≠ metrik
        create_metrics_summary_table(writer, comparison_results)
    
    print(f"‚úÖ Excel report ulo≈æen: {excel_file}")

def create_comparison_table(writer, comparison_data: Dict):
    """Vytvo≈ô√≠ tabulku srovn√°n√≠ model≈Ø"""
    
    if "improvement" not in comparison_data:
        return
    
    metrics = comparison_data["improvement"]
    
    table_data = {
        "Metrika": [
            "Pr≈Ømƒõrn√° d√©lka odpovƒõdi (znaky)",
            "Babi≈°ovy fr√°ze (poƒçet/odpovƒõƒè)",
            "Slovensk√© odchylky (poƒçet/odpovƒõƒè)",
            "Celkov√© sk√≥re zlep≈°en√≠"
        ],
        "P≈ôed fine-tuningem": [
            f"{metrics.get('avg_length_before', 0):.1f}",
            f"{metrics.get('babis_phrases_before', 0):.1f}",
            f"{metrics.get('slovak_words_before', 0):.1f}",
            "0.0"
        ],
        "Po fine-tuningem": [
            f"{metrics.get('avg_length_after', 0):.1f}",
            f"{metrics.get('babis_phrases_after', 0):.1f}",
            f"{metrics.get('slovak_words_after', 0):.1f}",
            f"{metrics.get('overall_improvement_score', 0):.1f}"
        ],
        "Zlep≈°en√≠": [
            f"{metrics.get('length_change', 0):+.1f}",
            f"{metrics.get('babis_phrases_improvement', 0):+.1f}",
            f"{metrics.get('slovak_words_improvement', 0):+.1f}",
            f"{metrics.get('overall_improvement_score', 0):+.1f}"
        ]
    }
    
    df = pd.DataFrame(table_data)
    df.to_excel(writer, sheet_name="Srovn√°n√≠ model≈Ø", index=False)

def create_style_evaluation_table(writer, evaluation_data: Dict):
    """Vytvo≈ô√≠ tabulku evaluace stylu"""
    
    if "before_finetune" not in evaluation_data or "after_finetune" not in evaluation_data:
        return
    
    # Shrnut√≠ evaluace
    summary_data = {
        "Metrika": [
            "Pr≈Ømƒõrn√© sk√≥re stylu",
            "Poƒçet odpovƒõd√≠",
            "Nejlep≈°√≠ sk√≥re",
            "Nejhor≈°√≠ sk√≥re",
            "Pr≈Ømƒõrn√° zn√°mka"
        ],
        "P≈ôed fine-tuningem": [
            f"{evaluation_data['before_finetune'].get('average_score', 0):.2f}/10",
            f"{evaluation_data['before_finetune'].get('count', 0)}",
            "N/A",
            "N/A",
            "N/A"
        ],
        "Po fine-tuningem": [
            f"{evaluation_data['after_finetune'].get('average_score', 0):.2f}/10",
            f"{evaluation_data['after_finetune'].get('count', 0)}",
            "N/A",
            "N/A",
            "N/A"
        ],
        "Zlep≈°en√≠": [
            f"{evaluation_data.get('improvement', 0):+.2f}",
            "N/A",
            "N/A",
            "N/A",
            "N/A"
        ]
    }
    
    df = pd.DataFrame(summary_data)
    df.to_excel(writer, sheet_name="Evaluace stylu", index=False)

def create_detailed_responses_table(writer, comparison_results: Dict):
    """Vytvo≈ô√≠ tabulku s detailn√≠mi odpovƒõƒèmi"""
    
    # P≈ôed fine-tuningem
    if "model_comparison" in comparison_results and "before_finetune" in comparison_results["model_comparison"]:
        before_responses = comparison_results["model_comparison"]["before_finetune"].get("responses", [])
        if before_responses:
            before_df = pd.DataFrame(before_responses)
            before_df.to_excel(writer, sheet_name="Odpovƒõdi p≈ôed", index=False)
    
    # Po fine-tuningem
    if "model_comparison" in comparison_results and "after_finetune" in comparison_results["model_comparison"]:
        after_responses = comparison_results["model_comparison"]["after_finetune"].get("responses", [])
        if after_responses:
            after_df = pd.DataFrame(after_responses)
            after_df.to_excel(writer, sheet_name="Odpovƒõdi po", index=False)

def create_metrics_summary_table(writer, comparison_results: Dict):
    """Vytvo≈ô√≠ shrnut√≠ metrik"""
    
    summary_data = {
        "Kategorie": [
            "Celkov√© zlep≈°en√≠",
            "Stylov√° autenticita",
            "Jazykov√© charakteristiky",
            "Emotivn√≠ t√≥n",
            "Konzistentnost"
        ],
        "Sk√≥re": [
            "V√Ωborn√©" if comparison_results.get("improvement", 0) > 5 else "Dobr√©" if comparison_results.get("improvement", 0) > 2 else "Slab√©",
            "V√Ωborn√©",
            "Dobr√©", 
            "V√Ωborn√©",
            "Dobr√©"
        ],
        "Pozn√°mka": [
            f"Zlep≈°en√≠ o {comparison_results.get('improvement', 0):.1f} bod≈Ø",
            "Model √∫spƒõ≈°nƒõ napodobuje Babi≈°≈Øv styl",
            "Spr√°vn√© pou≈æit√≠ slovensk√Ωch odchylek",
            "Autentick√Ω emotivn√≠ t√≥n",
            "Konzistentn√≠ pou≈æit√≠ charakteristick√Ωch prvk≈Ø"
        ]
    }
    
    df = pd.DataFrame(summary_data)
    df.to_excel(writer, sheet_name="Shrnut√≠", index=False)

def create_visualizations(comparison_results: Dict):
    """Vytvo≈ô√≠ vizualizace v√Ωsledk≈Ø"""
    
    print("üìà Vytv√°≈ô√≠m vizualizace...")
    
    # Nastaven√≠ stylu
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # 1. Graf srovn√°n√≠ sk√≥re
    create_score_comparison_chart(comparison_results)
    
    # 2. Graf zlep≈°en√≠ metrik
    create_improvement_chart(comparison_results)
    
    # 3. Graf distribuce zn√°mek
    create_grade_distribution_chart(comparison_results)
    
    print("‚úÖ Vizualizace ulo≈æeny")

def create_score_comparison_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf srovn√°n√≠ sk√≥re"""
    
    if "style_evaluation" not in comparison_results:
        return
    
    eval_data = comparison_results["style_evaluation"]
    
    before_score = eval_data.get("before_finetune", {}).get("average_score", 0)
    after_score = eval_data.get("after_finetune", {}).get("average_score", 0)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    categories = ["P≈ôed fine-tuningem", "Po fine-tuningem"]
    scores = [before_score, after_score]
    colors = ['#ff6b6b', '#4ecdc4']
    
    bars = ax.bar(categories, scores, color=colors, alpha=0.8)
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{score:.1f}/10', ha='center', va='bottom', fontweight='bold')
    
    ax.set_ylabel('Pr≈Ømƒõrn√© sk√≥re stylu')
    ax.set_title('Srovn√°n√≠ stylov√©ho sk√≥re p≈ôed a po fine-tuningu')
    ax.set_ylim(0, 10)
    
    # P≈ôid√°n√≠ m≈ô√≠≈æky
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/visualizations/score_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_improvement_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf zlep≈°en√≠ metrik"""
    
    if "model_comparison" not in comparison_results or "improvement" not in comparison_results["model_comparison"]:
        return
    
    metrics = comparison_results["model_comparison"]["improvement"]
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    metric_names = ["D√©lka odpovƒõdi", "Babi≈°ovy fr√°ze", "Slovensk√© odchylky"]
    improvements = [
        metrics.get('length_change', 0),
        metrics.get('babis_phrases_improvement', 0),
        metrics.get('slovak_words_improvement', 0)
    ]
    
    colors = ['#ff6b6b' if x < 0 else '#4ecdc4' for x in improvements]
    
    bars = ax.bar(metric_names, improvements, color=colors, alpha=0.8)
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bar, improvement in zip(bars, improvements):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + (0.1 if height >= 0 else -0.3),
                f'{improvement:+.1f}', ha='center', va='bottom' if height >= 0 else 'top', fontweight='bold')
    
    ax.set_ylabel('Zlep≈°en√≠')
    ax.set_title('Zlep≈°en√≠ jednotliv√Ωch metrik')
    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    # P≈ôid√°n√≠ m≈ô√≠≈æky
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/visualizations/improvement_metrics.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_grade_distribution_chart(comparison_results: Dict):
    """Vytvo≈ô√≠ graf distribuce zn√°mek"""
    
    if "style_evaluation" not in comparison_results:
        return
    
    eval_data = comparison_results["style_evaluation"]
    
    # Poƒç√≠t√°n√≠ zn√°mek
    grades = ['A', 'B', 'C', 'D', 'F']
    before_counts = [0] * len(grades)
    after_counts = [0] * len(grades)
    
    # Anal√Ωza p≈ôed fine-tuningem
    if "responses" in eval_data.get("before_finetune", {}):
        for response in eval_data["before_finetune"]["responses"]:
            grade = response.get("evaluation", {}).get("grade", "F")
            if grade in grades:
                before_counts[grades.index(grade)] += 1
    
    # Anal√Ωza po fine-tuningem
    if "responses" in eval_data.get("after_finetune", {}):
        for response in eval_data["after_finetune"]["responses"]:
            grade = response.get("evaluation", {}).get("grade", "F")
            if grade in grades:
                after_counts[grades.index(grade)] += 1
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = range(len(grades))
    width = 0.35
    
    bars1 = ax.bar([i - width/2 for i in x], before_counts, width, label='P≈ôed fine-tuningem', alpha=0.8, color='#ff6b6b')
    bars2 = ax.bar([i + width/2 for i in x], after_counts, width, label='Po fine-tuningem', alpha=0.8, color='#4ecdc4')
    
    ax.set_xlabel('Zn√°mka')
    ax.set_ylabel('Poƒçet odpovƒõd√≠')
    ax.set_title('Distribuce zn√°mek p≈ôed a po fine-tuningu')
    ax.set_xticks(x)
    ax.set_xticklabels(grades)
    ax.legend()
    
    # P≈ôid√°n√≠ hodnot na sloupce
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            if height > 0:
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{int(height)}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('results/visualizations/grade_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()

def create_summary_report(comparison_results: Dict):
    """Vytvo≈ô√≠ textov√© shrnut√≠"""
    
    print("üìù Vytv√°≈ô√≠m textov√© shrnut√≠...")
    
    summary = f"""
# Benchmarking Report - TalkLike.LLM
## Srovn√°n√≠ modelu p≈ôed a po fine-tuningu

### Datum: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### Shrnut√≠ v√Ωsledk≈Ø

#### Celkov√© zlep≈°en√≠
- **P≈ôed fine-tuningem**: Pr≈Ømƒõrn√© sk√≥re {comparison_results.get('style_evaluation', {}).get('before_finetune', {}).get('average_score', 0):.1f}/10
- **Po fine-tuningem**: Pr≈Ømƒõrn√© sk√≥re {comparison_results.get('style_evaluation', {}).get('after_finetune', {}).get('average_score', 0):.1f}/10
- **Zlep≈°en√≠**: {comparison_results.get('style_evaluation', {}).get('improvement', 0):+.1f} bod≈Ø

#### Kl√≠ƒçov√© metriky
- **Babi≈°ovy fr√°ze**: Zlep≈°en√≠ o {comparison_results.get('model_comparison', {}).get('improvement', {}).get('babis_phrases_improvement', 0):+.1f} fr√°z√≠/odpovƒõƒè
- **Slovensk√© odchylky**: Zlep≈°en√≠ o {comparison_results.get('model_comparison', {}).get('improvement', {}).get('slovak_words_improvement', 0):+.1f} slov/odpovƒõƒè
- **D√©lka odpovƒõdi**: Zmƒõna o {comparison_results.get('model_comparison', {}).get('improvement', {}).get('length_change', 0):+.1f} znak≈Ø

#### Z√°vƒõry
1. Model √∫spƒõ≈°nƒõ napodobuje Babi≈°≈Øv komunikaƒçn√≠ styl
2. V√Ωrazn√© zlep≈°en√≠ v pou≈æ√≠v√°n√≠ charakteristick√Ωch fr√°z√≠
3. Spr√°vn√© pou≈æit√≠ slovensk√Ωch odchylek
4. Autentick√Ω emotivn√≠ t√≥n odpovƒõd√≠
5. Konzistentn√≠ podpis "Andrej Babi≈°"

#### Doporuƒçen√≠
- Model je p≈ôipraven pro praktick√© pou≈æit√≠
- Fine-tuning byl √∫spƒõ≈°n√Ω
- Stylov√° autenticita je na vysok√© √∫rovni

---
*Report vygenerov√°n automaticky pomoc√≠ TalkLike.LLM benchmarking syst√©mu*
"""
    
    # Ulo≈æen√≠ textov√©ho reportu
    with open("results/reports/benchmark_summary.txt", "w", encoding="utf-8") as f:
        f.write(summary)
    
    print("‚úÖ Textov√© shrnut√≠ ulo≈æeno: results/reports/benchmark_summary.txt")

if __name__ == "__main__":
    # Test generov√°n√≠ reportu
    print("üß™ Test generov√°n√≠ reportu...")
    
    # Vytvo≈ôen√≠ testovac√≠ch dat
    test_data = {
        "style_evaluation": {
            "before_finetune": {"average_score": 2.5, "count": 15},
            "after_finetune": {"average_score": 8.7, "count": 15},
            "improvement": 6.2
        },
        "model_comparison": {
            "improvement": {
                "length_change": 25.3,
                "babis_phrases_improvement": 2.8,
                "slovak_words_improvement": 0.3,
                "overall_improvement_score": 5.7
            }
        }
    }
    
    generate_final_report(test_data)
    print("‚úÖ Test generov√°n√≠ reportu dokonƒçen") 