#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TestovacÃ­ skript pro benchmarking TalkLike.LLM
OvÄ›Å™uje funkÄnost vÅ¡ech komponent
"""

import os
import json
import sys
from datetime import datetime

def test_imports():
    """Testuje import vÅ¡ech modulÅ¯"""
    print("ğŸ” Testuji importy...")
    
    try:
        from evaluate_style import evaluate_babis_style, BabisStyleEvaluator
        print("âœ… evaluate_style.py - OK")
    except ImportError as e:
        print(f"âŒ evaluate_style.py - Chyba: {e}")
        return False
    
    try:
        from compare_models import compare_models, calculate_comparison_metrics
        print("âœ… compare_models.py - OK")
    except ImportError as e:
        print(f"âŒ compare_models.py - Chyba: {e}")
        return False
    
    try:
        from generate_responses import generate_responses, generate_mock_response
        print("âœ… generate_responses.py - OK")
    except ImportError as e:
        print(f"âŒ generate_responses.py - Chyba: {e}")
        return False
    
    try:
        from create_benchmark_dataset import create_benchmark_dataset, validate_benchmark_dataset
        print("âœ… create_benchmark_dataset.py - OK")
    except ImportError as e:
        print(f"âŒ create_benchmark_dataset.py - Chyba: {e}")
        return False
    
    return True

def test_data_files():
    """Testuje pÅ™Ã­tomnost datovÃ½ch souborÅ¯"""
    print("\nğŸ“ Testuji datovÃ© soubory...")
    
    required_files = [
        "benchmark_questions.json",
        "LLM.Benchmark.systemPrompt.md"
    ]
    
    all_exist = True
    for file in required_files:
        if os.path.exists(file):
            print(f"âœ… {file} - OK")
        else:
            print(f"âŒ {file} - ChybÃ­")
            all_exist = False
    
    return all_exist

def test_evaluation():
    """Testuje evaluaci stylu"""
    print("\nğŸ¯ Testuji evaluaci stylu...")
    
    try:
        from evaluate_style import evaluate_babis_style
        
        # Test Å¡patnÃ© odpovÄ›di (pÅ™ed fine-tuningem)
        bad_response = "Inflace je vÃ¡Å¾nÃ½ problÃ©m, kterÃ½ postihuje vÅ¡echny obÄany."
        bad_result = evaluate_babis_style(bad_response)
        
        print(f"   Å patnÃ¡ odpovÄ›Ä: {bad_result['total_score']}/10 ({bad_result['grade']})")
        
        # Test dobrÃ© odpovÄ›di (po fine-tuningem)
        good_response = "Hele, inflace je jak kdyÅ¾ krÃ¡va hraje na klavÃ­r! JÃ¡ makÃ¡m, ale opozice krade. To je skandÃ¡l! Andrej BabiÅ¡"
        good_result = evaluate_babis_style(good_response)
        
        print(f"   DobrÃ¡ odpovÄ›Ä: {good_result['total_score']}/10 ({good_result['grade']})")
        
        # Kontrola zlepÅ¡enÃ­
        improvement = good_result['total_score'] - bad_result['total_score']
        print(f"   ZlepÅ¡enÃ­: {improvement:.1f} bodÅ¯")
        
        if improvement > 0:
            print("âœ… Evaluace funguje sprÃ¡vnÄ›")
            return True
        else:
            print("âŒ Evaluace nefunguje sprÃ¡vnÄ›")
            return False
            
    except Exception as e:
        print(f"âŒ Chyba pÅ™i evaluaci: {e}")
        return False

def test_dataset_creation():
    """Testuje vytvoÅ™enÃ­ datasetu"""
    print("\nğŸ“‹ Testuji vytvoÅ™enÃ­ datasetu...")
    
    try:
        from create_benchmark_dataset import create_benchmark_dataset, validate_benchmark_dataset
        
        # VytvoÅ™enÃ­ datasetu
        dataset = create_benchmark_dataset()
        
        if dataset is None:
            print("âŒ NepodaÅ™ilo se vytvoÅ™it dataset")
            return False
        
        # Validace datasetu
        is_valid, message = validate_benchmark_dataset(dataset)
        
        if is_valid:
            print(f"âœ… Dataset je validnÃ­: {message}")
            print(f"   PoÄet otÃ¡zek: {len(dataset['questions'])}")
            return True
        else:
            print(f"âŒ Dataset nenÃ­ validnÃ­: {message}")
            return False
            
    except Exception as e:
        print(f"âŒ Chyba pÅ™i vytvÃ¡Å™enÃ­ datasetu: {e}")
        return False

def test_response_generation():
    """Testuje generovÃ¡nÃ­ odpovÄ›dÃ­"""
    print("\nğŸ¤– Testuji generovÃ¡nÃ­ odpovÄ›dÃ­...")
    
    try:
        from generate_responses import generate_mock_response
        
        test_question = "Pane BabiÅ¡i, jak hodnotÃ­te souÄasnou inflaci?"
        
        # Test pÅ™ed fine-tuningem
        base_response = generate_mock_response(test_question, "base")
        print(f"   PÅ™ed fine-tuningem: {base_response}")
        
        # Test po fine-tuningem
        finetuned_response = generate_mock_response(test_question, "finetuned")
        print(f"   Po fine-tuningem: {finetuned_response}")
        
        # Kontrola rozdÃ­lu
        if len(finetuned_response) > len(base_response):
            print("âœ… GenerovÃ¡nÃ­ odpovÄ›dÃ­ funguje")
            return True
        else:
            print("âŒ GenerovÃ¡nÃ­ odpovÄ›dÃ­ nefunguje sprÃ¡vnÄ›")
            return False
            
    except Exception as e:
        print(f"âŒ Chyba pÅ™i generovÃ¡nÃ­ odpovÄ›dÃ­: {e}")
        return False

def test_comparison():
    """Testuje srovnÃ¡nÃ­ modelÅ¯"""
    print("\nğŸ“Š Testuji srovnÃ¡nÃ­ modelÅ¯...")
    
    try:
        from compare_models import calculate_comparison_metrics
        
        # TestovacÃ­ data
        before_data = [
            {"response": "Inflace je vÃ¡Å¾nÃ½ problÃ©m."},
            {"response": "Opozice mÃ¡ prÃ¡vo na kritiku."}
        ]
        
        after_data = [
            {"response": "Hele, inflace je jak kdyÅ¾ krÃ¡va hraje na klavÃ­r! Andrej BabiÅ¡"},
            {"response": "Opozice krade, to je skandÃ¡l! Andrej BabiÅ¡"}
        ]
        
        # VÃ½poÄet metrik
        metrics = calculate_comparison_metrics(before_data, after_data)
        
        print(f"   PrÅ¯mÄ›rnÃ¡ dÃ©lka pÅ™ed: {metrics['avg_length_before']:.1f}")
        print(f"   PrÅ¯mÄ›rnÃ¡ dÃ©lka po: {metrics['avg_length_after']:.1f}")
        print(f"   ZlepÅ¡enÃ­: {metrics['overall_improvement_score']:.1f}")
        
        if metrics['overall_improvement_score'] > 0:
            print("âœ… SrovnÃ¡nÃ­ funguje sprÃ¡vnÄ›")
            return True
        else:
            print("âŒ SrovnÃ¡nÃ­ nefunguje sprÃ¡vnÄ›")
            return False
            
    except Exception as e:
        print(f"âŒ Chyba pÅ™i srovnÃ¡nÃ­: {e}")
        return False

def test_directory_structure():
    """Testuje strukturu adresÃ¡Å™Å¯"""
    print("\nğŸ“‚ Testuji strukturu adresÃ¡Å™Å¯...")
    
    # VytvoÅ™enÃ­ testovacÃ­ch adresÃ¡Å™Å¯
    test_dirs = [
        "results/before_finetune",
        "results/after_finetune",
        "results/comparison",
        "results/reports",
        "results/visualizations"
    ]
    
    for directory in test_dirs:
        os.makedirs(directory, exist_ok=True)
        if os.path.exists(directory):
            print(f"âœ… {directory} - OK")
        else:
            print(f"âŒ {directory} - Chyba pÅ™i vytvÃ¡Å™enÃ­")
            return False
    
    return True

def main():
    """HlavnÃ­ testovacÃ­ funkce"""
    print("ğŸ§ª SpouÅ¡tÃ­m testy pro benchmarking...")
    print("=" * 50)
    
    tests = [
        ("Importy", test_imports),
        ("DatovÃ© soubory", test_data_files),
        ("Struktura adresÃ¡Å™Å¯", test_directory_structure),
        ("Evaluace stylu", test_evaluation),
        ("VytvoÅ™enÃ­ datasetu", test_dataset_creation),
        ("GenerovÃ¡nÃ­ odpovÄ›dÃ­", test_response_generation),
        ("SrovnÃ¡nÃ­ modelÅ¯", test_comparison)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ NeoÄekÃ¡vanÃ¡ chyba v {test_name}: {e}")
            results.append((test_name, False))
    
    # ShrnutÃ­ vÃ½sledkÅ¯
    print("\n" + "=" * 50)
    print("ğŸ“‹ ShrnutÃ­ testÅ¯:")
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š VÃ½sledek: {passed}/{total} testÅ¯ ÃºspÄ›Å¡nÃ½ch")
    
    if passed == total:
        print("ğŸ‰ VÅ¡echny testy proÅ¡ly! Benchmarking je pÅ™ipraven.")
        return True
    else:
        print("âš ï¸  NÄ›kterÃ© testy selhaly. Zkontrolujte chyby vÃ½Å¡e.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 