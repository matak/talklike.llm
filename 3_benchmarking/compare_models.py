#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Srovn√°n√≠ model≈Ø pro TalkLike.LLM
Porovn√°v√° v√Ωkon p≈ôed a po fine-tuningu
"""

import json
import os
from datetime import datetime
from typing import Dict, List

def compare_models():
    """Porovn√°v√° modely p≈ôed a po fine-tuningu"""
    
    print("üìä Porovn√°v√°m modely p≈ôed a po fine-tuningu...")
    
    comparison_results = {
        "before_finetune": {},
        "after_finetune": {},
        "improvement": {},
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "total_questions": 0
        }
    }
    
    # Naƒçten√≠ odpovƒõd√≠ p≈ôed fine-tuningem
    with open("results/before_finetune/responses.json", "r", encoding="utf-8") as f:
        before_data = json.load(f)
    
    comparison_results["before_finetune"] = {
        "responses": before_data,
        "count": len(before_data)
    }
    print(f"‚úÖ Naƒçteno {len(before_data)} odpovƒõd√≠ p≈ôed fine-tuningem")
    
    # Naƒçten√≠ odpovƒõd√≠ po fine-tuningem
    with open("results/after_finetune/responses.json", "r", encoding="utf-8") as f:
        after_data = json.load(f)
    
    comparison_results["after_finetune"] = {
        "responses": after_data,
        "count": len(after_data)
    }
    print(f"‚úÖ Naƒçteno {len(after_data)} odpovƒõd√≠ po fine-tuningem")
    
    # V√Ωpoƒçet metrik
    metrics = calculate_comparison_metrics(before_data, after_data)
    comparison_results["improvement"] = metrics
    
    print(f"\nüìà Metriky srovn√°n√≠:")
    print(f"   Pr≈Ømƒõrn√° d√©lka odpovƒõdi:")
    print(f"     P≈ôed: {metrics['avg_length_before']:.1f} znak≈Ø")
    print(f"     Po: {metrics['avg_length_after']:.1f} znak≈Ø")
    print(f"     Zmƒõna: {metrics['length_change']:+.1f} znak≈Ø")
    
    print(f"   Pou≈æit√≠ Babi≈°ov√Ωch fr√°z√≠:")
    print(f"     P≈ôed: {metrics['babis_phrases_before']:.1f} fr√°z√≠/odpovƒõƒè")
    print(f"     Po: {metrics['babis_phrases_after']:.1f} fr√°z√≠/odpovƒõƒè")
    print(f"     Zlep≈°en√≠: {metrics['babis_phrases_improvement']:+.1f} fr√°z√≠")
    
    print(f"   Slovensk√© odchylky:")
    print(f"     P≈ôed: {metrics['slovak_words_before']:.1f} slov/odpovƒõƒè")
    print(f"     Po: {metrics['slovak_words_after']:.1f} slov/odpovƒõƒè")
    print(f"     Zlep≈°en√≠: {metrics['slovak_words_improvement']:+.1f} slov")
    
    # Ulo≈æen√≠ v√Ωsledk≈Ø srovn√°n√≠
    with open("results/comparison/model_comparison.json", "w", encoding="utf-8") as f:
        json.dump(comparison_results, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ Srovn√°n√≠ ulo≈æeno: results/comparison/model_comparison.json")
    
    return comparison_results

def calculate_comparison_metrics(before_data: List, after_data: List) -> Dict:
    """Vypoƒç√≠t√° metriky pro srovn√°n√≠ model≈Ø"""
    
    # Babi≈°ovy fr√°ze pro anal√Ωzu
    babis_phrases = [
        "hele", "skand√°l", "mak√°m", "opozice krade", "brusel",
        "to je", "j√° jsem", "moje rodina", "√∫≈ôady", "parlament"
    ]
    
    # Slovensk√© odchylky
    slovak_words = [
        "sme", "som", "mak√°me", "centraliz√°cia", "efektiviz√°cia"
    ]
    
    def analyze_responses(responses: List) -> Dict:
        """Analyzuje sadu odpovƒõd√≠"""
        total_length = 0
        total_babis_phrases = 0
        total_slovak_words = 0
        
        for resp in responses:
            text_lower = resp["response"].lower()
            total_length += len(resp["response"])
            
            # Poƒç√≠t√°n√≠ Babi≈°ov√Ωch fr√°z√≠
            for phrase in babis_phrases:
                if phrase in text_lower:
                    total_babis_phrases += 1
            
            # Poƒç√≠t√°n√≠ slovensk√Ωch slov
            for word in slovak_words:
                if word in text_lower:
                    total_slovak_words += 1
        
        return {
            "avg_length": total_length / len(responses) if responses else 0,
            "avg_babis_phrases": total_babis_phrases / len(responses) if responses else 0,
            "avg_slovak_words": total_slovak_words / len(responses) if responses else 0
        }
    
    # Anal√Ωza p≈ôed fine-tuningem
    before_metrics = analyze_responses(before_data)
    
    # Anal√Ωza po fine-tuningem
    after_metrics = analyze_responses(after_data)
    
    # V√Ωpoƒçet zlep≈°en√≠
    improvement = {
        "avg_length_before": before_metrics["avg_length"],
        "avg_length_after": after_metrics["avg_length"],
        "length_change": after_metrics["avg_length"] - before_metrics["avg_length"],
        
        "babis_phrases_before": before_metrics["avg_babis_phrases"],
        "babis_phrases_after": after_metrics["avg_babis_phrases"],
        "babis_phrases_improvement": after_metrics["avg_babis_phrases"] - before_metrics["avg_babis_phrases"],
        
        "slovak_words_before": before_metrics["avg_slovak_words"],
        "slovak_words_after": after_metrics["avg_slovak_words"],
        "slovak_words_improvement": after_metrics["avg_slovak_words"] - before_metrics["avg_slovak_words"],
        
        "overall_improvement_score": (
            (after_metrics["avg_babis_phrases"] - before_metrics["avg_babis_phrases"]) * 2 +
            (after_metrics["avg_slovak_words"] - before_metrics["avg_slovak_words"]) * 1.5
        )
    }
    
    return improvement

def create_comparison_table():
    """Vytvo≈ô√≠ tabulku pro srovn√°n√≠"""
    
    with open("results/comparison/model_comparison.json", "r", encoding="utf-8") as f:
        comparison_data = json.load(f)
    
    metrics = comparison_data["improvement"]
    
    # Vytvo≈ôen√≠ tabulky
    table_data = {
        "Metrika": [
            "Pr≈Ømƒõrn√° d√©lka odpovƒõdi (znaky)",
            "Babi≈°ovy fr√°ze (poƒçet/odpovƒõƒè)",
            "Slovensk√© odchylky (poƒçet/odpovƒõƒè)",
            "Celkov√© sk√≥re zlep≈°en√≠"
        ],
        "P≈ôed fine-tuningem": [
            f"{metrics['avg_length_before']:.1f}",
            f"{metrics['babis_phrases_before']:.1f}",
            f"{metrics['slovak_words_before']:.1f}",
            "0.0"
        ],
        "Po fine-tuningem": [
            f"{metrics['avg_length_after']:.1f}",
            f"{metrics['babis_phrases_after']:.1f}",
            f"{metrics['slovak_words_after']:.1f}",
            f"{metrics['overall_improvement_score']:.1f}"
        ],
        "Zlep≈°en√≠": [
            f"{metrics['length_change']:+.1f}",
            f"{metrics['babis_phrases_improvement']:+.1f}",
            f"{metrics['slovak_words_improvement']:+.1f}",
            f"{metrics['overall_improvement_score']:+.1f}"
        ]
    }
    
    # Vytvo≈ôen√≠ tabulky bez pandas
    headers = ["Metrika", "P≈ôed fine-tuningem", "Po fine-tuningem", "Zlep≈°en√≠"]
    rows = []
    for i in range(len(table_data["Metrika"])):
        row = [
            table_data["Metrika"][i],
            table_data["P≈ôed fine-tuningem"][i],
            table_data["Po fine-tuningem"][i],
            table_data["Zlep≈°en√≠"][i]
        ]
        rows.append(row)
    
    # V√Ωpoƒçet ≈°√≠≈ôky sloupc≈Ø
    col_widths = []
    for header in headers:
        col_widths.append(len(header))
    
    for row in rows:
        for i, cell in enumerate(row):
            col_widths[i] = max(col_widths[i], len(cell))
    
    # Vytvo≈ôen√≠ tabulky
    table_lines = []
    
    # Hlaviƒçka
    header_line = "|"
    separator_line = "|"
    for i, header in enumerate(headers):
        header_line += f" {header:<{col_widths[i]}} |"
        separator_line += f" {'-' * col_widths[i]} |"
    table_lines.append(header_line)
    table_lines.append(separator_line)
    
    # ≈ò√°dky dat
    for row in rows:
        data_line = "|"
        for i, cell in enumerate(row):
            data_line += f" {cell:<{col_widths[i]}} |"
        table_lines.append(data_line)
    
    table_text = "\n".join(table_lines)
    
    print(f"üìä Tabulka srovn√°n√≠:")
    print(table_text)
    
    return table_data

if __name__ == "__main__":
    # Test srovn√°n√≠ model≈Ø
    print("üß™ Test srovn√°n√≠ model≈Ø...")
    
    # Nejd≈ô√≠ve vygenerovat testovac√≠ data
    from generate_responses import generate_responses
    
    generate_responses("base", "results/before_finetune/")
    generate_responses("finetuned", "results/after_finetune/")
    
    # Spustit srovn√°n√≠
    results = compare_models()
    
    # Vytvo≈ôit tabulku
    table_data = create_comparison_table()
    
    print("\nüìã Tabulka srovn√°n√≠ vytvo≈ôena") 